# -*- coding: utf-8 -*-
"""train_mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1knrIfrsD2Fz3AFzNCmfMkqHVuPrDXeDc
"""

##############################################################################
#
# This python notebook downloads MNIST data from kaggle and performs
# standardization, training, validation and testing data splits before using the
# manual MLP model to train to predict MNST digit images.
#
#     02/25/2025
#     Vimal Thomas Joseph
#
# Initial Draft
#
#############################################################################

import kagglehub
import os
import numpy as np

# Download the dataset
path = kagglehub.dataset_download("hojjatk/mnist-dataset", force_download=True)

# Define data loading functions and normalize the pixel values to 0 and 1.

def load_mnist_images(filename):
    with open(filename, "rb") as f:
        f.read(16)
        data = np.fromfile(f, dtype=np.uint8)
    return data.reshape(-1, 28 * 28) / 255.0

def load_mnist_labels(filename):
    with open(filename, "rb") as f:
        f.read(8)
        labels = np.fromfile(f, dtype=np.uint8)
    return labels

# Initialize data variables
train_x, train_y, test_x, test_y = None, None, None, None

# List items in the downloaded directory and load data if the item is of type file.

for item in os.listdir(path):
    item_path = os.path.join(path, item)

    if os.path.isfile(item_path):
        print(f"Found file: {item}")
        if "train-images" in item and item.endswith(".idx3-ubyte"):
            train_x = load_mnist_images(item_path)
        elif "train-labels" in item and item.endswith(".idx1-ubyte"):
            train_y = load_mnist_labels(item_path)
        elif "t10k-images" in item and item.endswith(".idx3-ubyte"):
            test_x = load_mnist_images(item_path)
        elif "t10k-labels" in item and item.endswith(".idx1-ubyte"):
            test_y = load_mnist_labels(item_path)


if train_x is not None and train_y is not None and test_x is not None and test_y is not None:
    print(f"Train Images Shape: {train_x.shape}, Train Labels Shape: {train_y.shape}")
    print(f"Test Images Shape: {test_x.shape}, Test Labels Shape: {test_y.shape}")
else:
    print("Error: Could not load all necessary data files.")

#Divide the dataset furthen into training and validation in the 8:2 ratio.


# Calculate the split index
split_index = int(0.8 * len(train_x))

# Split the training data into training and validation sets
train_x_new, val_x = train_x[:split_index], train_x[split_index:]
train_y_new, val_y = train_y[:split_index], train_y[split_index:]

# Print the shapes of the new datasets
print(f"New Train Images Shape: {train_x_new.shape}, New Train Labels Shape: {train_y_new.shape}")
print(f"Validation Images Shape: {val_x.shape}, Validation Labels Shape: {val_y.shape}")

import matplotlib.pyplot as plt

# Function to visualize mnist image

def show_image(image_vector, label):
    image = image_vector.reshape(28, 28)
    plt.imshow(image, cmap="gray")
    plt.title(f"Label: {label}")
    plt.axis("off")
    plt.show()

# Show 5 random images
num_samples = 5
indices = np.random.choice(len(train_x_new), num_samples, replace=False)

for i in indices:
    show_image(train_x_new[i], train_y[i])

#function to one hot encode the clases so that the values are either 0 or 1

def one_hot_encode(labels, num_classes=10):
    return np.eye(num_classes)[labels]

# Convert labels
train_y_one_hot = one_hot_encode(train_y_new)
val_y_one_hot = one_hot_encode(val_y)

# Print  example
print(f"Example Label: {train_y_new[0]}, One-Hot: {train_y_one_hot[0]}")

# Commented out IPython magic to ensure Python compatibility.
#step to run the model from github. The multilayer perceptron is loaded from the github

!git clone https://github.com/vimalthomas-db/deeplearning.git
# %cd deeplearning/

# Run the model notebook
# %run "deploy_multilayerperceptron.ipynb"

# Check the shapes of training and test data
print(f"Train X shape: {train_x_new.shape}, Train Y shape: {train_y_one_hot.shape}")
print(f"Val X shape: {val_x.shape}, Test Y shape: {val_y_one_hot.shape}")

#initialize the variables for better readability

training_losses,validation_losses=[],[]

print(training_losses)

# Define the MLP architecture

#The MLP architecture contains Relu hidden layers and Softmax based output layer.

mlp = MultilayerPerceptron([
    Layer(784, 128, Relu(),dropout_rate=0.4),
    Layer(128, 64, Relu(),dropout_rate=0.4),
    Layer(64, 10, Softmax())
])

# Define the loss function
loss_function = CrossEntropy()

# Train the model
training_losses, validation_losses = mlp.train(
    train_x_new, train_y_one_hot,  # Train Data
    val_x, val_y_one_hot,    # Validation Data
    loss_function,
    learning_rate=0.0001,
    batch_size=32,
    epochs=30, # Adjust epochs as needed
    model_type='classification',
    RMSProp=True

)

print(training_losses)

# Compute test accuracy
val_accuracy = compute_accuracy(mlp, val_x, val_y_one_hot)
print(f"Test Accuracy: {val_accuracy:.2f}%")

import matplotlib.pyplot as plt

# Function to plot training and validation losses
def plot_losses(training_losses, validation_losses):
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, len(training_losses) + 1), training_losses, label="Training Loss", marker="o")
    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label="Validation Loss", marker="s")

    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training & Validation Loss Over Epochs")
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot the loss curves
plot_losses(training_losses, validation_losses)

print(training_losses)

#validate the test data based on the trained model.

import numpy as np

# Get predictions on test data
test_y_one_hot = one_hot_encode(test_y)

test_predictions = mlp.forward(test_x,training=False)
test_pred_classes = np.argmax(test_predictions, axis=1)
test_true_classes = np.argmax(test_y_one_hot, axis=1)

# Compute per-class accuracy
unique_classes = np.unique(test_true_classes)
for cls in unique_classes:
    cls_mask = test_true_classes == cls
    cls_accuracy = np.mean(test_pred_classes[cls_mask] == test_true_classes[cls_mask]) * 100
    print(f"Accuracy for digit {cls}: {cls_accuracy:.2f}%")


# Calculate overall accuracy
overall_accuracy = np.mean(test_pred_classes == test_true_classes) * 100
print(f"Overall Test Accuracy: {overall_accuracy:.2f}%")

print(f"Overall Test Accuracy: {overall_accuracy:.2f}%")

import matplotlib.pyplot as plt


# Find misclassified indices
misclassified = np.where(test_pred_classes != test_true_classes)[0]

# Plot a few misclassified samples
num_samples = 10  # Number of misclassified images to show
plt.figure(figsize=(10, 5))

for i, idx in enumerate(misclassified[:num_samples]):
    plt.subplot(2, 5, i + 1)
    plt.imshow(test_x[idx].reshape(28, 28), cmap="gray")
    plt.title(f"True: {test_true_classes[idx]}, Pred: {test_pred_classes[idx]}")
    plt.axis("off")

plt.suptitle("Misclassified Images")
plt.show()

import matplotlib.pyplot as plt
import numpy as np



# Find correctly classified indices for each digit
classified_indices = {}
for digit in range(10):
    classified_indices[digit] = np.where((test_pred_classes == digit) & (test_true_classes == digit))[0]

# Plot one sample for each digit
plt.figure(figsize=(10, 5))
for digit in range(10):
    if len(classified_indices[digit]) > 0:
        idx = classified_indices[digit][0]
        plt.subplot(2, 5, digit + 1)
        plt.imshow(test_x[idx].reshape(28, 28), cmap="gray")
        plt.title(f"True: {test_true_classes[idx]}, Pred: {test_pred_classes[idx]}")
        plt.axis("off")
    else:
        print(f"No correctly classified samples found for digit {digit}")

plt.suptitle("Correctly Classified Images (One per Digit)")
plt.show()

import seaborn as sns
import numpy as np
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
conf_matrix = confusion_matrix(test_true_classes, test_pred_classes)

# Plot heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()