# -*- coding: utf-8 -*-
"""train_mpg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPbwHCjltWVFA2HQGlgCFnSpTQisGEDv
"""

##############################################################################
#
# This python notebook downloads MPG data from UCIML repo and performs
# standardization, training, validation and testing data splits before using the
# manual MLP model to train to predict mile per gallon for the given classes.
#
#     02/25/2025
#     Vimal Thomas Joseph
#
# Initial Draft
#
#############################################################################


import math
!pip install ucimlrepo
!pip install scikit-learn

from ucimlrepo import fetch_ucirepo
import pandas as pd

# fetch dataset
auto_mpg = fetch_ucirepo(id=9)

# data (as pandas dataframes)
X = auto_mpg.data.features
y = auto_mpg.data.targets

# Combine features and target into one DataFrame for easy filtering
data = pd.concat([X, y], axis=1)

# Drop rows where the target variable is NaN
cleaned_data = data.dropna()

# Split the data back into features (X) and target (y)
X = cleaned_data.iloc[:, :-1]
y = cleaned_data.iloc[:, -1]

# Display the number of rows removed
rows_removed = len(data) - len(cleaned_data)
print(f"Rows removed: {rows_removed}")

from sklearn.model_selection import train_test_split

# Do a 70/30 split (e.g., 70% train, 30% other)
X_train, X_leftover, y_train, y_leftover = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    shuffle=True,
)

# Split the remaining 30% into validation/testing (15%/15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_leftover, y_leftover,
    test_size=0.5,
    random_state=42,
    shuffle=True,
)

# Compute statistics for X (features)
X_mean = X_train.mean(axis=0)
X_std = X_train.std(axis=0)

# Standardize X
X_train = (X_train - X_mean) / X_std
X_val = (X_val - X_mean) / X_std
X_test = (X_test - X_mean) / X_std

# Compute statistics for y (targets)
y_mean = y_train.mean()
y_std = y_train.std()

# Standardize y
y_train = (y_train - y_mean) / y_std
y_val = (y_val - y_mean) / y_std
y_test = (y_test - y_mean) / y_std

#store the spilt data values in the train_x,train_y... for further steps.
train_x, train_y = X_train.values, y_train.values
val_x, val_y =     X_val.values, y_val.values
test_x, test_y =   X_test.values, y_test.values

train_y = train_y.reshape(-1, 1)
val_y = val_y.reshape(-1, 1)
test_y = test_y.reshape(-1, 1)

#print the initial shapes.
print(train_x.shape, train_y.shape)
print(val_x.shape, val_y.shape)
print(test_x.shape, test_y.shape)

# Commented out IPython magic to ensure Python compatibility.
# Download the MLP model stored in github.

!git clone https://github.com/vimalthomas-db/deeplearning.git
# %cd deeplearning/

# %run "deploy_multilayerperceptron.ipynb"

import numpy as np


## RUN THIS STEP ONLY IF YOU NEED TO FIND OUT BEST POSSIBLE COMBINATIONS
## RUN THIS STEP ONLY IF YOU NEED TO FIND OUT BEST POSSIBLE COMBINATIONS
## RUN THIS STEP ONLY IF YOU NEED TO FIND OUT BEST POSSIBLE COMBINATIONS


#this is an additional block we used for testing to bring out best possible combination of learning rates, bacth sizes and epoch limits.

# Define the hyperparameters
learning_rates = [0.001, 0.01, 0.1]
batch_sizes = [32, 64, 128]
epochs_list = [1000, 2000, 3000]

# Store results
results = []

# Iterate through hyperparameter combinations
for lr in learning_rates:
    for batch_size in batch_sizes:
        for epochs in epochs_list:
            # Initialize the MLP
            mlp = MultilayerPerceptron([
                Layer(train_x.shape[1], 256, Relu(), dropout_rate=0.3),
                Layer(256, 128, Relu(), dropout_rate=0.3),
                Layer(128, 64, Relu(), dropout_rate=0.3),
                Layer(64, 64, Relu(), dropout_rate=0.3),
                Layer(64, 1, Linear())
            ])

            loss_function = SquaredError()

            # Train the model
            train_losses, val_losses = mlp.train(
                train_x=train_x, train_y=train_y,
                val_x=val_x, val_y=val_y,
                loss_func=loss_function,
                learning_rate=lr, batch_size=batch_size, epochs=epochs, model_type='regression', RMSProp=True
            )

            # Store the results
            results.append({
                'learning_rate': lr,
                'batch_size': batch_size,
                'epochs': epochs,
                'train_loss': train_losses[-1],
                'val_loss': val_losses[-1]
            })

# Display the results
for result in results:
    print(result)

# Create a DataFrame from the results and store the top 5 entries based on best validation loses.
df_result = pd.DataFrame(results)


top_5 = df_result.sort_values(by=['val_loss']).head(5)

# Display the top 5 results
print(top_5)

# Create a DataFrame from the results and store the top 5 entries based on best training loses.
df_result = pd.DataFrame(results)

# Sort by validation loss (ascending) and get the top 5
top_5 = df_result.sort_values(by=['train_loss']).head(5)

# Display the top 5 results
print(top_5)

#calling MLP model built. This is basically chosen from the results above.

mlp = MultilayerPerceptron([
    Layer(train_x.shape[1], 256, Relu(), dropout_rate=0.3),
    Layer(256, 128, Relu(), dropout_rate=0.3),
    Layer(128, 64, Relu(), dropout_rate=0.3),
    Layer(64, 64, Relu(), dropout_rate=0.3),
    Layer(64, 1, Linear())
])

loss_function = SquaredError()
train_losses, val_losses = mlp.train(
    train_x=train_x, train_y=train_y,
    val_x=val_x, val_y=val_y,
    loss_func=loss_function,
    learning_rate=0.001, batch_size=64, epochs=3000, model_type='regression', RMSProp=True
)

#Training and Validaiton Loss Plots

import matplotlib.pyplot as plt


plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#print total losses from the training

print(f"Total Training Loss: {train_losses[-1]:.4f}")
print(f"Total Validation Loss: {val_losses[-1]:.4f}")

#evaluating with test dataset


test_result= mlp.forward(test_x,training=False)
actual_predictions = (test_result * y_std) + y_mean
actual_y = (test_y * y_std) + y_mean

loss_function = SquaredError()
test_loss = loss_function.loss(test_result, test_y)

test_loss_value = np.mean(test_loss)
print(f"Test Loss: {test_loss_value:.4f}")

#show predicted values and actual values side by side


for i in range(3,13):
    print(f"Predicted: {actual_predictions[i][0]:.2f}, Actual: {actual_y[i][0]:.2f}")