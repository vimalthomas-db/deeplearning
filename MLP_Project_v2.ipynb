{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvzParQCE7XaAE7Drvga3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimalthomas/deeplearning/blob/main/MLP_Project_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hO48dKJRV-ob"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def batch_generator(train_x, train_y, batch_size):\n",
        "    indices = np.arange(len(train_x))\n",
        "    np.random.shuffle(indices)  # Shuffle indices\n",
        "\n",
        "    for i in range(0, len(train_x), batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "        batch_x = train_x[batch_idx]\n",
        "        batch_y = train_y[batch_idx]\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivationFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the output of the activation function, evaluated on x\n",
        "\n",
        "        Input args may differ in the case of softmax\n",
        "\n",
        "        :param x (np.ndarray): input\n",
        "        :return: output of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the activation function, evaluated on x\n",
        "        :param x (np.ndarray): input\n",
        "        :return: activation function's derivative at x\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return self.forward(x) * (1 - self.forward(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(ActivationFunction):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Relu(ActivationFunction):\n",
        "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0, x)  # ReLU function: max(0, x)\n",
        "\n",
        "  def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "    return (x > 0).astype(float)  # Derivative: 1 for x > 0, else 0\n",
        "\n",
        "\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Linear(ActivationFunction):\n",
        "    pass\n",
        "\n",
        "\n",
        "class LossFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "\n",
        "class SquaredError(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return 1/2 * np.square(y_pred-y_true)\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return (y_pred - y_true)/y_pred.shape[0]\n",
        "\n",
        "\n",
        "class CrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -y_true / y_pred\n",
        "\n",
        "class BinaryCrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Prevent log(0) issues\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes gradient of binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n",
        "\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction):\n",
        "        \"\"\"\n",
        "        Initializes a layer of neurons\n",
        "\n",
        "        :param fan_in: number of neurons in previous (presynpatic) layer\n",
        "        :param fan_out: number of neurons in this layer\n",
        "        :param activation_function: instance of an ActivationFunction\n",
        "        \"\"\"\n",
        "        self.fan_in = fan_in\n",
        "        self.fan_out = fan_out\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        # this will store the activations (forward prop)\n",
        "        self.activations = None\n",
        "        # this will store the delta term (dL_dPhi, backward prop)\n",
        "        self.delta = None\n",
        "\n",
        "        # Initialize weights and biaes\n",
        "        # self.W = None  # weights\n",
        "        # self.b = None  # biases\n",
        "\n",
        "        #note to self. looks like He initialization will help relu function\n",
        "        #come back later\n",
        "\n",
        "        #self.W = np.random.randn(fan_in, fan_out) * 0.01\n",
        "        self.W = np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)  # He_init for Relu\n",
        "\n",
        "        self.b = np.zeros((fan_out,))\n",
        "\n",
        "    def forward(self, h: np.ndarray):\n",
        "        \"\"\"\n",
        "        Computes the activations for this layer\n",
        "\n",
        "        :param h: input to layer\n",
        "        :return: layer activations\n",
        "        \"\"\"\n",
        "        #Z calculation\n",
        "\n",
        "        Z = np.dot(h, self.W) + self.b\n",
        "        #self.activations = None\n",
        "        self.activations = self.activation_function.forward(Z)\n",
        "        return self.activations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    #     \"\"\"\n",
        "    #     Apply backpropagation to this layer and return the weight and bias gradients\n",
        "\n",
        "    #     :param h: input to this layer\n",
        "    #     :param delta: delta term from layer above\n",
        "    #     :return: (weight gradients, bias gradients)\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     #compute dZ\n",
        "    #     dZ = delta * self.activation_function.derivative(self.activations)\n",
        "    #     #compute dW, db\n",
        "\n",
        "    #     dL_dW = np.dot(h.T, dZ) / h.shape[0]\n",
        "    #     dL_db = np.sum(dZ, axis=0) / h.shape[0]\n",
        "\n",
        "\n",
        "    #     self.delta = dZ\n",
        "    #     return dL_dW, dL_db\n",
        "\n",
        "    def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "      \"\"\"\n",
        "\n",
        "      Apply backpropagation to this layer and return the weight and bias gradients.\n",
        "\n",
        "      :param h: Input to this layer.\n",
        "      :param delta: Delta term from the layer above.\n",
        "      :return: (Weight gradients, Bias gradients).\n",
        "      \"\"\"\n",
        "\n",
        "      # Compute dZ\n",
        "      dZ = delta * self.activation_function.derivative(self.activations)\n",
        "\n",
        "    # Compute weight and bias gradients\n",
        "      dL_dW = np.dot(h.T, dZ) / h.shape[0]  # (fan_in, fan_out)\n",
        "      dL_db = np.sum(dZ, axis=0, keepdims=True) / h.shape[0]  # (1, fan_out)\n",
        "\n",
        "    # Convert to NumPy array (ensures no list issue)\n",
        "      dL_db = np.array(dL_db)\n",
        "\n",
        "    # Correctly propagate `self.delta`\n",
        "      self.delta = np.dot(dZ, self.W.T)  # (batch_size, fan_in)\n",
        "\n",
        "      return dL_dW, dL_db\n",
        "\n",
        "\n",
        "\n",
        "class MultilayerPerceptron:\n",
        "    def __init__(self, layers: Tuple[Layer]):\n",
        "        \"\"\"\n",
        "        Create a multilayer perceptron (densely connected multilayer neural network)\n",
        "        :param layers: list or Tuple of layers\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        This takes the network input and computes the network output (forward propagation)\n",
        "        :param x: network input\n",
        "        :return: network output\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, loss_grad: np.ndarray, input_data: np.ndarray) -> Tuple[list, list]:\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      Applies backpropagation to compute gradients of weights and biases for all layers in the network.\n",
        "\n",
        "      :param loss_grad: Gradient of loss w.r.t. final layer output (dL/dA).\n",
        "      :param input_data: The input data to the network (train_x for the first layer).\n",
        "      :return: (List of weight gradients for all layers, List of bias gradients for all layers).\n",
        "      \"\"\"\n",
        "\n",
        "      dl_dw_all = []\n",
        "      dl_db_all = []\n",
        "\n",
        "      dL_dA = loss_grad  # Start with gradient from the loss function\n",
        "\n",
        "    # Iterate backward through layers\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "        layer = self.layers[i]\n",
        "\n",
        "        # Get the correct input for this layer\n",
        "        if i == 0:\n",
        "            h = input_data  # First layer gets train_x\n",
        "        else:\n",
        "            h = self.layers[i - 1].activations  # Other layers get activations from previous layer\n",
        "\n",
        "        # Compute backpropagation step for this layer\n",
        "        dL_dW, dL_db = layer.backward(h, dL_dA)\n",
        "\n",
        "        # Store gradients\n",
        "        dl_dw_all.append(dL_dW)\n",
        "        dl_db_all.append(dL_db)\n",
        "\n",
        "        dL_dA = layer.delta\n",
        "\n",
        "    # Reverse lists to match layer order\n",
        "      dl_dw_all.reverse()\n",
        "      dl_db_all.reverse()\n",
        "\n",
        "      return dl_dw_all, dl_db_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Train the multilayer perceptron\n",
        "\n",
        "        :param train_x: full training set input of shape (n x d) n = number of samples, d = number of features\n",
        "        :param train_y: full training set output of shape (n x q) n = number of samples, q = number of outputs per sample\n",
        "        :param val_x: full validation set input\n",
        "        :param val_y: full validation set output\n",
        "        :param loss_func: instance of a LossFunction\n",
        "        :param learning_rate: learning rate for parameter updates\n",
        "        :param batch_size: size of each batch\n",
        "        :param epochs: number of epochs\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        #define the epoch loop\n",
        "\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "\n",
        "\n",
        "        #defin epoch loop\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "          #define batch loop\n",
        "          total_loss = 0\n",
        "\n",
        "          for batch_x, batch_y in batch_generator(train_x, train_y, batch_size):\n",
        "\n",
        "\n",
        "            #forward pass\n",
        "            y_pred = self.forward(batch_x)\n",
        "\n",
        "            #compute loss\n",
        "\n",
        "            batchloss = loss_func.loss(batch_y, y_pred)\n",
        "            total_loss = total_loss + np.mean(batchloss)\n",
        "\n",
        "            #print(total_loss)\n",
        "\n",
        "\n",
        "            #backward pass and compute gradianet\n",
        "            #dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y, output), batch_x)\n",
        "            dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y[:len(y_pred)], y_pred), batch_x)\n",
        "\n",
        "\n",
        "            #update weights\n",
        "            max_grad_norm = 1.0  # Limits the maximum gradient value\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(len(self.layers)):\n",
        "              # Clip gradients\n",
        "              dL_dW[i] = np.clip(dL_dW[i], -max_grad_norm, max_grad_norm)\n",
        "              dL_db[i] = np.clip(dL_db[i], -max_grad_norm, max_grad_norm)\n",
        "\n",
        "              self.layers[i].W -= learning_rate * dL_dW[i]\n",
        "              #self.layers[i].b -= learning_rate * dL_db[i]\n",
        "              self.layers[i].b -= learning_rate * np.array(dL_db[i]).flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          training_losses.append(total_loss / len(train_x))\n",
        "\n",
        "          # Compute Validation Loss\n",
        "          val_output = self.forward(val_x)\n",
        "          val_loss = loss_func.loss(val_y, val_output)\n",
        "          validation_losses.append(np.mean(val_loss))\n",
        "\n",
        "          # Compute training accuracy at the end of the epoch\n",
        "          train_acc = compute_accuracy(self, train_x, train_y)\n",
        "          val_acc = compute_accuracy(self, val_x, val_y)\n",
        "\n",
        "          print(f\"dL_dW max: {np.max([np.max(w) for w in dL_dW])}, min: {np.min([np.min(w) for w in dL_dW])}\")\n",
        "          print(f\"dL_db max: {np.max([np.max(b) for b in dL_db])}, min: {np.min([np.min(b) for b in dL_db])}\")\n",
        "\n",
        "          print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {total_loss:.4f} - Training Acc: {train_acc:.2f}% - Validation Acc: {val_acc:.2f}% - Validation Loss: {validation_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return training_losses, validation_losses\n",
        "\n",
        "def compute_accuracy(model, X, y):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model.\n",
        "\n",
        "    :param model: The trained MLP model\n",
        "    :param X: Input features (numpy array)\n",
        "    :param y: True labels (numpy array)\n",
        "    :return: Accuracy in percentage (%)\n",
        "    \"\"\"\n",
        "    y_pred = model.forward(X)  # Forward pass\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to 0 or 1\n",
        "    accuracy = np.mean(y_pred_binary == y) * 100  # Compute accuracy\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "\n",
        "# Features: x1 and x2 sampled from normal distribution\n",
        "x1 = np.random.randn(num_samples)\n",
        "x2 = np.random.randn(num_samples)\n",
        "\n",
        "# Labels: 1 if x1 + x2 > 0, else 0\n",
        "y = (x1 + x2 > 0).astype(int)\n",
        "\n",
        "# Stack features into input matrix\n",
        "X = np.column_stack((x1, x2))\n",
        "Y = y.reshape(-1, 1)  # Convert to column vector\n",
        "\n",
        "# Split into train and validation sets\n",
        "split = int(0.8 * num_samples)\n",
        "train_x, val_x = X[:split], X[split:]\n",
        "train_y, val_y = Y[:split], Y[split:]\n",
        "\n",
        "# Plot the dataset\n",
        "plt.scatter(x1[y == 0], x2[y == 0], color='red', label='Class 0')\n",
        "plt.scatter(x1[y == 1], x2[y == 1], color='blue', label='Class 1')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.title('Synthetic Dataset for MLP Testing')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True)\n",
        "print(f\"Class distribution: {dict(zip(unique, counts))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "A5lUI7-9yHby",
        "outputId": "65dd3e87-f45e-4640-bb4f-89506ef73279"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: {0: 382, 1: 418}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an MLP model\n",
        "layers = [\n",
        "    Layer(fan_in=2, fan_out=8, activation_function=Relu()),  # Hidden layer with 4 neurons\n",
        "    Layer(fan_in=8, fan_out=8, activation_function=Relu()),  # Hidden layer with 4 neurons\n",
        "    Layer(fan_in=8, fan_out=1, activation_function=Sigmoid()) # Output layer with 1 neuron\n",
        "]\n",
        "\n",
        "mlp = MultilayerPerceptron(layers)\n",
        "\n",
        "# Train the model\n",
        "loss_function = BinaryCrossEntropy()\n",
        "train_losses, val_losses = mlp.train(\n",
        "    train_x=train_x, train_y=train_y,\n",
        "    val_x=val_x, val_y=val_y,\n",
        "    loss_func=loss_function,\n",
        "    learning_rate=0.01, batch_size=32, epochs=300\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN7cBZFcyNJp",
        "outputId": "20888c3d-a7b6-439a-e0b6-a6262f9b4ec4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL_dW max: 0.020524986357197367, min: -0.02693894956255776\n",
            "dL_db max: 0.014065401537837422, min: -0.01468538823588397\n",
            "Epoch 1/300 - Training Loss: 17.8600 - Training Acc: 47.75% - Validation Acc: 47.00% - Validation Loss: 0.6888\n",
            "dL_dW max: 0.01720289725205523, min: -0.024480366622328012\n",
            "dL_db max: 0.009684720971959404, min: -0.0093150729115019\n",
            "Epoch 2/300 - Training Loss: 17.6867 - Training Acc: 48.12% - Validation Acc: 47.00% - Validation Loss: 0.6823\n",
            "dL_dW max: 0.008839781271075115, min: -0.013049055528807148\n",
            "dL_db max: 0.005976646177828806, min: -0.006773905607878351\n",
            "Epoch 3/300 - Training Loss: 17.5166 - Training Acc: 48.38% - Validation Acc: 47.00% - Validation Loss: 0.6761\n",
            "dL_dW max: 0.033781224134040264, min: -0.022401518517021983\n",
            "dL_db max: 0.01709969690794724, min: -0.01824503737109955\n",
            "Epoch 4/300 - Training Loss: 17.3609 - Training Acc: 48.50% - Validation Acc: 48.00% - Validation Loss: 0.6702\n",
            "dL_dW max: 0.006778221902965915, min: -0.007338357478273395\n",
            "dL_db max: 0.0033302173070124605, min: -0.007065212426749808\n",
            "Epoch 5/300 - Training Loss: 17.2105 - Training Acc: 49.00% - Validation Acc: 48.00% - Validation Loss: 0.6647\n",
            "dL_dW max: 0.015066466532246171, min: -0.017909312705206962\n",
            "dL_db max: 0.012107961790529417, min: -0.013237185418421258\n",
            "Epoch 6/300 - Training Loss: 17.0707 - Training Acc: 49.62% - Validation Acc: 48.00% - Validation Loss: 0.6594\n",
            "dL_dW max: 0.011685104731973947, min: -0.020401334306717568\n",
            "dL_db max: 0.00859984491544573, min: -0.012277351577068451\n",
            "Epoch 7/300 - Training Loss: 16.9347 - Training Acc: 49.62% - Validation Acc: 48.50% - Validation Loss: 0.6543\n",
            "dL_dW max: 0.00879720537604627, min: -0.01510850311165015\n",
            "dL_db max: 0.006434931633569931, min: -0.008301929850426106\n",
            "Epoch 8/300 - Training Loss: 16.8066 - Training Acc: 49.75% - Validation Acc: 49.00% - Validation Loss: 0.6495\n",
            "dL_dW max: 0.023557536534150988, min: -0.0212417418662313\n",
            "dL_db max: 0.008951213506280477, min: -0.013232111655481084\n",
            "Epoch 9/300 - Training Loss: 16.6852 - Training Acc: 50.38% - Validation Acc: 49.00% - Validation Loss: 0.6449\n",
            "dL_dW max: 0.005579688406620838, min: -0.007887012601039859\n",
            "dL_db max: 0.0017259535012821518, min: -0.0038809166132225984\n",
            "Epoch 10/300 - Training Loss: 16.5691 - Training Acc: 50.88% - Validation Acc: 50.00% - Validation Loss: 0.6405\n",
            "dL_dW max: 0.01690449878139318, min: -0.020158125012254363\n",
            "dL_db max: 0.012388999398244843, min: -0.014123976007596567\n",
            "Epoch 11/300 - Training Loss: 16.4574 - Training Acc: 51.25% - Validation Acc: 51.00% - Validation Loss: 0.6362\n",
            "dL_dW max: 0.008542352173194807, min: -0.008767301909124826\n",
            "dL_db max: 0.0018467337602333603, min: -0.00753874496381894\n",
            "Epoch 12/300 - Training Loss: 16.3488 - Training Acc: 51.75% - Validation Acc: 51.00% - Validation Loss: 0.6322\n",
            "dL_dW max: 0.00907647242904646, min: -0.007504356865682814\n",
            "dL_db max: 0.005575004071939189, min: -0.005012463183802405\n",
            "Epoch 13/300 - Training Loss: 16.2464 - Training Acc: 52.12% - Validation Acc: 51.50% - Validation Loss: 0.6282\n",
            "dL_dW max: 0.027968750282438955, min: -0.019307712941019822\n",
            "dL_db max: 0.0130673273683161, min: -0.012922279375289879\n",
            "Epoch 14/300 - Training Loss: 16.1453 - Training Acc: 52.62% - Validation Acc: 51.50% - Validation Loss: 0.6244\n",
            "dL_dW max: 0.008690673687632182, min: -0.010478887201094718\n",
            "dL_db max: 0.007648590801047749, min: -0.007613363022416589\n",
            "Epoch 15/300 - Training Loss: 16.0476 - Training Acc: 53.25% - Validation Acc: 52.50% - Validation Loss: 0.6208\n",
            "dL_dW max: 0.00609845770412726, min: -0.010376074722924965\n",
            "dL_db max: 0.006223959927200373, min: -0.00957456551078774\n",
            "Epoch 16/300 - Training Loss: 15.9546 - Training Acc: 53.62% - Validation Acc: 52.50% - Validation Loss: 0.6172\n",
            "dL_dW max: 0.011997818834990381, min: -0.019157594045940706\n",
            "dL_db max: 0.006425511845344156, min: -0.007954887222054313\n",
            "Epoch 17/300 - Training Loss: 15.8636 - Training Acc: 54.12% - Validation Acc: 53.50% - Validation Loss: 0.6138\n",
            "dL_dW max: 0.0049505414018642165, min: -0.008237065677170996\n",
            "dL_db max: 0.003032149696922922, min: -0.0034512639780531506\n",
            "Epoch 18/300 - Training Loss: 15.7715 - Training Acc: 54.87% - Validation Acc: 54.00% - Validation Loss: 0.6104\n",
            "dL_dW max: 0.008328170876627159, min: -0.014620516961160967\n",
            "dL_db max: 0.005595842783640242, min: -0.007969399749627154\n",
            "Epoch 19/300 - Training Loss: 15.6884 - Training Acc: 55.25% - Validation Acc: 54.00% - Validation Loss: 0.6072\n",
            "dL_dW max: 0.01328045020156043, min: -0.01967769325411038\n",
            "dL_db max: 0.008016548126123981, min: -0.010234853241872701\n",
            "Epoch 20/300 - Training Loss: 15.6044 - Training Acc: 55.50% - Validation Acc: 54.00% - Validation Loss: 0.6041\n",
            "dL_dW max: 0.007331743643458931, min: -0.005866628946075051\n",
            "dL_db max: 0.0031899353935642043, min: -0.00376999319792036\n",
            "Epoch 21/300 - Training Loss: 15.5244 - Training Acc: 55.88% - Validation Acc: 54.50% - Validation Loss: 0.6011\n",
            "dL_dW max: 0.005319025930152498, min: -0.009826147629831004\n",
            "dL_db max: 0.003989489893042154, min: -0.004074618939298648\n",
            "Epoch 22/300 - Training Loss: 15.4474 - Training Acc: 56.25% - Validation Acc: 54.50% - Validation Loss: 0.5982\n",
            "dL_dW max: 0.0060030395165494325, min: -0.011211112536081909\n",
            "dL_db max: 0.004439228452914363, min: -0.005500936663475413\n",
            "Epoch 23/300 - Training Loss: 15.3703 - Training Acc: 56.25% - Validation Acc: 55.00% - Validation Loss: 0.5953\n",
            "dL_dW max: 0.012912482154002619, min: -0.011955535451989028\n",
            "dL_db max: 0.006375491233653997, min: -0.007467474931169709\n",
            "Epoch 24/300 - Training Loss: 15.2963 - Training Acc: 56.50% - Validation Acc: 56.00% - Validation Loss: 0.5925\n",
            "dL_dW max: 0.0065481623816406815, min: -0.013919273696560618\n",
            "dL_db max: 0.0038863614830241386, min: -0.0077156062748979255\n",
            "Epoch 25/300 - Training Loss: 15.2227 - Training Acc: 57.12% - Validation Acc: 56.50% - Validation Loss: 0.5897\n",
            "dL_dW max: 0.008158586195189577, min: -0.00952076525919116\n",
            "dL_db max: 0.005162175983797954, min: -0.0070658894167827235\n",
            "Epoch 26/300 - Training Loss: 15.1537 - Training Acc: 57.38% - Validation Acc: 56.50% - Validation Loss: 0.5870\n",
            "dL_dW max: 0.008748132755154497, min: -0.00872355503535594\n",
            "dL_db max: 0.0026026335100420212, min: -0.006439224661432746\n",
            "Epoch 27/300 - Training Loss: 15.0840 - Training Acc: 57.75% - Validation Acc: 56.50% - Validation Loss: 0.5844\n",
            "dL_dW max: 0.004308643475405494, min: -0.007938786213586041\n",
            "dL_db max: 0.002243189534625614, min: -0.004562054729985891\n",
            "Epoch 28/300 - Training Loss: 15.0155 - Training Acc: 58.13% - Validation Acc: 57.00% - Validation Loss: 0.5818\n",
            "dL_dW max: 0.007436892773722053, min: -0.008166111994266837\n",
            "dL_db max: 0.002376740628471301, min: -0.008295822816996765\n",
            "Epoch 29/300 - Training Loss: 14.9513 - Training Acc: 58.25% - Validation Acc: 57.00% - Validation Loss: 0.5793\n",
            "dL_dW max: 0.009418687054926425, min: -0.005920202713064976\n",
            "dL_db max: 0.005742727340150695, min: -0.005047007625965956\n",
            "Epoch 30/300 - Training Loss: 14.8874 - Training Acc: 58.25% - Validation Acc: 57.50% - Validation Loss: 0.5769\n",
            "dL_dW max: 0.005374593278195347, min: -0.012891653140913017\n",
            "dL_db max: 0.0045491150100449565, min: -0.010390231362754816\n",
            "Epoch 31/300 - Training Loss: 14.8226 - Training Acc: 58.38% - Validation Acc: 57.50% - Validation Loss: 0.5745\n",
            "dL_dW max: 0.0053137052843876, min: -0.0067096968564430625\n",
            "dL_db max: 0.0016492404886529791, min: -0.005352909227893904\n",
            "Epoch 32/300 - Training Loss: 14.7620 - Training Acc: 59.00% - Validation Acc: 57.50% - Validation Loss: 0.5721\n",
            "dL_dW max: 0.00681940969481051, min: -0.00801720902147461\n",
            "dL_db max: 0.0043203844390650855, min: -0.004920979074175004\n",
            "Epoch 33/300 - Training Loss: 14.7015 - Training Acc: 59.50% - Validation Acc: 58.50% - Validation Loss: 0.5698\n",
            "dL_dW max: 0.006586853057557119, min: -0.006527253163017745\n",
            "dL_db max: 0.00485673845651177, min: -0.0063828671900786095\n",
            "Epoch 34/300 - Training Loss: 14.6417 - Training Acc: 59.62% - Validation Acc: 59.00% - Validation Loss: 0.5676\n",
            "dL_dW max: 0.030255996411664196, min: -0.02123913633400739\n",
            "dL_db max: 0.00935436789351966, min: -0.013050141799199928\n",
            "Epoch 35/300 - Training Loss: 14.5841 - Training Acc: 59.88% - Validation Acc: 59.00% - Validation Loss: 0.5654\n",
            "dL_dW max: 0.004193838672607894, min: -0.005029449715648433\n",
            "dL_db max: 0.002852774705800428, min: -0.00503067488763969\n",
            "Epoch 36/300 - Training Loss: 14.5256 - Training Acc: 60.00% - Validation Acc: 59.50% - Validation Loss: 0.5633\n",
            "dL_dW max: 0.006226727573578086, min: -0.007734778134480636\n",
            "dL_db max: 0.001367522578672474, min: -0.007540039043001779\n",
            "Epoch 37/300 - Training Loss: 14.4700 - Training Acc: 60.12% - Validation Acc: 59.50% - Validation Loss: 0.5612\n",
            "dL_dW max: 0.005775627169900213, min: -0.009891065889517634\n",
            "dL_db max: 0.003663382028776185, min: -0.006780141734574725\n",
            "Epoch 38/300 - Training Loss: 14.4140 - Training Acc: 60.12% - Validation Acc: 59.50% - Validation Loss: 0.5591\n",
            "dL_dW max: 0.005238262524986912, min: -0.007478068590640058\n",
            "dL_db max: 0.003139333448910806, min: -0.005665814416163481\n",
            "Epoch 39/300 - Training Loss: 14.3606 - Training Acc: 60.25% - Validation Acc: 60.50% - Validation Loss: 0.5571\n",
            "dL_dW max: 0.004650548300601392, min: -0.008027746318400024\n",
            "dL_db max: 0.0024380126414268576, min: -0.005552394353234635\n",
            "Epoch 40/300 - Training Loss: 14.3068 - Training Acc: 60.50% - Validation Acc: 61.00% - Validation Loss: 0.5551\n",
            "dL_dW max: 0.006018440464867396, min: -0.007322498638427638\n",
            "dL_db max: 0.0019517516106938467, min: -0.006835688539960168\n",
            "Epoch 41/300 - Training Loss: 14.2535 - Training Acc: 60.62% - Validation Acc: 61.00% - Validation Loss: 0.5531\n",
            "dL_dW max: 0.006394468810550841, min: -0.008839840771168896\n",
            "dL_db max: 0.005270824750221624, min: -0.006641276400402832\n",
            "Epoch 42/300 - Training Loss: 14.2025 - Training Acc: 61.00% - Validation Acc: 61.00% - Validation Loss: 0.5511\n",
            "dL_dW max: 0.0064885844121394445, min: -0.007504022992338649\n",
            "dL_db max: 0.0018740657680141111, min: -0.004218048908233017\n",
            "Epoch 43/300 - Training Loss: 14.1522 - Training Acc: 61.00% - Validation Acc: 61.00% - Validation Loss: 0.5492\n",
            "dL_dW max: 0.006279816068977658, min: -0.013663298393248157\n",
            "dL_db max: 0.00466326452331474, min: -0.00858893251935797\n",
            "Epoch 44/300 - Training Loss: 14.1008 - Training Acc: 61.25% - Validation Acc: 61.50% - Validation Loss: 0.5474\n",
            "dL_dW max: 0.0067146506973243, min: -0.00706508109675718\n",
            "dL_db max: 0.0015339885971716015, min: -0.0074576077730446185\n",
            "Epoch 45/300 - Training Loss: 14.0531 - Training Acc: 61.38% - Validation Acc: 62.00% - Validation Loss: 0.5455\n",
            "dL_dW max: 0.006984728931683262, min: -0.004911267394898947\n",
            "dL_db max: 0.001646775104798998, min: -0.004180609454967003\n",
            "Epoch 46/300 - Training Loss: 14.0033 - Training Acc: 61.88% - Validation Acc: 62.00% - Validation Loss: 0.5438\n",
            "dL_dW max: 0.006116395451425021, min: -0.011303155604952242\n",
            "dL_db max: 0.0037142211142186543, min: -0.0040532067381202005\n",
            "Epoch 47/300 - Training Loss: 13.9546 - Training Acc: 61.88% - Validation Acc: 62.50% - Validation Loss: 0.5420\n",
            "dL_dW max: 0.012866208385420698, min: -0.00748670875145215\n",
            "dL_db max: 0.0027995117206672136, min: -0.00660488341459989\n",
            "Epoch 48/300 - Training Loss: 13.9096 - Training Acc: 62.12% - Validation Acc: 62.50% - Validation Loss: 0.5403\n",
            "dL_dW max: 0.010593194868395493, min: -0.005426884808117366\n",
            "dL_db max: 0.0025917414131998665, min: -0.005335726944665877\n",
            "Epoch 49/300 - Training Loss: 13.8611 - Training Acc: 62.38% - Validation Acc: 62.50% - Validation Loss: 0.5386\n",
            "dL_dW max: 0.008663898062596025, min: -0.00762929668932699\n",
            "dL_db max: 0.0020178156535563692, min: -0.0069686244320013545\n",
            "Epoch 50/300 - Training Loss: 13.8157 - Training Acc: 62.62% - Validation Acc: 62.50% - Validation Loss: 0.5369\n",
            "dL_dW max: 0.006610293101379806, min: -0.007393153832157172\n",
            "dL_db max: 0.0030074246318412177, min: -0.006320443033942831\n",
            "Epoch 51/300 - Training Loss: 13.7699 - Training Acc: 62.62% - Validation Acc: 62.50% - Validation Loss: 0.5352\n",
            "dL_dW max: 0.007820646025673958, min: -0.00846494402151691\n",
            "dL_db max: 0.0020048172206500363, min: -0.006154295425696788\n",
            "Epoch 52/300 - Training Loss: 13.7255 - Training Acc: 62.75% - Validation Acc: 62.50% - Validation Loss: 0.5336\n",
            "dL_dW max: 0.0068342911871694355, min: -0.011366593660559714\n",
            "dL_db max: 0.0029248347786136352, min: -0.006677213828781668\n",
            "Epoch 53/300 - Training Loss: 13.6811 - Training Acc: 63.25% - Validation Acc: 62.50% - Validation Loss: 0.5320\n",
            "dL_dW max: 0.006504051605744945, min: -0.00605761089913863\n",
            "dL_db max: 0.0030090437242223985, min: -0.0053828309907531695\n",
            "Epoch 54/300 - Training Loss: 13.6367 - Training Acc: 63.50% - Validation Acc: 62.50% - Validation Loss: 0.5304\n",
            "dL_dW max: 0.00739652518088828, min: -0.007528194619667622\n",
            "dL_db max: 0.0022616363125718872, min: -0.006558550986234889\n",
            "Epoch 55/300 - Training Loss: 13.5953 - Training Acc: 63.62% - Validation Acc: 64.00% - Validation Loss: 0.5288\n",
            "dL_dW max: 0.004764429004349041, min: -0.009053661128468313\n",
            "dL_db max: 0.004386230314310498, min: -0.008368199225615295\n",
            "Epoch 56/300 - Training Loss: 13.5510 - Training Acc: 64.00% - Validation Acc: 64.00% - Validation Loss: 0.5272\n",
            "dL_dW max: 0.00812055296706329, min: -0.006931520608833069\n",
            "dL_db max: 0.0023856379841474614, min: -0.003646970366485101\n",
            "Epoch 57/300 - Training Loss: 13.5095 - Training Acc: 64.12% - Validation Acc: 64.00% - Validation Loss: 0.5257\n",
            "dL_dW max: 0.00985666375365308, min: -0.011576747798653232\n",
            "dL_db max: 0.0019351494907760762, min: -0.005320300291912069\n",
            "Epoch 58/300 - Training Loss: 13.4686 - Training Acc: 64.25% - Validation Acc: 64.50% - Validation Loss: 0.5242\n",
            "dL_dW max: 0.005664867825765394, min: -0.006002562065036926\n",
            "dL_db max: 0.0008667391936050151, min: -0.006127367316439694\n",
            "Epoch 59/300 - Training Loss: 13.4282 - Training Acc: 64.38% - Validation Acc: 65.00% - Validation Loss: 0.5226\n",
            "dL_dW max: 0.005640216930404139, min: -0.005363863208382207\n",
            "dL_db max: 0.0009407279002483465, min: -0.004450750181370576\n",
            "Epoch 60/300 - Training Loss: 13.3869 - Training Acc: 64.62% - Validation Acc: 65.00% - Validation Loss: 0.5211\n",
            "dL_dW max: 0.007329003038012184, min: -0.007194054707029068\n",
            "dL_db max: 0.0028048856601165448, min: -0.005386848397301998\n",
            "Epoch 61/300 - Training Loss: 13.3470 - Training Acc: 64.62% - Validation Acc: 65.50% - Validation Loss: 0.5197\n",
            "dL_dW max: 0.006407507841966897, min: -0.009552811876697492\n",
            "dL_db max: 0.005132308829142229, min: -0.009143994826315973\n",
            "Epoch 62/300 - Training Loss: 13.3073 - Training Acc: 64.88% - Validation Acc: 65.50% - Validation Loss: 0.5182\n",
            "dL_dW max: 0.0059383298296071954, min: -0.009323566600271251\n",
            "dL_db max: 0.0027532575280171607, min: -0.006052773773217577\n",
            "Epoch 63/300 - Training Loss: 13.2673 - Training Acc: 65.12% - Validation Acc: 66.50% - Validation Loss: 0.5167\n",
            "dL_dW max: 0.01475594009568596, min: -0.006052481640497587\n",
            "dL_db max: 0.0042748790594432234, min: -0.006287156983152122\n",
            "Epoch 64/300 - Training Loss: 13.2292 - Training Acc: 65.62% - Validation Acc: 66.50% - Validation Loss: 0.5153\n",
            "dL_dW max: 0.0046737745164991355, min: -0.01024860320651522\n",
            "dL_db max: 0.00328045360062385, min: -0.005969390123336557\n",
            "Epoch 65/300 - Training Loss: 13.1896 - Training Acc: 65.62% - Validation Acc: 66.50% - Validation Loss: 0.5139\n",
            "dL_dW max: 0.0058471789099856265, min: -0.007299255674881223\n",
            "dL_db max: 0.0028666212639092544, min: -0.007076081866688665\n",
            "Epoch 66/300 - Training Loss: 13.1521 - Training Acc: 65.75% - Validation Acc: 66.50% - Validation Loss: 0.5125\n",
            "dL_dW max: 0.0076627871322951235, min: -0.007493893882055139\n",
            "dL_db max: 0.0010430572822907292, min: -0.004180831216062692\n",
            "Epoch 67/300 - Training Loss: 13.1136 - Training Acc: 66.12% - Validation Acc: 67.00% - Validation Loss: 0.5111\n",
            "dL_dW max: 0.026661106217671433, min: -0.013993104770454581\n",
            "dL_db max: 0.0035418048415766182, min: -0.004578000538901752\n",
            "Epoch 68/300 - Training Loss: 13.0777 - Training Acc: 66.38% - Validation Acc: 67.00% - Validation Loss: 0.5097\n",
            "dL_dW max: 0.004605971157001043, min: -0.005875893264023234\n",
            "dL_db max: 0.0018287756012507622, min: -0.005471406328331309\n",
            "Epoch 69/300 - Training Loss: 13.0403 - Training Acc: 66.62% - Validation Acc: 67.50% - Validation Loss: 0.5083\n",
            "dL_dW max: 0.005060902821009607, min: -0.009860348181380619\n",
            "dL_db max: 0.003292063203443395, min: -0.008502933508214986\n",
            "Epoch 70/300 - Training Loss: 13.0042 - Training Acc: 66.88% - Validation Acc: 68.00% - Validation Loss: 0.5069\n",
            "dL_dW max: 0.011851761308618007, min: -0.006570280309053266\n",
            "dL_db max: 0.004760432804972475, min: -0.006616984088697073\n",
            "Epoch 71/300 - Training Loss: 12.9688 - Training Acc: 66.88% - Validation Acc: 68.50% - Validation Loss: 0.5056\n",
            "dL_dW max: 0.012987015279239629, min: -0.006565040020663621\n",
            "dL_db max: 0.0021868094939790288, min: -0.004842707195332016\n",
            "Epoch 72/300 - Training Loss: 12.9328 - Training Acc: 67.25% - Validation Acc: 70.50% - Validation Loss: 0.5042\n",
            "dL_dW max: 0.007871527137957244, min: -0.008637126534076271\n",
            "dL_db max: 0.0014907534510743233, min: -0.00686092549646595\n",
            "Epoch 73/300 - Training Loss: 12.8964 - Training Acc: 67.62% - Validation Acc: 71.00% - Validation Loss: 0.5029\n",
            "dL_dW max: 0.006403017087795131, min: -0.009747847319843054\n",
            "dL_db max: 0.003528319701634682, min: -0.008272427429945049\n",
            "Epoch 74/300 - Training Loss: 12.8631 - Training Acc: 67.75% - Validation Acc: 71.50% - Validation Loss: 0.5016\n",
            "dL_dW max: 0.02559710492904124, min: -0.01985713365649546\n",
            "dL_db max: 0.010720968763821812, min: -0.011481402934840242\n",
            "Epoch 75/300 - Training Loss: 12.8299 - Training Acc: 67.88% - Validation Acc: 71.50% - Validation Loss: 0.5003\n",
            "dL_dW max: 0.030175754213034336, min: -0.015774589997454763\n",
            "dL_db max: 0.006124632136471393, min: -0.009638557719863052\n",
            "Epoch 76/300 - Training Loss: 12.7954 - Training Acc: 68.00% - Validation Acc: 71.50% - Validation Loss: 0.4990\n",
            "dL_dW max: 0.005880829794117778, min: -0.011101111181816158\n",
            "dL_db max: 0.0019892634336672447, min: -0.007014684515007971\n",
            "Epoch 77/300 - Training Loss: 12.7596 - Training Acc: 68.12% - Validation Acc: 71.50% - Validation Loss: 0.4977\n",
            "dL_dW max: 0.004288659206257911, min: -0.004309563219886859\n",
            "dL_db max: 0.001896001188673605, min: -0.006380576824144225\n",
            "Epoch 78/300 - Training Loss: 12.7267 - Training Acc: 68.12% - Validation Acc: 71.50% - Validation Loss: 0.4964\n",
            "dL_dW max: 0.006729521720317552, min: -0.0077941471808140525\n",
            "dL_db max: 0.0030242762849836107, min: -0.006568068481547869\n",
            "Epoch 79/300 - Training Loss: 12.6935 - Training Acc: 68.50% - Validation Acc: 72.00% - Validation Loss: 0.4952\n",
            "dL_dW max: 0.008806546868342986, min: -0.009882876847981375\n",
            "dL_db max: 0.0012498718951434165, min: -0.006296626679592452\n",
            "Epoch 80/300 - Training Loss: 12.6602 - Training Acc: 68.62% - Validation Acc: 72.00% - Validation Loss: 0.4939\n",
            "dL_dW max: 0.007557017677222118, min: -0.006225879589422378\n",
            "dL_db max: 0.002044115788950127, min: -0.004231083602696419\n",
            "Epoch 81/300 - Training Loss: 12.6287 - Training Acc: 69.00% - Validation Acc: 72.00% - Validation Loss: 0.4927\n",
            "dL_dW max: 0.005436381517226686, min: -0.007541827552285197\n",
            "dL_db max: 0.002118881700610679, min: -0.005647600685753749\n",
            "Epoch 82/300 - Training Loss: 12.5969 - Training Acc: 69.50% - Validation Acc: 72.00% - Validation Loss: 0.4914\n",
            "dL_dW max: 0.009671756468790982, min: -0.010715447036130189\n",
            "dL_db max: 0.0013411752987285963, min: -0.009769023206031905\n",
            "Epoch 83/300 - Training Loss: 12.5631 - Training Acc: 69.88% - Validation Acc: 72.00% - Validation Loss: 0.4902\n",
            "dL_dW max: 0.0048350925020163695, min: -0.010411286165690417\n",
            "dL_db max: 0.0035394073390094282, min: -0.008724858277601692\n",
            "Epoch 84/300 - Training Loss: 12.5317 - Training Acc: 70.25% - Validation Acc: 72.50% - Validation Loss: 0.4890\n",
            "dL_dW max: 0.01004323979877111, min: -0.00811154341043131\n",
            "dL_db max: 0.0008313626313968937, min: -0.002966954725386796\n",
            "Epoch 85/300 - Training Loss: 12.4984 - Training Acc: 70.62% - Validation Acc: 72.50% - Validation Loss: 0.4878\n",
            "dL_dW max: 0.007622608505891157, min: -0.005416782001843458\n",
            "dL_db max: 0.0023497215211937686, min: -0.004089327500499507\n",
            "Epoch 86/300 - Training Loss: 12.4666 - Training Acc: 71.00% - Validation Acc: 72.00% - Validation Loss: 0.4866\n",
            "dL_dW max: 0.00334342381075947, min: -0.0061797883970911025\n",
            "dL_db max: 0.001811097170791495, min: -0.0059487077771442284\n",
            "Epoch 87/300 - Training Loss: 12.4379 - Training Acc: 71.25% - Validation Acc: 72.00% - Validation Loss: 0.4854\n",
            "dL_dW max: 0.009414043993595998, min: -0.009803856237234775\n",
            "dL_db max: 0.0012962154903408503, min: -0.007514229407990982\n",
            "Epoch 88/300 - Training Loss: 12.4058 - Training Acc: 71.62% - Validation Acc: 72.50% - Validation Loss: 0.4842\n",
            "dL_dW max: 0.005275799077578316, min: -0.008513972226113\n",
            "dL_db max: 0.0015891844856564645, min: -0.006091453094511696\n",
            "Epoch 89/300 - Training Loss: 12.3752 - Training Acc: 72.12% - Validation Acc: 73.00% - Validation Loss: 0.4830\n",
            "dL_dW max: 0.005926103872647177, min: -0.005532800733242364\n",
            "dL_db max: 0.002392483533344175, min: -0.006928627923618138\n",
            "Epoch 90/300 - Training Loss: 12.3426 - Training Acc: 72.25% - Validation Acc: 73.50% - Validation Loss: 0.4818\n",
            "dL_dW max: 0.005262286513603346, min: -0.012066097398062822\n",
            "dL_db max: 0.0018081108381070592, min: -0.006160383455116379\n",
            "Epoch 91/300 - Training Loss: 12.3132 - Training Acc: 72.25% - Validation Acc: 73.50% - Validation Loss: 0.4807\n",
            "dL_dW max: 0.006529200967274427, min: -0.007015472939072808\n",
            "dL_db max: 0.0012346999665465556, min: -0.006461320979365434\n",
            "Epoch 92/300 - Training Loss: 12.2836 - Training Acc: 72.50% - Validation Acc: 73.50% - Validation Loss: 0.4795\n",
            "dL_dW max: 0.0067562786233006985, min: -0.00726430069254741\n",
            "dL_db max: 0.0013741115203312099, min: -0.005480275817012908\n",
            "Epoch 93/300 - Training Loss: 12.2525 - Training Acc: 72.62% - Validation Acc: 74.00% - Validation Loss: 0.4783\n",
            "dL_dW max: 0.005143779976005387, min: -0.007821218217490277\n",
            "dL_db max: 0.0018019765468748718, min: -0.007728330678293042\n",
            "Epoch 94/300 - Training Loss: 12.2226 - Training Acc: 72.75% - Validation Acc: 74.50% - Validation Loss: 0.4772\n",
            "dL_dW max: 0.004189617248149417, min: -0.005213287044048812\n",
            "dL_db max: 0.0015333055069059309, min: -0.004891896327338727\n",
            "Epoch 95/300 - Training Loss: 12.1929 - Training Acc: 72.88% - Validation Acc: 75.00% - Validation Loss: 0.4761\n",
            "dL_dW max: 0.004198796872006868, min: -0.007559752154715195\n",
            "dL_db max: 0.0032350948447728073, min: -0.009650888458886644\n",
            "Epoch 96/300 - Training Loss: 12.1642 - Training Acc: 73.25% - Validation Acc: 75.00% - Validation Loss: 0.4749\n",
            "dL_dW max: 0.003816302624402217, min: -0.004269574720403115\n",
            "dL_db max: 0.000960676984251802, min: -0.0049627353473007795\n",
            "Epoch 97/300 - Training Loss: 12.1349 - Training Acc: 73.50% - Validation Acc: 75.00% - Validation Loss: 0.4738\n",
            "dL_dW max: 0.0049081171658621045, min: -0.005433378142200887\n",
            "dL_db max: 0.0014123015178212663, min: -0.005642362060874687\n",
            "Epoch 98/300 - Training Loss: 12.1053 - Training Acc: 73.88% - Validation Acc: 75.50% - Validation Loss: 0.4727\n",
            "dL_dW max: 0.007645646876894104, min: -0.006087003629480994\n",
            "dL_db max: 0.0016229391698974892, min: -0.00484172495111709\n",
            "Epoch 99/300 - Training Loss: 12.0754 - Training Acc: 74.12% - Validation Acc: 76.50% - Validation Loss: 0.4716\n",
            "dL_dW max: 0.004012237202676367, min: -0.006516223343583929\n",
            "dL_db max: 0.0022434633344954468, min: -0.0049765174602270515\n",
            "Epoch 100/300 - Training Loss: 12.0458 - Training Acc: 74.38% - Validation Acc: 76.50% - Validation Loss: 0.4705\n",
            "dL_dW max: 0.007287477059688738, min: -0.007645945599605579\n",
            "dL_db max: 0.0012989870309219583, min: -0.006021161112981563\n",
            "Epoch 101/300 - Training Loss: 12.0176 - Training Acc: 74.88% - Validation Acc: 78.00% - Validation Loss: 0.4694\n",
            "dL_dW max: 0.0074170759349068945, min: -0.007555995485845293\n",
            "dL_db max: 0.000979741829348871, min: -0.007100891073467695\n",
            "Epoch 102/300 - Training Loss: 11.9882 - Training Acc: 75.00% - Validation Acc: 78.00% - Validation Loss: 0.4683\n",
            "dL_dW max: 0.004310668200406427, min: -0.008781492417328531\n",
            "dL_db max: 0.0018481812737327161, min: -0.005875648888846286\n",
            "Epoch 103/300 - Training Loss: 11.9616 - Training Acc: 75.50% - Validation Acc: 78.50% - Validation Loss: 0.4672\n",
            "dL_dW max: 0.007262295810752984, min: -0.006453878235510138\n",
            "dL_db max: 0.0009175424133216245, min: -0.0031713175308916066\n",
            "Epoch 104/300 - Training Loss: 11.9328 - Training Acc: 75.75% - Validation Acc: 79.50% - Validation Loss: 0.4661\n",
            "dL_dW max: 0.019036186087822086, min: -0.015027825310948557\n",
            "dL_db max: 0.009781283533285527, min: -0.010148353785923794\n",
            "Epoch 105/300 - Training Loss: 11.9051 - Training Acc: 76.38% - Validation Acc: 81.00% - Validation Loss: 0.4651\n",
            "dL_dW max: 0.006652562246605714, min: -0.0070040252428980355\n",
            "dL_db max: 0.0015648991139157748, min: -0.0052417400164671\n",
            "Epoch 106/300 - Training Loss: 11.8779 - Training Acc: 76.50% - Validation Acc: 81.50% - Validation Loss: 0.4640\n",
            "dL_dW max: 0.004441405054600563, min: -0.0076161091996154\n",
            "dL_db max: 0.0016662396255928292, min: -0.0053416190754222714\n",
            "Epoch 107/300 - Training Loss: 11.8467 - Training Acc: 76.75% - Validation Acc: 81.50% - Validation Loss: 0.4630\n",
            "dL_dW max: 0.011867421054437487, min: -0.01129930843088139\n",
            "dL_db max: 0.001614317004628034, min: -0.010408591052818172\n",
            "Epoch 108/300 - Training Loss: 11.8223 - Training Acc: 77.00% - Validation Acc: 82.00% - Validation Loss: 0.4619\n",
            "dL_dW max: 0.030824408878389498, min: -0.0242332914002108\n",
            "dL_db max: 0.010650399634097564, min: -0.013308898238510673\n",
            "Epoch 109/300 - Training Loss: 11.7952 - Training Acc: 77.00% - Validation Acc: 82.00% - Validation Loss: 0.4609\n",
            "dL_dW max: 0.008906290834614855, min: -0.007130714475531332\n",
            "dL_db max: 0.002035915915812207, min: -0.005745179849719742\n",
            "Epoch 110/300 - Training Loss: 11.7674 - Training Acc: 77.38% - Validation Acc: 82.00% - Validation Loss: 0.4599\n",
            "dL_dW max: 0.007178649360655878, min: -0.006737363989393345\n",
            "dL_db max: 0.003310514324725213, min: -0.0065538313840927485\n",
            "Epoch 111/300 - Training Loss: 11.7396 - Training Acc: 77.62% - Validation Acc: 82.00% - Validation Loss: 0.4588\n",
            "dL_dW max: 0.008063582665444631, min: -0.007920302689999645\n",
            "dL_db max: 0.00153946952377268, min: -0.00935869055219003\n",
            "Epoch 112/300 - Training Loss: 11.7118 - Training Acc: 78.38% - Validation Acc: 82.00% - Validation Loss: 0.4578\n",
            "dL_dW max: 0.005859031175053128, min: -0.0057840654187116805\n",
            "dL_db max: 0.0015784476842337029, min: -0.00719914395607775\n",
            "Epoch 113/300 - Training Loss: 11.6854 - Training Acc: 78.75% - Validation Acc: 82.00% - Validation Loss: 0.4567\n",
            "dL_dW max: 0.007762492216399321, min: -0.005570264997639757\n",
            "dL_db max: 0.003343339730804245, min: -0.00886432096087875\n",
            "Epoch 114/300 - Training Loss: 11.6603 - Training Acc: 79.38% - Validation Acc: 82.00% - Validation Loss: 0.4557\n",
            "dL_dW max: 0.01726694810605772, min: -0.006881641411057994\n",
            "dL_db max: 0.004276645784706758, min: -0.006849796610828856\n",
            "Epoch 115/300 - Training Loss: 11.6337 - Training Acc: 79.62% - Validation Acc: 83.00% - Validation Loss: 0.4546\n",
            "dL_dW max: 0.0047291718960728495, min: -0.00802911784634534\n",
            "dL_db max: 0.002462477187358302, min: -0.0069749396675621265\n",
            "Epoch 116/300 - Training Loss: 11.6057 - Training Acc: 80.38% - Validation Acc: 83.00% - Validation Loss: 0.4536\n",
            "dL_dW max: 0.007380737606298838, min: -0.0075943742204042704\n",
            "dL_db max: 0.0005690805027698965, min: -0.008185490185139865\n",
            "Epoch 117/300 - Training Loss: 11.5798 - Training Acc: 80.88% - Validation Acc: 83.00% - Validation Loss: 0.4526\n",
            "dL_dW max: 0.007063136463107992, min: -0.007173029813860447\n",
            "dL_db max: 0.001958501093921955, min: -0.004853107606012742\n",
            "Epoch 118/300 - Training Loss: 11.5544 - Training Acc: 81.00% - Validation Acc: 83.00% - Validation Loss: 0.4515\n",
            "dL_dW max: 0.009737950762011241, min: -0.009851532192393335\n",
            "dL_db max: 0.0006843425329697363, min: -0.009716001858647641\n",
            "Epoch 119/300 - Training Loss: 11.5270 - Training Acc: 81.25% - Validation Acc: 83.50% - Validation Loss: 0.4505\n",
            "dL_dW max: 0.008291501797573974, min: -0.007867121013308895\n",
            "dL_db max: 0.0012189971962912683, min: -0.005093873768002849\n",
            "Epoch 120/300 - Training Loss: 11.5018 - Training Acc: 81.75% - Validation Acc: 83.50% - Validation Loss: 0.4495\n",
            "dL_dW max: 0.008070182956739416, min: -0.008395339258662109\n",
            "dL_db max: 0.0012308727925143153, min: -0.009575910629576608\n",
            "Epoch 121/300 - Training Loss: 11.4765 - Training Acc: 81.88% - Validation Acc: 83.50% - Validation Loss: 0.4485\n",
            "dL_dW max: 0.005986986324689852, min: -0.00752053749050291\n",
            "dL_db max: 0.0029147826527967484, min: -0.007764079204261325\n",
            "Epoch 122/300 - Training Loss: 11.4521 - Training Acc: 82.50% - Validation Acc: 84.00% - Validation Loss: 0.4475\n",
            "dL_dW max: 0.006902331133865971, min: -0.009476347110705571\n",
            "dL_db max: 0.003579616566365636, min: -0.009602726954337486\n",
            "Epoch 123/300 - Training Loss: 11.4265 - Training Acc: 83.00% - Validation Acc: 85.00% - Validation Loss: 0.4465\n",
            "dL_dW max: 0.004751725923041194, min: -0.007202064525306924\n",
            "dL_db max: 0.0017313564341698226, min: -0.00807635158949023\n",
            "Epoch 124/300 - Training Loss: 11.4011 - Training Acc: 83.12% - Validation Acc: 85.00% - Validation Loss: 0.4455\n",
            "dL_dW max: 0.01219229449245144, min: -0.005871446330440133\n",
            "dL_db max: 0.0016775640291395901, min: -0.005625940642080994\n",
            "Epoch 125/300 - Training Loss: 11.3773 - Training Acc: 83.88% - Validation Acc: 85.50% - Validation Loss: 0.4445\n",
            "dL_dW max: 0.006047642017307317, min: -0.0045485602452103695\n",
            "dL_db max: 0.0023346116987760985, min: -0.0030386015358242987\n",
            "Epoch 126/300 - Training Loss: 11.3509 - Training Acc: 84.25% - Validation Acc: 85.50% - Validation Loss: 0.4435\n",
            "dL_dW max: 0.007359099871385515, min: -0.007110713031143216\n",
            "dL_db max: 0.0006758052661237925, min: -0.004187397214611646\n",
            "Epoch 127/300 - Training Loss: 11.3261 - Training Acc: 84.88% - Validation Acc: 85.50% - Validation Loss: 0.4425\n",
            "dL_dW max: 0.006628994451297382, min: -0.01156683376681811\n",
            "dL_db max: 0.001969329602553885, min: -0.006709685399064315\n",
            "Epoch 128/300 - Training Loss: 11.3022 - Training Acc: 85.12% - Validation Acc: 86.50% - Validation Loss: 0.4415\n",
            "dL_dW max: 0.0069604783186847635, min: -0.006684312602121232\n",
            "dL_db max: 0.0014886180480701166, min: -0.007483346869093561\n",
            "Epoch 129/300 - Training Loss: 11.2769 - Training Acc: 85.38% - Validation Acc: 86.50% - Validation Loss: 0.4406\n",
            "dL_dW max: 0.006944675236161725, min: -0.0076690527160201884\n",
            "dL_db max: 0.0009210422838452561, min: -0.005929219010501431\n",
            "Epoch 130/300 - Training Loss: 11.2540 - Training Acc: 85.62% - Validation Acc: 86.50% - Validation Loss: 0.4396\n",
            "dL_dW max: 0.010674610712815436, min: -0.010868663573883202\n",
            "dL_db max: 0.0012547279746601351, min: -0.006896386809319317\n",
            "Epoch 131/300 - Training Loss: 11.2293 - Training Acc: 86.00% - Validation Acc: 86.50% - Validation Loss: 0.4386\n",
            "dL_dW max: 0.009269983774982735, min: -0.010358931390566149\n",
            "dL_db max: 0.0017290260212056805, min: -0.010574545996777682\n",
            "Epoch 132/300 - Training Loss: 11.2036 - Training Acc: 86.12% - Validation Acc: 86.50% - Validation Loss: 0.4377\n",
            "dL_dW max: 0.010052635297849586, min: -0.008793090281952194\n",
            "dL_db max: 0.001192351765134064, min: -0.004087764813028524\n",
            "Epoch 133/300 - Training Loss: 11.1805 - Training Acc: 86.50% - Validation Acc: 87.00% - Validation Loss: 0.4367\n",
            "dL_dW max: 0.00460809376203305, min: -0.005988251820740562\n",
            "dL_db max: 0.0016774529105335477, min: -0.006879378710766768\n",
            "Epoch 134/300 - Training Loss: 11.1557 - Training Acc: 86.75% - Validation Acc: 87.00% - Validation Loss: 0.4357\n",
            "dL_dW max: 0.009825460250480977, min: -0.009789383596580934\n",
            "dL_db max: 0.0011918093336650425, min: -0.00930779325062151\n",
            "Epoch 135/300 - Training Loss: 11.1308 - Training Acc: 86.88% - Validation Acc: 87.00% - Validation Loss: 0.4347\n",
            "dL_dW max: 0.006387151795688007, min: -0.006387206544720089\n",
            "dL_db max: 0.0005348267899133873, min: -0.008032195233207454\n",
            "Epoch 136/300 - Training Loss: 11.1058 - Training Acc: 87.12% - Validation Acc: 87.50% - Validation Loss: 0.4338\n",
            "dL_dW max: 0.00503280865214009, min: -0.005667681091544861\n",
            "dL_db max: 0.0014487333561849253, min: -0.007815453849879698\n",
            "Epoch 137/300 - Training Loss: 11.0837 - Training Acc: 87.38% - Validation Acc: 88.00% - Validation Loss: 0.4328\n",
            "dL_dW max: 0.009912095292963531, min: -0.0065835506024097\n",
            "dL_db max: 0.003557837072420326, min: -0.00845488063424055\n",
            "Epoch 138/300 - Training Loss: 11.0600 - Training Acc: 87.50% - Validation Acc: 88.50% - Validation Loss: 0.4318\n",
            "dL_dW max: 0.027778951331918506, min: -0.017732325153856438\n",
            "dL_db max: 0.009853824029546404, min: -0.00960963894346275\n",
            "Epoch 139/300 - Training Loss: 11.0362 - Training Acc: 87.62% - Validation Acc: 88.50% - Validation Loss: 0.4309\n",
            "dL_dW max: 0.009109412766213252, min: -0.008000576358980714\n",
            "dL_db max: 0.0010686152597486382, min: -0.004304598216703071\n",
            "Epoch 140/300 - Training Loss: 11.0110 - Training Acc: 87.62% - Validation Acc: 88.50% - Validation Loss: 0.4299\n",
            "dL_dW max: 0.004354226079326888, min: -0.005733251235537617\n",
            "dL_db max: 0.0011915144856340524, min: -0.006626349765401116\n",
            "Epoch 141/300 - Training Loss: 10.9884 - Training Acc: 87.88% - Validation Acc: 88.50% - Validation Loss: 0.4290\n",
            "dL_dW max: 0.012248355289085376, min: -0.011011971251401817\n",
            "dL_db max: 0.0006301571453023473, min: -0.008067157972009667\n",
            "Epoch 142/300 - Training Loss: 10.9656 - Training Acc: 88.12% - Validation Acc: 88.50% - Validation Loss: 0.4280\n",
            "dL_dW max: 0.00838621044889816, min: -0.008929206913495453\n",
            "dL_db max: 0.0016331766487818612, min: -0.009915667999209192\n",
            "Epoch 143/300 - Training Loss: 10.9399 - Training Acc: 88.00% - Validation Acc: 89.50% - Validation Loss: 0.4270\n",
            "dL_dW max: 0.008215582054216856, min: -0.008183248207297022\n",
            "dL_db max: 0.0005499646791909166, min: -0.008732077518153034\n",
            "Epoch 144/300 - Training Loss: 10.9190 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4261\n",
            "dL_dW max: 0.004444461810835966, min: -0.004542233604302752\n",
            "dL_db max: 0.0020418691700125166, min: -0.006879944461722848\n",
            "Epoch 145/300 - Training Loss: 10.8955 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4251\n",
            "dL_dW max: 0.012194038290714533, min: -0.011670998465880073\n",
            "dL_db max: 0.0009442692122529644, min: -0.007958587130609453\n",
            "Epoch 146/300 - Training Loss: 10.8718 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4242\n",
            "dL_dW max: 0.011176345422864263, min: -0.010216005264577731\n",
            "dL_db max: 0.0004412731107695233, min: -0.005686130370969238\n",
            "Epoch 147/300 - Training Loss: 10.8505 - Training Acc: 88.38% - Validation Acc: 89.50% - Validation Loss: 0.4232\n",
            "dL_dW max: 0.00848522649055976, min: -0.0051319041055162655\n",
            "dL_db max: 0.001881493841446697, min: -0.004429212989885138\n",
            "Epoch 148/300 - Training Loss: 10.8263 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4223\n",
            "dL_dW max: 0.007669648597573295, min: -0.007085197641079102\n",
            "dL_db max: 0.0009210028555321941, min: -0.006593911009837123\n",
            "Epoch 149/300 - Training Loss: 10.8020 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4213\n",
            "dL_dW max: 0.008974516727213548, min: -0.005958669233748911\n",
            "dL_db max: 0.00172251417997459, min: -0.004232162166268944\n",
            "Epoch 150/300 - Training Loss: 10.7802 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4204\n",
            "dL_dW max: 0.004572024413136898, min: -0.007479807354260551\n",
            "dL_db max: 0.0010468285012337217, min: -0.004484582575998166\n",
            "Epoch 151/300 - Training Loss: 10.7586 - Training Acc: 88.50% - Validation Acc: 90.00% - Validation Loss: 0.4195\n",
            "dL_dW max: 0.01702308499898541, min: -0.009204256978365492\n",
            "dL_db max: 0.0032912975358288157, min: -0.008780306035139766\n",
            "Epoch 152/300 - Training Loss: 10.7369 - Training Acc: 88.50% - Validation Acc: 90.00% - Validation Loss: 0.4185\n",
            "dL_dW max: 0.007385504370077801, min: -0.007464983069586137\n",
            "dL_db max: 0.0004584130878159008, min: -0.007886713078028984\n",
            "Epoch 153/300 - Training Loss: 10.7137 - Training Acc: 88.50% - Validation Acc: 89.50% - Validation Loss: 0.4176\n",
            "dL_dW max: 0.00735415084647961, min: -0.006577173478403056\n",
            "dL_db max: 0.0018292033919998475, min: -0.00593864464981189\n",
            "Epoch 154/300 - Training Loss: 10.6911 - Training Acc: 88.75% - Validation Acc: 89.50% - Validation Loss: 0.4167\n",
            "dL_dW max: 0.01062177232377832, min: -0.0071468993775280374\n",
            "dL_db max: 0.0019465317560706838, min: -0.004683666671199878\n",
            "Epoch 155/300 - Training Loss: 10.6692 - Training Acc: 89.00% - Validation Acc: 90.00% - Validation Loss: 0.4158\n",
            "dL_dW max: 0.010398688199149226, min: -0.009547747437245815\n",
            "dL_db max: 0.0006815420001676717, min: -0.009662199401143596\n",
            "Epoch 156/300 - Training Loss: 10.6469 - Training Acc: 89.25% - Validation Acc: 90.00% - Validation Loss: 0.4149\n",
            "dL_dW max: 0.005660239708897671, min: -0.005073797621629691\n",
            "dL_db max: 0.0021067829368335562, min: -0.008350181733183477\n",
            "Epoch 157/300 - Training Loss: 10.6255 - Training Acc: 89.25% - Validation Acc: 90.00% - Validation Loss: 0.4140\n",
            "dL_dW max: 0.00771958126247252, min: -0.006242329577844767\n",
            "dL_db max: 0.0004961893655944083, min: -0.004681203789388983\n",
            "Epoch 158/300 - Training Loss: 10.6050 - Training Acc: 89.38% - Validation Acc: 90.00% - Validation Loss: 0.4131\n",
            "dL_dW max: 0.03473922094357681, min: -0.024444203170171763\n",
            "dL_db max: 0.009472141535421193, min: -0.011817639936700554\n",
            "Epoch 159/300 - Training Loss: 10.5830 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4122\n",
            "dL_dW max: 0.008349538582002186, min: -0.007458816645207076\n",
            "dL_db max: 0.0007990179795088222, min: -0.006869986091767982\n",
            "Epoch 160/300 - Training Loss: 10.5611 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4113\n",
            "dL_dW max: 0.0061765987410187136, min: -0.006368023416599766\n",
            "dL_db max: 0.0009154823363300312, min: -0.0049124019213327135\n",
            "Epoch 161/300 - Training Loss: 10.5413 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4104\n",
            "dL_dW max: 0.012620849079169182, min: -0.012125045855104018\n",
            "dL_db max: 0.00039795375623050816, min: -0.008961693544794966\n",
            "Epoch 162/300 - Training Loss: 10.5216 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4095\n",
            "dL_dW max: 0.007004383711097382, min: -0.006628302737133742\n",
            "dL_db max: 0.0006084327065299378, min: -0.006145894922259192\n",
            "Epoch 163/300 - Training Loss: 10.5002 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4086\n",
            "dL_dW max: 0.008082692374393892, min: -0.005091343799935331\n",
            "dL_db max: 0.0035520669130944277, min: -0.007686391807009847\n",
            "Epoch 164/300 - Training Loss: 10.4794 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4077\n",
            "dL_dW max: 0.008285148196328697, min: -0.007994773115406138\n",
            "dL_db max: 0.0013531203164611726, min: -0.007513829220041863\n",
            "Epoch 165/300 - Training Loss: 10.4560 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4069\n",
            "dL_dW max: 0.010435673866661364, min: -0.007079997466244278\n",
            "dL_db max: 0.0009290499086625982, min: -0.00448497552486691\n",
            "Epoch 166/300 - Training Loss: 10.4348 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4060\n",
            "dL_dW max: 0.014491277139313436, min: -0.012879545971393036\n",
            "dL_db max: 0.0009833843057246749, min: -0.010355017438632248\n",
            "Epoch 167/300 - Training Loss: 10.4153 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4051\n",
            "dL_dW max: 0.01834429162031446, min: -0.010669028392515378\n",
            "dL_db max: 0.004546340673667554, min: -0.007772784798773077\n",
            "Epoch 168/300 - Training Loss: 10.3955 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4043\n",
            "dL_dW max: 0.009843425554421354, min: -0.007240626714457117\n",
            "dL_db max: 0.0012269185990150216, min: -0.005152977870618503\n",
            "Epoch 169/300 - Training Loss: 10.3741 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4034\n",
            "dL_dW max: 0.009736049877915235, min: -0.009388988319354034\n",
            "dL_db max: 0.0018341448416718054, min: -0.006274152165603674\n",
            "Epoch 170/300 - Training Loss: 10.3526 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4026\n",
            "dL_dW max: 0.007147561009595394, min: -0.005080370407624992\n",
            "dL_db max: 0.000597056824653598, min: -0.004399078982847068\n",
            "Epoch 171/300 - Training Loss: 10.3329 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4017\n",
            "dL_dW max: 0.010416158571321554, min: -0.008262473455047997\n",
            "dL_db max: 0.0020921603397080967, min: -0.007253135369059093\n",
            "Epoch 172/300 - Training Loss: 10.3129 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4008\n",
            "dL_dW max: 0.011105367709391759, min: -0.004645916185306126\n",
            "dL_db max: 0.0029159076501878035, min: -0.005666490614807037\n",
            "Epoch 173/300 - Training Loss: 10.2932 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4000\n",
            "dL_dW max: 0.00566831389470071, min: -0.008713913562851715\n",
            "dL_db max: 0.0006403941704780348, min: -0.00823729636061507\n",
            "Epoch 174/300 - Training Loss: 10.2718 - Training Acc: 90.00% - Validation Acc: 90.50% - Validation Loss: 0.3991\n",
            "dL_dW max: 0.012690827119288752, min: -0.011872683549533455\n",
            "dL_db max: 0.0014780189759256318, min: -0.01228490779070369\n",
            "Epoch 175/300 - Training Loss: 10.2526 - Training Acc: 90.25% - Validation Acc: 90.50% - Validation Loss: 0.3983\n",
            "dL_dW max: 0.00659875478049154, min: -0.008407206248421482\n",
            "dL_db max: 0.0016184044658006922, min: -0.007442051140129587\n",
            "Epoch 176/300 - Training Loss: 10.2313 - Training Acc: 90.25% - Validation Acc: 91.00% - Validation Loss: 0.3974\n",
            "dL_dW max: 0.008988248170776246, min: -0.008362777603800371\n",
            "dL_db max: 0.0007798270140600612, min: -0.012020129258953604\n",
            "Epoch 177/300 - Training Loss: 10.2122 - Training Acc: 90.25% - Validation Acc: 91.00% - Validation Loss: 0.3965\n",
            "dL_dW max: 0.007027234366341717, min: -0.006905112753476191\n",
            "dL_db max: 0.0025322917943199665, min: -0.007882866527319295\n",
            "Epoch 178/300 - Training Loss: 10.1928 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3957\n",
            "dL_dW max: 0.0355809345131776, min: -0.023412869403170614\n",
            "dL_db max: 0.007023627708876031, min: -0.0076283492731069906\n",
            "Epoch 179/300 - Training Loss: 10.1737 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3949\n",
            "dL_dW max: 0.007269748620874962, min: -0.006533239612459261\n",
            "dL_db max: 0.0005983418165618711, min: -0.008543669969870889\n",
            "Epoch 180/300 - Training Loss: 10.1519 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3940\n",
            "dL_dW max: 0.007889832059209423, min: -0.006387190090176233\n",
            "dL_db max: 0.0018313536000890325, min: -0.00543776688039741\n",
            "Epoch 181/300 - Training Loss: 10.1319 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3932\n",
            "dL_dW max: 0.008296659202774479, min: -0.007975795523092853\n",
            "dL_db max: 0.0013912111234338311, min: -0.008999364757977147\n",
            "Epoch 182/300 - Training Loss: 10.1127 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3924\n",
            "dL_dW max: 0.006587058728083783, min: -0.007101729985764363\n",
            "dL_db max: 0.0011329962953072596, min: -0.011364240917977714\n",
            "Epoch 183/300 - Training Loss: 10.0951 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3915\n",
            "dL_dW max: 0.05754239119956689, min: -0.04231915831121594\n",
            "dL_db max: 0.02001185341359843, min: -0.011988538448989354\n",
            "Epoch 184/300 - Training Loss: 10.0760 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3907\n",
            "dL_dW max: 0.008266380799931645, min: -0.007818073495095761\n",
            "dL_db max: 0.0005822076388067165, min: -0.009158713339504484\n",
            "Epoch 185/300 - Training Loss: 10.0535 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3899\n",
            "dL_dW max: 0.009775996255354535, min: -0.0077775294009180575\n",
            "dL_db max: 0.00041426588964676054, min: -0.005234097866776633\n",
            "Epoch 186/300 - Training Loss: 10.0359 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3891\n",
            "dL_dW max: 0.020182246585293863, min: -0.008183075904638275\n",
            "dL_db max: 0.0012063962508254583, min: -0.005291274355053259\n",
            "Epoch 187/300 - Training Loss: 10.0160 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3883\n",
            "dL_dW max: 0.007431208596493809, min: -0.006772475932448286\n",
            "dL_db max: 0.0015707364975294218, min: -0.010822287672533208\n",
            "Epoch 188/300 - Training Loss: 9.9962 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3875\n",
            "dL_dW max: 0.014863192967764042, min: -0.0140051011393207\n",
            "dL_db max: 0.0006990139768297748, min: -0.008945793713260595\n",
            "Epoch 189/300 - Training Loss: 9.9776 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3867\n",
            "dL_dW max: 0.014787125156660188, min: -0.006041123774792651\n",
            "dL_db max: 0.0026691512055697785, min: -0.0044690320466735595\n",
            "Epoch 190/300 - Training Loss: 9.9581 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3859\n",
            "dL_dW max: 0.007357080559421059, min: -0.00821430143146228\n",
            "dL_db max: 0.000492330971214714, min: -0.006479929689292713\n",
            "Epoch 191/300 - Training Loss: 9.9375 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3851\n",
            "dL_dW max: 0.007506207604192762, min: -0.005853316455031251\n",
            "dL_db max: 0.001993814275438168, min: -0.006907925917150605\n",
            "Epoch 192/300 - Training Loss: 9.9192 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3843\n",
            "dL_dW max: 0.011476854390777015, min: -0.009442293044278951\n",
            "dL_db max: 0.0012940842818144384, min: -0.010177523610961097\n",
            "Epoch 193/300 - Training Loss: 9.8998 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3835\n",
            "dL_dW max: 0.009935500656736818, min: -0.009651736989176663\n",
            "dL_db max: 0.0011049868695728942, min: -0.007909050723694473\n",
            "Epoch 194/300 - Training Loss: 9.8816 - Training Acc: 90.50% - Validation Acc: 91.50% - Validation Loss: 0.3828\n",
            "dL_dW max: 0.009714587983540033, min: -0.00935077817084161\n",
            "dL_db max: 0.0005327976765901434, min: -0.007227622916075955\n",
            "Epoch 195/300 - Training Loss: 9.8619 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3820\n",
            "dL_dW max: 0.016650374564413876, min: -0.008353568036563092\n",
            "dL_db max: 0.00606252713339033, min: -0.008637250439858933\n",
            "Epoch 196/300 - Training Loss: 9.8446 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3812\n",
            "dL_dW max: 0.006884340580572455, min: -0.006562178285470753\n",
            "dL_db max: 0.0005927067477660762, min: -0.006088850197695875\n",
            "Epoch 197/300 - Training Loss: 9.8228 - Training Acc: 90.75% - Validation Acc: 91.50% - Validation Loss: 0.3804\n",
            "dL_dW max: 0.009806292761968321, min: -0.006171927179190602\n",
            "dL_db max: 0.0025311994489785276, min: -0.006218703861783079\n",
            "Epoch 198/300 - Training Loss: 9.8058 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3796\n",
            "dL_dW max: 0.009596327552004047, min: -0.006139274205922166\n",
            "dL_db max: 0.002452813443745518, min: -0.005859636360040424\n",
            "Epoch 199/300 - Training Loss: 9.7861 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3789\n",
            "dL_dW max: 0.010518640436380776, min: -0.009442002035882024\n",
            "dL_db max: 0.001005311089393089, min: -0.013309528428688386\n",
            "Epoch 200/300 - Training Loss: 9.7658 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3781\n",
            "dL_dW max: 0.010364451186957268, min: -0.008855543540630952\n",
            "dL_db max: 0.0004970734098351557, min: -0.0062723373101670685\n",
            "Epoch 201/300 - Training Loss: 9.7482 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3773\n",
            "dL_dW max: 0.012065324219083975, min: -0.010973003444248355\n",
            "dL_db max: 0.0006209224951878275, min: -0.009802790428992845\n",
            "Epoch 202/300 - Training Loss: 9.7297 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3765\n",
            "dL_dW max: 0.010211391171036786, min: -0.009510058398542805\n",
            "dL_db max: 0.00029131355355657327, min: -0.005593955389847027\n",
            "Epoch 203/300 - Training Loss: 9.7090 - Training Acc: 90.62% - Validation Acc: 92.00% - Validation Loss: 0.3758\n",
            "dL_dW max: 0.014648743978088664, min: -0.013493161705167548\n",
            "dL_db max: 0.0012211892392187812, min: -0.011831280383544663\n",
            "Epoch 204/300 - Training Loss: 9.6923 - Training Acc: 90.62% - Validation Acc: 92.00% - Validation Loss: 0.3750\n",
            "dL_dW max: 0.009155578546377083, min: -0.008449932446742556\n",
            "dL_db max: 0.0009901354639995788, min: -0.008501822157735518\n",
            "Epoch 205/300 - Training Loss: 9.6732 - Training Acc: 90.50% - Validation Acc: 92.00% - Validation Loss: 0.3743\n",
            "dL_dW max: 0.008211971826017744, min: -0.0073343932285855565\n",
            "dL_db max: 0.0012620231809877041, min: -0.005805695323551015\n",
            "Epoch 206/300 - Training Loss: 9.6528 - Training Acc: 90.38% - Validation Acc: 92.00% - Validation Loss: 0.3735\n",
            "dL_dW max: 0.009244305639194825, min: -0.008144637685187566\n",
            "dL_db max: 0.0006018196390943478, min: -0.0058923218731844135\n",
            "Epoch 207/300 - Training Loss: 9.6343 - Training Acc: 90.38% - Validation Acc: 92.50% - Validation Loss: 0.3728\n",
            "dL_dW max: 0.013903894642299311, min: -0.005990890898105049\n",
            "dL_db max: 0.0023501042461555825, min: -0.0035659779627787384\n",
            "Epoch 208/300 - Training Loss: 9.6147 - Training Acc: 90.38% - Validation Acc: 92.50% - Validation Loss: 0.3720\n",
            "dL_dW max: 0.02046999612432621, min: -0.009722290153170667\n",
            "dL_db max: 0.0017647025722946409, min: -0.0047354356828369325\n",
            "Epoch 209/300 - Training Loss: 9.5970 - Training Acc: 90.50% - Validation Acc: 92.50% - Validation Loss: 0.3713\n",
            "dL_dW max: 0.01334737927854471, min: -0.011426857038587496\n",
            "dL_db max: 0.0008052320045100271, min: -0.012659068054094618\n",
            "Epoch 210/300 - Training Loss: 9.5770 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3706\n",
            "dL_dW max: 0.010216146116146374, min: -0.009691756792205703\n",
            "dL_db max: 0.0008796630415912926, min: -0.008173376987314155\n",
            "Epoch 211/300 - Training Loss: 9.5591 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3698\n",
            "dL_dW max: 0.010082314443737658, min: -0.009048913342201188\n",
            "dL_db max: 0.0007378013812533421, min: -0.008285035391921748\n",
            "Epoch 212/300 - Training Loss: 9.5403 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3691\n",
            "dL_dW max: 0.01364219781374059, min: -0.012209880702074528\n",
            "dL_db max: 0.0015881918054131312, min: -0.014078178592762955\n",
            "Epoch 213/300 - Training Loss: 9.5197 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3684\n",
            "dL_dW max: 0.01558611354080093, min: -0.011943293261540722\n",
            "dL_db max: 0.0013131865810590668, min: -0.009644814400864477\n",
            "Epoch 214/300 - Training Loss: 9.5040 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3677\n",
            "dL_dW max: 0.01100372335715432, min: -0.010270846850251458\n",
            "dL_db max: 0.0002551956029032387, min: -0.00884606635557391\n",
            "Epoch 215/300 - Training Loss: 9.4836 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3670\n",
            "dL_dW max: 0.0211393003249403, min: -0.009932134356559574\n",
            "dL_db max: 0.002621917074876449, min: -0.0065486372979407965\n",
            "Epoch 216/300 - Training Loss: 9.4661 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3662\n",
            "dL_dW max: 0.013525189160176991, min: -0.009402105379435229\n",
            "dL_db max: 0.0004214711257409352, min: -0.0044346975704169685\n",
            "Epoch 217/300 - Training Loss: 9.4464 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3655\n",
            "dL_dW max: 0.009848224528369776, min: -0.008485301011081353\n",
            "dL_db max: 0.0005151304548408668, min: -0.009627803859060721\n",
            "Epoch 218/300 - Training Loss: 9.4265 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3649\n",
            "dL_dW max: 0.009465312609475534, min: -0.008986775724643696\n",
            "dL_db max: 0.0013663197074574435, min: -0.01093129099616986\n",
            "Epoch 219/300 - Training Loss: 9.4067 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3642\n",
            "dL_dW max: 0.013126401234777632, min: -0.006725878425258567\n",
            "dL_db max: 0.0020076686749200814, min: -0.005909368818427831\n",
            "Epoch 220/300 - Training Loss: 9.3917 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3635\n",
            "dL_dW max: 0.011686904937288863, min: -0.010646135860080575\n",
            "dL_db max: 0.0014382422309069053, min: -0.013472404124890789\n",
            "Epoch 221/300 - Training Loss: 9.3718 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3628\n",
            "dL_dW max: 0.011576789716305104, min: -0.010460719526516178\n",
            "dL_db max: 0.0011120686081356511, min: -0.012472084954261966\n",
            "Epoch 222/300 - Training Loss: 9.3531 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3621\n",
            "dL_dW max: 0.025554804719800515, min: -0.016283803186433656\n",
            "dL_db max: 0.007907453721759205, min: -0.010067980371123027\n",
            "Epoch 223/300 - Training Loss: 9.3381 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3614\n",
            "dL_dW max: 0.011439740984998756, min: -0.010365457767155103\n",
            "dL_db max: 0.0014843272182002549, min: -0.013438175157935843\n",
            "Epoch 224/300 - Training Loss: 9.3156 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3608\n",
            "dL_dW max: 0.00962326443893267, min: -0.008808484140580424\n",
            "dL_db max: 0.00027039716722207887, min: -0.008607737930585222\n",
            "Epoch 225/300 - Training Loss: 9.2991 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3601\n",
            "dL_dW max: 0.015738188392399124, min: -0.007716967016443011\n",
            "dL_db max: 0.003424932463330009, min: -0.005241749330824417\n",
            "Epoch 226/300 - Training Loss: 9.2814 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3594\n",
            "dL_dW max: 0.013096117586434633, min: -0.011055325053006078\n",
            "dL_db max: 0.001966958176460435, min: -0.006794531714510219\n",
            "Epoch 227/300 - Training Loss: 9.2622 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3588\n",
            "dL_dW max: 0.014111519620854924, min: -0.012808998150048873\n",
            "dL_db max: 0.00171619272432764, min: -0.011730124968271855\n",
            "Epoch 228/300 - Training Loss: 9.2481 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3581\n",
            "dL_dW max: 0.02026510160287122, min: -0.009951156748001243\n",
            "dL_db max: 0.0010353738886013967, min: -0.0035167911744634225\n",
            "Epoch 229/300 - Training Loss: 9.2271 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3575\n",
            "dL_dW max: 0.010611894335267104, min: -0.0095010167171976\n",
            "dL_db max: 0.00026747341656991046, min: -0.010222220792832169\n",
            "Epoch 230/300 - Training Loss: 9.2094 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3569\n",
            "dL_dW max: 0.01666323166652372, min: -0.014781749716580136\n",
            "dL_db max: 0.0015113265332929928, min: -0.011811033266466408\n",
            "Epoch 231/300 - Training Loss: 9.1926 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3562\n",
            "dL_dW max: 0.017019897629683166, min: -0.011173827964918842\n",
            "dL_db max: 0.001168122220805245, min: -0.007431833827755468\n",
            "Epoch 232/300 - Training Loss: 9.1750 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3555\n",
            "dL_dW max: 0.007782634358758998, min: -0.007471334119663373\n",
            "dL_db max: 0.00025483409240580643, min: -0.005446414649329904\n",
            "Epoch 233/300 - Training Loss: 9.1585 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3549\n",
            "dL_dW max: 0.012599838433011828, min: -0.011059209806573777\n",
            "dL_db max: 0.0006890997733960899, min: -0.005979896130996487\n",
            "Epoch 234/300 - Training Loss: 9.1426 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3543\n",
            "dL_dW max: 0.04050919142530857, min: -0.023891906378752468\n",
            "dL_db max: 0.005372802612959406, min: -0.008714799870043949\n",
            "Epoch 235/300 - Training Loss: 9.1258 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3536\n",
            "dL_dW max: 0.007593623143171392, min: -0.006973307338680869\n",
            "dL_db max: 0.0006437151540119174, min: -0.008264472025532094\n",
            "Epoch 236/300 - Training Loss: 9.1095 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3530\n",
            "dL_dW max: 0.0074567743572231315, min: -0.007035353974507028\n",
            "dL_db max: 0.0005621266660125618, min: -0.009383877917100273\n",
            "Epoch 237/300 - Training Loss: 9.0916 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3524\n",
            "dL_dW max: 0.02370916930715192, min: -0.009295360175598013\n",
            "dL_db max: 0.0031797678860312944, min: -0.005501411502657647\n",
            "Epoch 238/300 - Training Loss: 9.0748 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3517\n",
            "dL_dW max: 0.01351932619951382, min: -0.011594768172346106\n",
            "dL_db max: 0.0006977993503742622, min: -0.007211597425177331\n",
            "Epoch 239/300 - Training Loss: 9.0548 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3511\n",
            "dL_dW max: 0.010058262130544204, min: -0.008378538851136505\n",
            "dL_db max: 0.0011622733296629917, min: -0.006944702529474195\n",
            "Epoch 240/300 - Training Loss: 9.0395 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3505\n",
            "dL_dW max: 0.007848160556046018, min: -0.006764093330516282\n",
            "dL_db max: 0.0005001593605407043, min: -0.010354771732287707\n",
            "Epoch 241/300 - Training Loss: 9.0248 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3498\n",
            "dL_dW max: 0.014049193027149895, min: -0.011186676521799192\n",
            "dL_db max: 0.002150811724324129, min: -0.008153198961637183\n",
            "Epoch 242/300 - Training Loss: 9.0074 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3492\n",
            "dL_dW max: 0.00950642648017353, min: -0.00843581094170094\n",
            "dL_db max: 0.0009632252995523054, min: -0.007172223230324927\n",
            "Epoch 243/300 - Training Loss: 8.9904 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3486\n",
            "dL_dW max: 0.013436425525806829, min: -0.00818758979269037\n",
            "dL_db max: 0.003449677468570351, min: -0.006759759127573996\n",
            "Epoch 244/300 - Training Loss: 8.9749 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3480\n",
            "dL_dW max: 0.011012758423273029, min: -0.008607197942127253\n",
            "dL_db max: 0.0008230784491086408, min: -0.003454114419035193\n",
            "Epoch 245/300 - Training Loss: 8.9561 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3474\n",
            "dL_dW max: 0.014932970474649218, min: -0.013369116453842011\n",
            "dL_db max: 0.0009695389188089439, min: -0.014462901368358904\n",
            "Epoch 246/300 - Training Loss: 8.9435 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3468\n",
            "dL_dW max: 0.008923569359875663, min: -0.008101479523539075\n",
            "dL_db max: 0.0009742291337797103, min: -0.009475306808253732\n",
            "Epoch 247/300 - Training Loss: 8.9246 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3462\n",
            "dL_dW max: 0.046005168800072764, min: -0.02889758756772274\n",
            "dL_db max: 0.005893414220632363, min: -0.007868886028686855\n",
            "Epoch 248/300 - Training Loss: 8.9093 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3456\n",
            "dL_dW max: 0.015577218613444823, min: -0.010873505409585038\n",
            "dL_db max: 0.0006661708582465696, min: -0.008783202078351232\n",
            "Epoch 249/300 - Training Loss: 8.8962 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3450\n",
            "dL_dW max: 0.014534024736822285, min: -0.012297111520498857\n",
            "dL_db max: 0.00033655465883442937, min: -0.009393458860367174\n",
            "Epoch 250/300 - Training Loss: 8.8790 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3444\n",
            "dL_dW max: 0.011620475415677053, min: -0.010098567558671607\n",
            "dL_db max: 0.0010865163546033216, min: -0.012083155316059318\n",
            "Epoch 251/300 - Training Loss: 8.8607 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3438\n",
            "dL_dW max: 0.005405661705769541, min: -0.007619886646733348\n",
            "dL_db max: 0.0014204315550046848, min: -0.006281913141759787\n",
            "Epoch 252/300 - Training Loss: 8.8485 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3433\n",
            "dL_dW max: 0.011342294239020464, min: -0.00924314073130437\n",
            "dL_db max: 0.0021696685348483284, min: -0.01228290282784833\n",
            "Epoch 253/300 - Training Loss: 8.8325 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3427\n",
            "dL_dW max: 0.009756018816278137, min: -0.008857143185411009\n",
            "dL_db max: 0.002109112313377089, min: -0.009728388096773835\n",
            "Epoch 254/300 - Training Loss: 8.8170 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3421\n",
            "dL_dW max: 0.017387642743287964, min: -0.00646864780353755\n",
            "dL_db max: 0.00259413885190283, min: -0.00609472912240731\n",
            "Epoch 255/300 - Training Loss: 8.8041 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3416\n",
            "dL_dW max: 0.010559025523964717, min: -0.008119764431389042\n",
            "dL_db max: 0.0015939279498321229, min: -0.006835360518381105\n",
            "Epoch 256/300 - Training Loss: 8.7863 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3410\n",
            "dL_dW max: 0.016393022197221427, min: -0.014277221991999206\n",
            "dL_db max: 0.0013561757154470233, min: -0.012302784434070926\n",
            "Epoch 257/300 - Training Loss: 8.7707 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3404\n",
            "dL_dW max: 0.009778325905473754, min: -0.008559923051232694\n",
            "dL_db max: 9.266711052865011e-05, min: -0.006854615535284018\n",
            "Epoch 258/300 - Training Loss: 8.7574 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3399\n",
            "dL_dW max: 0.009278191422561225, min: -0.006589799999429405\n",
            "dL_db max: 0.0011270670913841943, min: -0.004781289840263293\n",
            "Epoch 259/300 - Training Loss: 8.7447 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3393\n",
            "dL_dW max: 0.00921801490788418, min: -0.008231201790398756\n",
            "dL_db max: 0.0012237068986562763, min: -0.007156213541220696\n",
            "Epoch 260/300 - Training Loss: 8.7265 - Training Acc: 90.75% - Validation Acc: 93.50% - Validation Loss: 0.3388\n",
            "dL_dW max: 0.008590950595951401, min: -0.007626877577575983\n",
            "dL_db max: 0.0007495647886478385, min: -0.012366524077603221\n",
            "Epoch 261/300 - Training Loss: 8.7095 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3383\n",
            "dL_dW max: 0.015515875743850698, min: -0.010858080555928127\n",
            "dL_db max: 0.0009031757570019003, min: -0.007258729462580271\n",
            "Epoch 262/300 - Training Loss: 8.6959 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3377\n",
            "dL_dW max: 0.012702240887885661, min: -0.010955818292124811\n",
            "dL_db max: 0.0002057654040948103, min: -0.006223835189516352\n",
            "Epoch 263/300 - Training Loss: 8.6836 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3372\n",
            "dL_dW max: 0.00801093533751678, min: -0.007164483696225631\n",
            "dL_db max: 0.0020508929221904285, min: -0.007732014856670413\n",
            "Epoch 264/300 - Training Loss: 8.6679 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3367\n",
            "dL_dW max: 0.020126294402613387, min: -0.008359840875284841\n",
            "dL_db max: 0.0024194834428672633, min: -0.005053541854773305\n",
            "Epoch 265/300 - Training Loss: 8.6541 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3362\n",
            "dL_dW max: 0.009228739596357291, min: -0.004546752585388706\n",
            "dL_db max: 0.004155843950939055, min: -0.007285787279930295\n",
            "Epoch 266/300 - Training Loss: 8.6414 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3357\n",
            "dL_dW max: 0.0096897185072854, min: -0.007167302291150889\n",
            "dL_db max: 0.001093021876920043, min: -0.006586300123368749\n",
            "Epoch 267/300 - Training Loss: 8.6242 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3352\n",
            "dL_dW max: 0.01648109578186712, min: -0.012580437312462821\n",
            "dL_db max: 0.001253018483358336, min: -0.010166708464671109\n",
            "Epoch 268/300 - Training Loss: 8.6085 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3347\n",
            "dL_dW max: 0.007761070833541696, min: -0.008238040213178387\n",
            "dL_db max: 0.002544531803108432, min: -0.007628437157021929\n",
            "Epoch 269/300 - Training Loss: 8.5966 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3342\n",
            "dL_dW max: 0.01210375799857827, min: -0.008479930969699621\n",
            "dL_db max: 0.002534005248374023, min: -0.009291723797938672\n",
            "Epoch 270/300 - Training Loss: 8.5846 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3338\n",
            "dL_dW max: 0.009032902921356032, min: -0.006251989503318443\n",
            "dL_db max: 0.0029973818055098007, min: -0.0060197578356789684\n",
            "Epoch 271/300 - Training Loss: 8.5713 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3333\n",
            "dL_dW max: 0.009861344928981077, min: -0.008558573069645031\n",
            "dL_db max: 0.00021630026597898143, min: -0.011902182637706784\n",
            "Epoch 272/300 - Training Loss: 8.5571 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3328\n",
            "dL_dW max: 0.01633616676842771, min: -0.013797573731122328\n",
            "dL_db max: 0.0001968113687313932, min: -0.012689488938538445\n",
            "Epoch 273/300 - Training Loss: 8.5413 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3324\n",
            "dL_dW max: 0.01296716764670987, min: -0.007988335518713786\n",
            "dL_db max: 0.006098828869466095, min: -0.009549988207769877\n",
            "Epoch 274/300 - Training Loss: 8.5294 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3320\n",
            "dL_dW max: 0.011073895022163386, min: -0.00712000007717693\n",
            "dL_db max: 0.0020505196892750322, min: -0.00657154202910994\n",
            "Epoch 275/300 - Training Loss: 8.5142 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3316\n",
            "dL_dW max: 0.014987107896838433, min: -0.008712586640433267\n",
            "dL_db max: 0.0030437365576562094, min: -0.004809390596283665\n",
            "Epoch 276/300 - Training Loss: 8.5034 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3312\n",
            "dL_dW max: 0.014538617997273795, min: -0.012533725617008546\n",
            "dL_db max: 0.0008539376881602425, min: -0.014677183861890585\n",
            "Epoch 277/300 - Training Loss: 8.4850 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3309\n",
            "dL_dW max: 0.025839147366417677, min: -0.01079915152790831\n",
            "dL_db max: 0.002895576777173344, min: -0.005187516419701768\n",
            "Epoch 278/300 - Training Loss: 8.4774 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3305\n",
            "dL_dW max: 0.007212780175738926, min: -0.006726426230484487\n",
            "dL_db max: 0.0005375615528170416, min: -0.00921989624742511\n",
            "Epoch 279/300 - Training Loss: 8.4627 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3302\n",
            "dL_dW max: 0.010790299612171832, min: -0.009258589432919254\n",
            "dL_db max: 0.0009033581140787103, min: -0.009830829333485672\n",
            "Epoch 280/300 - Training Loss: 8.4490 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3298\n",
            "dL_dW max: 0.010310148044612824, min: -0.008235591452332967\n",
            "dL_db max: 0.00037574908024729414, min: -0.008303598558050325\n",
            "Epoch 281/300 - Training Loss: 8.4349 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3295\n",
            "dL_dW max: 0.0170143576522085, min: -0.014374335568964858\n",
            "dL_db max: 0.00024233489085469524, min: -0.009817127843330612\n",
            "Epoch 282/300 - Training Loss: 8.4251 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3291\n",
            "dL_dW max: 0.03344676567320903, min: -0.01756369591372008\n",
            "dL_db max: 0.008171117993282734, min: -0.007984041177792191\n",
            "Epoch 283/300 - Training Loss: 8.4148 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3288\n",
            "dL_dW max: 0.015132990467230772, min: -0.009672448098440565\n",
            "dL_db max: 0.00054129950697798, min: -0.008543667837990734\n",
            "Epoch 284/300 - Training Loss: 8.4028 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3285\n",
            "dL_dW max: 0.016498482301359484, min: -0.01373543808364546\n",
            "dL_db max: 0.0004088145860358774, min: -0.014689901886373235\n",
            "Epoch 285/300 - Training Loss: 8.3928 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3281\n",
            "dL_dW max: 0.012091094159525109, min: -0.01039627051794151\n",
            "dL_db max: 0.0001770685594184692, min: -0.011079426418160534\n",
            "Epoch 286/300 - Training Loss: 8.3763 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3278\n",
            "dL_dW max: 0.01798857758369518, min: -0.007040398192987603\n",
            "dL_db max: 0.0032692128842314156, min: -0.005253795905006179\n",
            "Epoch 287/300 - Training Loss: 8.3673 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3275\n",
            "dL_dW max: 0.006516140776278949, min: -0.006033737979501528\n",
            "dL_db max: 0.0015907094544613917, min: -0.008973209974510992\n",
            "Epoch 288/300 - Training Loss: 8.3610 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3272\n",
            "dL_dW max: 0.02171397122058597, min: -0.00946567644128511\n",
            "dL_db max: 0.0016425954984491055, min: -0.003525840965753384\n",
            "Epoch 289/300 - Training Loss: 8.3487 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3269\n",
            "dL_dW max: 0.045982726145682155, min: -0.030856795096472274\n",
            "dL_db max: 0.008240169958058088, min: -0.007278430376610758\n",
            "Epoch 290/300 - Training Loss: 8.3412 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3266\n",
            "dL_dW max: 0.005558241437036231, min: -0.006448641988852878\n",
            "dL_db max: 0.0028664729915731588, min: -0.007261164377959612\n",
            "Epoch 291/300 - Training Loss: 8.3248 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3264\n",
            "dL_dW max: 0.019801042070516747, min: -0.012292345545015678\n",
            "dL_db max: 0.0007993934765053873, min: -0.00566582135591321\n",
            "Epoch 292/300 - Training Loss: 8.3173 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3261\n",
            "dL_dW max: 0.013735213455787761, min: -0.011876119466653992\n",
            "dL_db max: 0.0014886802948021081, min: -0.012093861918810674\n",
            "Epoch 293/300 - Training Loss: 8.3082 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3259\n",
            "dL_dW max: 0.015823877896752634, min: -0.012917444188952607\n",
            "dL_db max: 0.0015839307148657875, min: -0.016357305443003928\n",
            "Epoch 294/300 - Training Loss: 8.2938 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3256\n",
            "dL_dW max: 0.005610923832120287, min: -0.0068035265900807\n",
            "dL_db max: 0.001395345350125263, min: -0.006865783127680782\n",
            "Epoch 295/300 - Training Loss: 8.2873 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3254\n",
            "dL_dW max: 0.008958038621097685, min: -0.00752629737288841\n",
            "dL_db max: 0.0010212646604146002, min: -0.004895828760213367\n",
            "Epoch 296/300 - Training Loss: 8.2814 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3252\n",
            "dL_dW max: 0.017456676207616203, min: -0.014392969726382557\n",
            "dL_db max: 0.0003774337572730943, min: -0.014902432515284628\n",
            "Epoch 297/300 - Training Loss: 8.2703 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3250\n",
            "dL_dW max: 0.014717506675511312, min: -0.012603831017336546\n",
            "dL_db max: 0.0016606778284716742, min: -0.017214471113223804\n",
            "Epoch 298/300 - Training Loss: 8.2601 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3249\n",
            "dL_dW max: 0.013319188513170273, min: -0.01135226851319867\n",
            "dL_db max: 0.0006842029921736554, min: -0.011846306203723997\n",
            "Epoch 299/300 - Training Loss: 8.2526 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3247\n",
            "dL_dW max: 0.009770585303372577, min: -0.008056506448290007\n",
            "dL_db max: 0.0010359562037724004, min: -0.012840169622406737\n",
            "Epoch 300/300 - Training Loss: 8.2441 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3246\n"
          ]
        }
      ]
    }
  ]
}