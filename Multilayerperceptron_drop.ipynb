{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimalthomas-db/deeplearning/blob/main/Multilayerperceptron_drop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hO48dKJRV-ob"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def batch_generator(train_x, train_y, batch_size):\n",
        "    indices = np.arange(len(train_x))\n",
        "    np.random.shuffle(indices)  # Shuffle indices\n",
        "\n",
        "    for i in range(0, len(train_x), batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "        batch_x = train_x[batch_idx]\n",
        "        batch_y = train_y[batch_idx]\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivationFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the output of the activation function, evaluated on x\n",
        "\n",
        "        Input args may differ in the case of softmax\n",
        "\n",
        "        :param x (np.ndarray): input\n",
        "        :return: output of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the activation function, evaluated on x\n",
        "        :param x (np.ndarray): input\n",
        "        :return: activation function's derivative at x\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return self.forward(x) * (1 - self.forward(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "\n",
        "class Relu(ActivationFunction):\n",
        "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0, x)  # ReLU function: max(0, x)\n",
        "\n",
        "  def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "    return (x > 0).astype(float)  # Derivative: 1 for x > 0, else 0\n",
        "\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the Softmax activation function.\n",
        "        Uses a stability trick to prevent overflow.\n",
        "\n",
        "        :param x: Input logits (batch_size, num_classes)\n",
        "        :return: Softmax probabilities (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        x_max = np.max(x, axis=-1, keepdims=True)  # Stability trick\n",
        "        exp_x = np.exp(x - x_max)\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the Jacobian matrix of Softmax for each sample in the batch.\n",
        "\n",
        "        :param x: Softmax output (batch_size, num_classes)\n",
        "        :return: Jacobian matrix (batch_size, num_classes, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size, num_classes = x.shape\n",
        "        jacobian_matrix = np.zeros((batch_size, num_classes, num_classes))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            s_i = x[i].reshape(-1, 1)  # Convert to column vector\n",
        "            jacobian_matrix[i] = np.diagflat(s_i) - np.dot(s_i, s_i.T)  # Softmax Jacobian\n",
        "\n",
        "        return jacobian_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Linear(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return x\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.ones_like(x)\n",
        "\n",
        "class Softplus(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.log(1 + np.exp(x))\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))  # Sigmoid function\n",
        "\n",
        "\n",
        "class Mish(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return x * np.tanh(np.log1p(np.exp(x)))  # log1p for numerical stability\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        softplus_x = np.log1p(np.exp(x))\n",
        "        tanh_softplus = np.tanh(softplus_x)\n",
        "        return tanh_softplus + x * (1 - tanh_softplus ** 2) * (1 / (1 + np.exp(-x)))\n",
        "\n",
        "\n",
        "class LossFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "\n",
        "class SquaredError(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return 1/2 * np.square(y_pred-y_true)\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return (y_pred - y_true)/y_pred.shape[0]\n",
        "\n",
        "\n",
        "class CrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -y_true / y_pred\n",
        "\n",
        "class BinaryCrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Prevent log(0) issues\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes gradient of binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction, dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        Initializes a layer of neurons\n",
        "\n",
        "        :param fan_in: number of neurons in previous (presynpatic) layer\n",
        "        :param fan_out: number of neurons in this layer\n",
        "        :param activation_function: instance of an ActivationFunction\n",
        "        \"\"\"\n",
        "        self.fan_in = fan_in\n",
        "        self.fan_out = fan_out\n",
        "        self.activation_function = activation_function\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "        #weight initilization\n",
        "\n",
        "\n",
        "        # this will store the activations (forward prop)\n",
        "        self.activations = None\n",
        "        # this will store the delta term (dL_dPhi, backward prop)\n",
        "        self.delta = None\n",
        "        self.dropout_mask = None\n",
        "\n",
        "\n",
        "        #change this to glorot uniform initialization\n",
        "\n",
        "\n",
        "\n",
        "        self.W = np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)  # He_init for Relu\n",
        "\n",
        "        self.b = np.zeros((fan_out,))\n",
        "\n",
        "    def forward(self, h: np.ndarray, training = True):\n",
        "        \"\"\"\n",
        "        Computes the activations for this layer\n",
        "\n",
        "        :param h: input to layer\n",
        "        :return: layer activations\n",
        "        \"\"\"\n",
        "        #Z calculation\n",
        "\n",
        "        Z = np.dot(h, self.W) + self.b\n",
        "        #self.activations = None\n",
        "        activations = self.activation_function.forward(Z)\n",
        "\n",
        "        #storing activations\n",
        "\n",
        "\n",
        "\n",
        "        if training and self.dropout_rate > 0:\n",
        "            # Apply dropout mask\n",
        "            self.dropout_mask = (np.random.rand(*activations.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
        "            activations *= self.dropout_mask\n",
        "        else:\n",
        "            self.dropout_mask = np.ones_like(activations)  # No dropout during inference\n",
        "\n",
        "        self.activations = activations\n",
        "        return self.activations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "      \"\"\"\n",
        "      Apply backpropagation to this layer and return the weight and bias gradients.\n",
        "\n",
        "      :param h: Input to this layer.\n",
        "      :param delta: Delta term from the layer above.\n",
        "      :return: (Weight gradients, Bias gradients).\n",
        "      \"\"\"\n",
        "      if isinstance(self.activation_function, Softmax):\n",
        "\n",
        "        # Compute the Softmax derivative using the Jacobian\n",
        "        softmax_out = self.activations  # Already computed during forward pass\n",
        "        softmax_jacobian = self.activation_function.derivative(softmax_out)  # Shape (batch_size, num_classes, num_classes)\n",
        "\n",
        "        # Multiply Jacobian by delta (loss gradient w.r.t. activations)\n",
        "        dZ = np.einsum('bij,bj->bi', softmax_jacobian, delta)  # Efficient batch-wise multiplication\n",
        "      else:\n",
        "        dZ = delta * self.activation_function.derivative(self.activations)\n",
        "\n",
        "\n",
        "    #apply dropout mask\n",
        "      if self.dropout_rate > 0 and self.dropout_mask is not None:\n",
        "        dZ *= self.dropout_mask\n",
        "\n",
        "\n",
        "\n",
        "    # Compute weight and bias gradients\n",
        "      dL_dW = np.dot(h.T, dZ) / h.shape[0]  # (fan_in, fan_out)\n",
        "      dL_db = np.sum(dZ, axis=0, keepdims=True) / h.shape[0]  # (1, fan_out)\n",
        "\n",
        "      self.delta = np.dot(dZ, self.W.T)  # (batch_size, fan_in)\n",
        "\n",
        "      return dL_dW, dL_db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultilayerPerceptron:\n",
        "    def __init__(self, layers: Tuple[Layer]):\n",
        "        \"\"\"\n",
        "        Create a multilayer perceptron (densely connected multilayer neural network)\n",
        "        :param layers: list or Tuple of layers\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x: np.ndarray,training=True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        This takes the network input and computes the network output (forward propagation)\n",
        "        :param x: network input\n",
        "        :return: network output\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x,training=training)\n",
        "        return x\n",
        "\n",
        "    def backward(self, loss_grad: np.ndarray, input_data: np.ndarray) -> Tuple[list, list]:\n",
        "      \"\"\"\n",
        "      Applies backpropagation to compute gradients of weights and biases for all layers in the network.\n",
        "\n",
        "      :param loss_grad: Gradient of loss w.r.t. final layer output (dL/dA).\n",
        "      :param input_data: The input data to the network (train_x for the first layer).\n",
        "      :return: (List of weight gradients for all layers, List of bias gradients for all layers).\n",
        "      \"\"\"\n",
        "\n",
        "      dl_dw_all = []\n",
        "      dl_db_all = []\n",
        "\n",
        "      dL_dA = loss_grad  # Start with gradient from the loss function\n",
        "\n",
        "    # Iterate backward through layers\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "        layer = self.layers[i]\n",
        "\n",
        "        # Get the correct input for this layer\n",
        "        if i == 0:\n",
        "            h = input_data  # First layer gets train_x\n",
        "        else:\n",
        "            h = self.layers[i - 1].activations  # Other layers get activations from previous layer\n",
        "\n",
        "        # Compute backpropagation step for this layer\n",
        "        dL_dW, dL_db = layer.backward(h, dL_dA)\n",
        "\n",
        "        # Store gradients\n",
        "        dl_dw_all.append(dL_dW)\n",
        "        dl_db_all.append(dL_db)\n",
        "\n",
        "        dL_dA = layer.delta  # Update delta for the next layer\n",
        "\n",
        "    # Reverse lists to match layer order\n",
        "      dl_dw_all.reverse()\n",
        "      dl_db_all.reverse()\n",
        "\n",
        "      return dl_dw_all, dl_db_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32, model_type: str=\"classification\",RMSProp: bool=False) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    #def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32,model_type:mod_type) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Train the multilayer perceptron\n",
        "\n",
        "        :param train_x: full training set input of shape (n x d) n = number of samples, d = number of features\n",
        "        :param train_y: full training set output of shape (n x q) n = number of samples, q = number of outputs per sample\n",
        "        :param val_x: full validation set input\n",
        "        :param val_y: full validation set output\n",
        "        :param loss_func: instance of a LossFunction\n",
        "        :param learning_rate: learning rate for parameter updates\n",
        "        :param batch_size: size of each batch\n",
        "        :param epochs: number of epochs\n",
        "        :param model_type: type of the model(regression/classification)\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        #initializing RMSProp parameters\n",
        "        if RMSProp:\n",
        "          self.beta = 0.9\n",
        "          self.epsilon = 1e-8\n",
        "\n",
        "          for layer in self.layers:\n",
        "            layer.m_W = np.zeros_like(layer.W)\n",
        "            layer.m_b = np.zeros_like(layer.b)\n",
        "\n",
        "        #define the epoch loop\n",
        "\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "\n",
        "\n",
        "        #defin epoch loop\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "          #define batch loop\n",
        "          total_loss = 0\n",
        "\n",
        "          for batch_x, batch_y in batch_generator(train_x, train_y, batch_size):\n",
        "\n",
        "\n",
        "            #forward pass\n",
        "            y_pred = self.forward(batch_x,training=True)\n",
        "\n",
        "            #compute loss\n",
        "\n",
        "            batchloss = loss_func.loss(batch_y, y_pred)\n",
        "\n",
        "            if batchloss.ndim > 0:\n",
        "              batchloss = np.mean(batchloss)\n",
        "\n",
        "\n",
        "            total_loss = total_loss + batchloss\n",
        "\n",
        "            #print(total_loss)\n",
        "\n",
        "\n",
        "            #backward pass and compute gradianet\n",
        "            #dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y, output), batch_x)\n",
        "            dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y[:len(y_pred)], y_pred), batch_x)\n",
        "\n",
        "\n",
        "            #update weights\n",
        "            max_grad_norm = 1.0  # Limits the maximum gradient value\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(len(self.layers)):\n",
        "\n",
        "                layer = self.layers[i]\n",
        "              #print(f\"Layer {i}: dL_dW dtype = {type(dL_dW[i])}, dL_db dtype = {type(dL_db[i])}\")\n",
        "\n",
        "              # Clip gradients\n",
        "              #commenting the gradient clipping part.\n",
        "              #dL_dW[i] = np.clip(dL_dW[i], -max_grad_norm, max_grad_norm)\n",
        "              #dL_db[i] = np.clip(dL_db[i], -max_grad_norm, max_grad_norm)\n",
        "\n",
        "              #print(f\"Layer {i}: dL_dW dtype = {type(dL_dW[i])}, dL_db dtype = {type(dL_db[i])}\")\n",
        "\n",
        "                if RMSProp:\n",
        "                  layer.m_W = self.beta * layer.m_W + (1 - self.beta) * (dL_dW[i] ** 2)\n",
        "                  layer.m_b = self.beta * layer.m_b + (1 - self.beta) * (dL_db[i].squeeze() ** 2)\n",
        "\n",
        "                  layer.W -= learning_rate * dL_dW[i] / (np.sqrt(layer.m_W) + epsilon)\n",
        "\n",
        "                  layer.b -= learning_rate * dL_db[i].squeeze() / (np.sqrt(layer.m_b) + epsilon)\n",
        "\n",
        "                else:\n",
        "\n",
        "                  layer.W -= learning_rate * dL_dW[i]\n",
        "                  layer.b -= learning_rate * dL_db[i].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # #training_losses.append(total_loss / len(train_x))\n",
        "          # num_batches = max(1, len(train_x) // batch_size)  # Avoid division by zero\n",
        "          # training_losses.append(total_loss / num_batches)\n",
        "\n",
        "          num_batches = len(train_x) / batch_size  # Use float division instead of integer division\n",
        "          #print(total_loss / num_batches)\n",
        "          training_losses.append(total_loss / num_batches)\n",
        "          #print(training_losses)\n",
        "\n",
        "\n",
        "          # Compute Validation Loss\n",
        "          # val_output = self.forward(val_x)\n",
        "          # val_loss = loss_func.loss(val_y, val_output)\n",
        "          # validation_losses.append(np.mean(val_loss))\n",
        "\n",
        "          val_output = self.forward(val_x,training = False)\n",
        "          val_loss = loss_func.loss(val_y, val_output)\n",
        "\n",
        "          # Ensure val_loss is scalar\n",
        "          validation_losses.append(np.mean(val_loss) if val_loss.ndim > 0 else val_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Compute training accuracy at the end of the epoch\n",
        "\n",
        "          if model_type == 'classification':\n",
        "            train_acc = compute_accuracy(self, train_x, train_y)\n",
        "            val_acc = compute_accuracy(self, val_x, val_y)\n",
        "\n",
        "            #print(f\"{training_losses}\")\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {training_losses[-1]:.4f} - Training Acc: {train_acc:.2f}% - Validation Acc: {val_acc:.2f}% - Validation Loss: {validation_losses[-1]:.4f}\")\n",
        "\n",
        "          else:\n",
        "            # Compute regression metrics first\n",
        "            train_mse, train_mae, train_r2 = compute_regression_metrics(self, train_x, train_y)\n",
        "            val_mse, val_mae, val_r2 = compute_regression_metrics(self, val_x, val_y)\n",
        "\n",
        "            # Ensure values are single scalar floats\n",
        "            train_mse = float(train_mse)\n",
        "            train_mae = float(train_mae)\n",
        "            train_r2 = float(train_r2)\n",
        "\n",
        "            val_mse = float(val_mse)\n",
        "            val_mae = float(val_mae)\n",
        "            val_r2 = float(val_r2)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Training MSE: {train_mse:.4f} - Validation MSE: {val_mse:.4f} - Training MAE: {train_mae:.4f} - Validation MAE: {val_mae:.4f} - Training R²: {train_r2:.4f} - Validation R²: {val_r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return training_losses, validation_losses\n",
        "\n",
        "def compute_accuracy(model, X, y):\n",
        "    y_pred = model.forward(X)\n",
        "    if y.shape[1] > 1:  # Multi-class (e.g., MNIST)\n",
        "        y_pred_class = np.argmax(y_pred, axis=1)\n",
        "        y_true_class = np.argmax(y, axis=1)\n",
        "    else:  # Binary classification\n",
        "        y_pred_class = (y_pred > 0.5).astype(int)\n",
        "        y_true_class = y.astype(int)\n",
        "\n",
        "    return np.mean(y_pred_class == y_true_class) * 100\n",
        "\n",
        "\n",
        "\n",
        "def compute_regression_metrics(model, X, y):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for regression tasks (MPG).\n",
        "\n",
        "    :param model: Trained MLP model\n",
        "    :param X: Input features (numpy array)\n",
        "    :param y: True labels (numpy array)\n",
        "    :return: MSE, MAE, R² Score (all as Python floats)\n",
        "    \"\"\"\n",
        "    y_pred = model.forward(X)  # Forward pass\n",
        "\n",
        "    mse = np.mean((y - y_pred) ** 2)\n",
        "    mae = np.mean(np.abs(y - y_pred))\n",
        "    ss_total = np.sum((y - np.mean(y)) ** 2)\n",
        "    ss_residual = np.sum((y - y_pred) ** 2)\n",
        "    r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0\n",
        "\n",
        "\n",
        "    # Convert values to floats if they are NumPy scalars\n",
        "    return float(mse), float(mae), float(r2_score)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}