{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiQj1Xyml4gN65Tq5sYd7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimalthomas/deeplearning/blob/main/MLP_Base_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hO48dKJRV-ob"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def batch_generator(train_x, train_y, batch_size):\n",
        "    indices = np.arange(len(train_x))\n",
        "    np.random.shuffle(indices)  # Shuffle indices\n",
        "\n",
        "    for i in range(0, len(train_x), batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "        batch_x = train_x[batch_idx]\n",
        "        batch_y = train_y[batch_idx]\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivationFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the output of the activation function, evaluated on x\n",
        "\n",
        "        Input args may differ in the case of softmax\n",
        "\n",
        "        :param x (np.ndarray): input\n",
        "        :return: output of the activation function\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the activation function, evaluated on x\n",
        "        :param x (np.ndarray): input\n",
        "        :return: activation function's derivative at x\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return self.forward(x) * (1 - self.forward(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "\n",
        "class Relu(ActivationFunction):\n",
        "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0, x)  # ReLU function: max(0, x)\n",
        "\n",
        "  def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "    return (x > 0).astype(float)  # Derivative: 1 for x > 0, else 0\n",
        "\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the Softmax activation function.\n",
        "        Uses a stability trick to prevent overflow.\n",
        "\n",
        "        :param x: Input logits (batch_size, num_classes)\n",
        "        :return: Softmax probabilities (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        x_max = np.max(x, axis=-1, keepdims=True)  # Stability trick\n",
        "        exp_x = np.exp(x - x_max)\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the Jacobian matrix of Softmax for each sample in the batch.\n",
        "\n",
        "        :param x: Softmax output (batch_size, num_classes)\n",
        "        :return: Jacobian matrix (batch_size, num_classes, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size, num_classes = x.shape\n",
        "        jacobian_matrix = np.zeros((batch_size, num_classes, num_classes))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            s_i = x[i].reshape(-1, 1)  # Convert to column vector\n",
        "            jacobian_matrix[i] = np.diagflat(s_i) - np.dot(s_i, s_i.T)  # Softmax Jacobian\n",
        "\n",
        "        return jacobian_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Linear(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return x\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.ones_like(x)\n",
        "\n",
        "class Softplus(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.log(1 + np.exp(x))\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))  # Sigmoid function\n",
        "\n",
        "\n",
        "class Mish(ActivationFunction):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        return x * np.tanh(np.log1p(np.exp(x)))  # log1p for numerical stability\n",
        "\n",
        "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        softplus_x = np.log1p(np.exp(x))\n",
        "        tanh_softplus = np.tanh(softplus_x)\n",
        "        return tanh_softplus + x * (1 - tanh_softplus ** 2) * (1 / (1 + np.exp(-x)))\n",
        "\n",
        "\n",
        "class LossFunction(ABC):\n",
        "    @abstractmethod\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "\n",
        "class SquaredError(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return 1/2 * np.square(y_pred-y_true)\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        return (y_pred - y_true)/y_pred.shape[0]\n",
        "\n",
        "\n",
        "class CrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -y_true / y_pred\n",
        "\n",
        "class BinaryCrossEntropy(LossFunction):\n",
        "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Prevent log(0) issues\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes gradient of binary cross-entropy loss\n",
        "        \"\"\"\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction):\n",
        "        \"\"\"\n",
        "        Initializes a layer of neurons\n",
        "\n",
        "        :param fan_in: number of neurons in previous (presynpatic) layer\n",
        "        :param fan_out: number of neurons in this layer\n",
        "        :param activation_function: instance of an ActivationFunction\n",
        "        \"\"\"\n",
        "        self.fan_in = fan_in\n",
        "        self.fan_out = fan_out\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        # this will store the activations (forward prop)\n",
        "        self.activations = None\n",
        "        # this will store the delta term (dL_dPhi, backward prop)\n",
        "        self.delta = None\n",
        "\n",
        "        # Initialize weights and biaes\n",
        "        # self.W = None  # weights\n",
        "        # self.b = None  # biases\n",
        "\n",
        "        #note to self. looks like He initialization will help relu function\n",
        "        #come back later\n",
        "\n",
        "        #self.W = np.random.randn(fan_in, fan_out) * 0.01\n",
        "        self.W = np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)  # He_init for Relu\n",
        "\n",
        "        self.b = np.zeros((fan_out,))\n",
        "\n",
        "    def forward(self, h: np.ndarray):\n",
        "        \"\"\"\n",
        "        Computes the activations for this layer\n",
        "\n",
        "        :param h: input to layer\n",
        "        :return: layer activations\n",
        "        \"\"\"\n",
        "        #Z calculation\n",
        "\n",
        "        Z = np.dot(h, self.W) + self.b\n",
        "        #self.activations = None\n",
        "        self.activations = self.activation_function.forward(Z)\n",
        "        return self.activations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    #     \"\"\"\n",
        "    #     Apply backpropagation to this layer and return the weight and bias gradients\n",
        "\n",
        "    #     :param h: input to this layer\n",
        "    #     :param delta: delta term from layer above\n",
        "    #     :return: (weight gradients, bias gradients)\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     #compute dZ\n",
        "    #     dZ = delta * self.activation_function.derivative(self.activations)\n",
        "    #     #compute dW, db\n",
        "\n",
        "    #     dL_dW = np.dot(h.T, dZ) / h.shape[0]\n",
        "    #     dL_db = np.sum(dZ, axis=0) / h.shape[0]\n",
        "\n",
        "\n",
        "    #     self.delta = dZ\n",
        "    #     return dL_dW, dL_db\n",
        "\n",
        "    def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "      \"\"\"\n",
        "      Apply backpropagation to this layer and return the weight and bias gradients.\n",
        "\n",
        "      :param h: Input to this layer.\n",
        "      :param delta: Delta term from the layer above.\n",
        "      :return: (Weight gradients, Bias gradients).\n",
        "      \"\"\"\n",
        "      if isinstance(self.activation_function, Softmax):\n",
        "\n",
        "        # Compute the Softmax derivative using the Jacobian\n",
        "        softmax_out = self.activations  # Already computed during forward pass\n",
        "        softmax_jacobian = self.activation_function.derivative(softmax_out)  # Shape (batch_size, num_classes, num_classes)\n",
        "\n",
        "        # Multiply Jacobian by delta (loss gradient w.r.t. activations)\n",
        "        dZ = np.einsum('bij,bj->bi', softmax_jacobian, delta)  # Efficient batch-wise multiplication\n",
        "      else:\n",
        "        dZ = delta * self.activation_function.derivative(self.activations)\n",
        "\n",
        "    # Compute weight and bias gradients\n",
        "      dL_dW = np.dot(h.T, dZ) / h.shape[0]  # (fan_in, fan_out)\n",
        "      dL_db = np.sum(dZ, axis=0, keepdims=True) / h.shape[0]  # (1, fan_out)\n",
        "\n",
        "      self.delta = np.dot(dZ, self.W.T)  # (batch_size, fan_in)\n",
        "\n",
        "      return dL_dW, dL_db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultilayerPerceptron:\n",
        "    def __init__(self, layers: Tuple[Layer]):\n",
        "        \"\"\"\n",
        "        Create a multilayer perceptron (densely connected multilayer neural network)\n",
        "        :param layers: list or Tuple of layers\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        This takes the network input and computes the network output (forward propagation)\n",
        "        :param x: network input\n",
        "        :return: network output\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, loss_grad: np.ndarray, input_data: np.ndarray) -> Tuple[list, list]:\n",
        "      \"\"\"\n",
        "      Applies backpropagation to compute gradients of weights and biases for all layers in the network.\n",
        "\n",
        "      :param loss_grad: Gradient of loss w.r.t. final layer output (dL/dA).\n",
        "      :param input_data: The input data to the network (train_x for the first layer).\n",
        "      :return: (List of weight gradients for all layers, List of bias gradients for all layers).\n",
        "      \"\"\"\n",
        "\n",
        "      dl_dw_all = []\n",
        "      dl_db_all = []\n",
        "\n",
        "      dL_dA = loss_grad  # Start with gradient from the loss function\n",
        "\n",
        "    # Iterate backward through layers\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "        layer = self.layers[i]\n",
        "\n",
        "        # Get the correct input for this layer\n",
        "        if i == 0:\n",
        "            h = input_data  # First layer gets train_x\n",
        "        else:\n",
        "            h = self.layers[i - 1].activations  # Other layers get activations from previous layer\n",
        "\n",
        "        # Compute backpropagation step for this layer\n",
        "        dL_dW, dL_db = layer.backward(h, dL_dA)\n",
        "\n",
        "        # Store gradients\n",
        "        dl_dw_all.append(dL_dW)\n",
        "        dl_db_all.append(dL_db)\n",
        "\n",
        "        dL_dA = layer.delta  # Update delta for the next layer\n",
        "\n",
        "    # Reverse lists to match layer order\n",
        "      dl_dw_all.reverse()\n",
        "      dl_db_all.reverse()\n",
        "\n",
        "      return dl_dw_all, dl_db_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Train the multilayer perceptron\n",
        "\n",
        "        :param train_x: full training set input of shape (n x d) n = number of samples, d = number of features\n",
        "        :param train_y: full training set output of shape (n x q) n = number of samples, q = number of outputs per sample\n",
        "        :param val_x: full validation set input\n",
        "        :param val_y: full validation set output\n",
        "        :param loss_func: instance of a LossFunction\n",
        "        :param learning_rate: learning rate for parameter updates\n",
        "        :param batch_size: size of each batch\n",
        "        :param epochs: number of epochs\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        #define the epoch loop\n",
        "\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "\n",
        "\n",
        "        #defin epoch loop\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "          #define batch loop\n",
        "          total_loss = 0\n",
        "\n",
        "          for batch_x, batch_y in batch_generator(train_x, train_y, batch_size):\n",
        "\n",
        "\n",
        "            #forward pass\n",
        "            y_pred = self.forward(batch_x)\n",
        "\n",
        "            #compute loss\n",
        "\n",
        "            batchloss = loss_func.loss(batch_y, y_pred)\n",
        "            total_loss = total_loss + np.mean(batchloss)\n",
        "\n",
        "            #print(total_loss)\n",
        "\n",
        "\n",
        "            #backward pass and compute gradianet\n",
        "            #dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y, output), batch_x)\n",
        "            dL_dW, dL_db  = self.backward(loss_func.derivative(batch_y[:len(y_pred)], y_pred), batch_x)\n",
        "\n",
        "\n",
        "            #update weights\n",
        "            max_grad_norm = 1.0  # Limits the maximum gradient value\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(len(self.layers)):\n",
        "              # Clip gradients\n",
        "              dL_dW[i] = np.clip(dL_dW[i], -max_grad_norm, max_grad_norm)\n",
        "              dL_db[i] = np.clip(dL_db[i], -max_grad_norm, max_grad_norm)\n",
        "\n",
        "              self.layers[i].W -= learning_rate * dL_dW[i]\n",
        "              #self.layers[i].b -= learning_rate * dL_db[i]\n",
        "              #self.layers[i].b -= learning_rate * np.array(dL_db[i]).flatten()\n",
        "              self.layers[i].b -= learning_rate * dL_db[i].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          training_losses.append(total_loss / len(train_x))\n",
        "\n",
        "          # Compute Validation Loss\n",
        "          val_output = self.forward(val_x)\n",
        "          val_loss = loss_func.loss(val_y, val_output)\n",
        "          validation_losses.append(np.mean(val_loss))\n",
        "\n",
        "          # Compute training accuracy at the end of the epoch\n",
        "          train_acc = compute_accuracy(self, train_x, train_y)\n",
        "          val_acc = compute_accuracy(self, val_x, val_y)\n",
        "\n",
        "          #print(f\"dL_dW max: {np.max([np.max(w) for w in dL_dW])}, min: {np.min([np.min(w) for w in dL_dW])}\")\n",
        "          #print(f\"dL_db max: {np.max([np.max(b) for b in dL_db])}, min: {np.min([np.min(b) for b in dL_db])}\")\n",
        "\n",
        "          print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {total_loss:.4f} - Training Acc: {train_acc:.2f}% - Validation Acc: {val_acc:.2f}% - Validation Loss: {validation_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return training_losses, validation_losses\n",
        "\n",
        "def compute_accuracy(model, X, y):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model.\n",
        "\n",
        "    :param model: The trained MLP model\n",
        "    :param X: Input features (numpy array)\n",
        "    :param y: True labels (numpy array)\n",
        "    :return: Accuracy in percentage (%)\n",
        "    \"\"\"\n",
        "    y_pred = model.forward(X)  # Forward pass\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to 0 or 1\n",
        "    accuracy = np.mean(y_pred_binary == y) * 100  # Compute accuracy\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "\n",
        "# Features: x1 and x2 sampled from normal distribution\n",
        "x1 = np.random.randn(num_samples)\n",
        "x2 = np.random.randn(num_samples)\n",
        "\n",
        "# Labels: 1 if x1 + x2 > 0, else 0\n",
        "y = (x1 + x2 > 0).astype(int)\n",
        "\n",
        "# Stack features into input matrix\n",
        "X = np.column_stack((x1, x2))\n",
        "Y = y.reshape(-1, 1)  # Convert to column vector\n",
        "\n",
        "# Split into train and validation sets\n",
        "split = int(0.8 * num_samples)\n",
        "train_x, val_x = X[:split], X[split:]\n",
        "train_y, val_y = Y[:split], Y[split:]\n",
        "\n",
        "# Plot the dataset\n",
        "plt.scatter(x1[y == 0], x2[y == 0], color='red', label='Class 0')\n",
        "plt.scatter(x1[y == 1], x2[y == 1], color='blue', label='Class 1')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.title('Synthetic Dataset for MLP Testing')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True)\n",
        "print(f\"Class distribution: {dict(zip(unique, counts))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "A5lUI7-9yHby",
        "outputId": "6859d363-9913-40c9-ebc9-8e1028e5b66a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: {0: 382, 1: 418}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an MLP model\n",
        "layers = [\n",
        "    Layer(fan_in=2, fan_out=8, activation_function=Relu()),  # Hidden layer with 4 neurons\n",
        "    Layer(fan_in=8, fan_out=8, activation_function=Relu()),  # Hidden layer with 4 neurons\n",
        "    Layer(fan_in=8, fan_out=1, activation_function=Sigmoid()) # Output layer with 1 neuron\n",
        "]\n",
        "\n",
        "mlp = MultilayerPerceptron(layers)\n",
        "\n",
        "# Train the model\n",
        "loss_function = BinaryCrossEntropy()\n",
        "train_losses, val_losses = mlp.train(\n",
        "    train_x=train_x, train_y=train_y,\n",
        "    val_x=val_x, val_y=val_y,\n",
        "    loss_func=loss_function,\n",
        "    learning_rate=0.01, batch_size=32, epochs=1000\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN7cBZFcyNJp",
        "outputId": "cdd4315a-babd-458f-a387-bcf0fee7d57f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 - Training Loss: 17.8600 - Training Acc: 47.75% - Validation Acc: 47.00% - Validation Loss: 0.6888\n",
            "Epoch 2/1000 - Training Loss: 17.6867 - Training Acc: 48.12% - Validation Acc: 47.00% - Validation Loss: 0.6823\n",
            "Epoch 3/1000 - Training Loss: 17.5166 - Training Acc: 48.38% - Validation Acc: 47.00% - Validation Loss: 0.6761\n",
            "Epoch 4/1000 - Training Loss: 17.3609 - Training Acc: 48.50% - Validation Acc: 48.00% - Validation Loss: 0.6702\n",
            "Epoch 5/1000 - Training Loss: 17.2105 - Training Acc: 49.00% - Validation Acc: 48.00% - Validation Loss: 0.6647\n",
            "Epoch 6/1000 - Training Loss: 17.0707 - Training Acc: 49.62% - Validation Acc: 48.00% - Validation Loss: 0.6594\n",
            "Epoch 7/1000 - Training Loss: 16.9347 - Training Acc: 49.62% - Validation Acc: 48.50% - Validation Loss: 0.6543\n",
            "Epoch 8/1000 - Training Loss: 16.8066 - Training Acc: 49.75% - Validation Acc: 49.00% - Validation Loss: 0.6495\n",
            "Epoch 9/1000 - Training Loss: 16.6852 - Training Acc: 50.38% - Validation Acc: 49.00% - Validation Loss: 0.6449\n",
            "Epoch 10/1000 - Training Loss: 16.5691 - Training Acc: 50.88% - Validation Acc: 50.00% - Validation Loss: 0.6405\n",
            "Epoch 11/1000 - Training Loss: 16.4574 - Training Acc: 51.25% - Validation Acc: 51.00% - Validation Loss: 0.6362\n",
            "Epoch 12/1000 - Training Loss: 16.3488 - Training Acc: 51.75% - Validation Acc: 51.00% - Validation Loss: 0.6322\n",
            "Epoch 13/1000 - Training Loss: 16.2464 - Training Acc: 52.12% - Validation Acc: 51.50% - Validation Loss: 0.6282\n",
            "Epoch 14/1000 - Training Loss: 16.1453 - Training Acc: 52.62% - Validation Acc: 51.50% - Validation Loss: 0.6244\n",
            "Epoch 15/1000 - Training Loss: 16.0476 - Training Acc: 53.25% - Validation Acc: 52.50% - Validation Loss: 0.6208\n",
            "Epoch 16/1000 - Training Loss: 15.9546 - Training Acc: 53.62% - Validation Acc: 52.50% - Validation Loss: 0.6172\n",
            "Epoch 17/1000 - Training Loss: 15.8636 - Training Acc: 54.12% - Validation Acc: 53.50% - Validation Loss: 0.6138\n",
            "Epoch 18/1000 - Training Loss: 15.7715 - Training Acc: 54.87% - Validation Acc: 54.00% - Validation Loss: 0.6104\n",
            "Epoch 19/1000 - Training Loss: 15.6884 - Training Acc: 55.25% - Validation Acc: 54.00% - Validation Loss: 0.6072\n",
            "Epoch 20/1000 - Training Loss: 15.6044 - Training Acc: 55.50% - Validation Acc: 54.00% - Validation Loss: 0.6041\n",
            "Epoch 21/1000 - Training Loss: 15.5244 - Training Acc: 55.88% - Validation Acc: 54.50% - Validation Loss: 0.6011\n",
            "Epoch 22/1000 - Training Loss: 15.4474 - Training Acc: 56.25% - Validation Acc: 54.50% - Validation Loss: 0.5982\n",
            "Epoch 23/1000 - Training Loss: 15.3703 - Training Acc: 56.25% - Validation Acc: 55.00% - Validation Loss: 0.5953\n",
            "Epoch 24/1000 - Training Loss: 15.2963 - Training Acc: 56.50% - Validation Acc: 56.00% - Validation Loss: 0.5925\n",
            "Epoch 25/1000 - Training Loss: 15.2227 - Training Acc: 57.12% - Validation Acc: 56.50% - Validation Loss: 0.5897\n",
            "Epoch 26/1000 - Training Loss: 15.1537 - Training Acc: 57.38% - Validation Acc: 56.50% - Validation Loss: 0.5870\n",
            "Epoch 27/1000 - Training Loss: 15.0840 - Training Acc: 57.75% - Validation Acc: 56.50% - Validation Loss: 0.5844\n",
            "Epoch 28/1000 - Training Loss: 15.0155 - Training Acc: 58.13% - Validation Acc: 57.00% - Validation Loss: 0.5818\n",
            "Epoch 29/1000 - Training Loss: 14.9513 - Training Acc: 58.25% - Validation Acc: 57.00% - Validation Loss: 0.5793\n",
            "Epoch 30/1000 - Training Loss: 14.8874 - Training Acc: 58.25% - Validation Acc: 57.50% - Validation Loss: 0.5769\n",
            "Epoch 31/1000 - Training Loss: 14.8226 - Training Acc: 58.38% - Validation Acc: 57.50% - Validation Loss: 0.5745\n",
            "Epoch 32/1000 - Training Loss: 14.7620 - Training Acc: 59.00% - Validation Acc: 57.50% - Validation Loss: 0.5721\n",
            "Epoch 33/1000 - Training Loss: 14.7015 - Training Acc: 59.50% - Validation Acc: 58.50% - Validation Loss: 0.5698\n",
            "Epoch 34/1000 - Training Loss: 14.6417 - Training Acc: 59.62% - Validation Acc: 59.00% - Validation Loss: 0.5676\n",
            "Epoch 35/1000 - Training Loss: 14.5841 - Training Acc: 59.88% - Validation Acc: 59.00% - Validation Loss: 0.5654\n",
            "Epoch 36/1000 - Training Loss: 14.5256 - Training Acc: 60.00% - Validation Acc: 59.50% - Validation Loss: 0.5633\n",
            "Epoch 37/1000 - Training Loss: 14.4700 - Training Acc: 60.12% - Validation Acc: 59.50% - Validation Loss: 0.5612\n",
            "Epoch 38/1000 - Training Loss: 14.4140 - Training Acc: 60.12% - Validation Acc: 59.50% - Validation Loss: 0.5591\n",
            "Epoch 39/1000 - Training Loss: 14.3606 - Training Acc: 60.25% - Validation Acc: 60.50% - Validation Loss: 0.5571\n",
            "Epoch 40/1000 - Training Loss: 14.3068 - Training Acc: 60.50% - Validation Acc: 61.00% - Validation Loss: 0.5551\n",
            "Epoch 41/1000 - Training Loss: 14.2535 - Training Acc: 60.62% - Validation Acc: 61.00% - Validation Loss: 0.5531\n",
            "Epoch 42/1000 - Training Loss: 14.2025 - Training Acc: 61.00% - Validation Acc: 61.00% - Validation Loss: 0.5511\n",
            "Epoch 43/1000 - Training Loss: 14.1522 - Training Acc: 61.00% - Validation Acc: 61.00% - Validation Loss: 0.5492\n",
            "Epoch 44/1000 - Training Loss: 14.1008 - Training Acc: 61.25% - Validation Acc: 61.50% - Validation Loss: 0.5474\n",
            "Epoch 45/1000 - Training Loss: 14.0531 - Training Acc: 61.38% - Validation Acc: 62.00% - Validation Loss: 0.5455\n",
            "Epoch 46/1000 - Training Loss: 14.0033 - Training Acc: 61.88% - Validation Acc: 62.00% - Validation Loss: 0.5438\n",
            "Epoch 47/1000 - Training Loss: 13.9546 - Training Acc: 61.88% - Validation Acc: 62.50% - Validation Loss: 0.5420\n",
            "Epoch 48/1000 - Training Loss: 13.9096 - Training Acc: 62.12% - Validation Acc: 62.50% - Validation Loss: 0.5403\n",
            "Epoch 49/1000 - Training Loss: 13.8611 - Training Acc: 62.38% - Validation Acc: 62.50% - Validation Loss: 0.5386\n",
            "Epoch 50/1000 - Training Loss: 13.8157 - Training Acc: 62.62% - Validation Acc: 62.50% - Validation Loss: 0.5369\n",
            "Epoch 51/1000 - Training Loss: 13.7699 - Training Acc: 62.62% - Validation Acc: 62.50% - Validation Loss: 0.5352\n",
            "Epoch 52/1000 - Training Loss: 13.7255 - Training Acc: 62.75% - Validation Acc: 62.50% - Validation Loss: 0.5336\n",
            "Epoch 53/1000 - Training Loss: 13.6811 - Training Acc: 63.25% - Validation Acc: 62.50% - Validation Loss: 0.5320\n",
            "Epoch 54/1000 - Training Loss: 13.6367 - Training Acc: 63.50% - Validation Acc: 62.50% - Validation Loss: 0.5304\n",
            "Epoch 55/1000 - Training Loss: 13.5953 - Training Acc: 63.62% - Validation Acc: 64.00% - Validation Loss: 0.5288\n",
            "Epoch 56/1000 - Training Loss: 13.5510 - Training Acc: 64.00% - Validation Acc: 64.00% - Validation Loss: 0.5272\n",
            "Epoch 57/1000 - Training Loss: 13.5095 - Training Acc: 64.12% - Validation Acc: 64.00% - Validation Loss: 0.5257\n",
            "Epoch 58/1000 - Training Loss: 13.4686 - Training Acc: 64.25% - Validation Acc: 64.50% - Validation Loss: 0.5242\n",
            "Epoch 59/1000 - Training Loss: 13.4282 - Training Acc: 64.38% - Validation Acc: 65.00% - Validation Loss: 0.5226\n",
            "Epoch 60/1000 - Training Loss: 13.3869 - Training Acc: 64.62% - Validation Acc: 65.00% - Validation Loss: 0.5211\n",
            "Epoch 61/1000 - Training Loss: 13.3470 - Training Acc: 64.62% - Validation Acc: 65.50% - Validation Loss: 0.5197\n",
            "Epoch 62/1000 - Training Loss: 13.3073 - Training Acc: 64.88% - Validation Acc: 65.50% - Validation Loss: 0.5182\n",
            "Epoch 63/1000 - Training Loss: 13.2673 - Training Acc: 65.12% - Validation Acc: 66.50% - Validation Loss: 0.5167\n",
            "Epoch 64/1000 - Training Loss: 13.2292 - Training Acc: 65.62% - Validation Acc: 66.50% - Validation Loss: 0.5153\n",
            "Epoch 65/1000 - Training Loss: 13.1896 - Training Acc: 65.62% - Validation Acc: 66.50% - Validation Loss: 0.5139\n",
            "Epoch 66/1000 - Training Loss: 13.1521 - Training Acc: 65.75% - Validation Acc: 66.50% - Validation Loss: 0.5125\n",
            "Epoch 67/1000 - Training Loss: 13.1136 - Training Acc: 66.12% - Validation Acc: 67.00% - Validation Loss: 0.5111\n",
            "Epoch 68/1000 - Training Loss: 13.0777 - Training Acc: 66.38% - Validation Acc: 67.00% - Validation Loss: 0.5097\n",
            "Epoch 69/1000 - Training Loss: 13.0403 - Training Acc: 66.62% - Validation Acc: 67.50% - Validation Loss: 0.5083\n",
            "Epoch 70/1000 - Training Loss: 13.0042 - Training Acc: 66.88% - Validation Acc: 68.00% - Validation Loss: 0.5069\n",
            "Epoch 71/1000 - Training Loss: 12.9688 - Training Acc: 66.88% - Validation Acc: 68.50% - Validation Loss: 0.5056\n",
            "Epoch 72/1000 - Training Loss: 12.9328 - Training Acc: 67.25% - Validation Acc: 70.50% - Validation Loss: 0.5042\n",
            "Epoch 73/1000 - Training Loss: 12.8964 - Training Acc: 67.62% - Validation Acc: 71.00% - Validation Loss: 0.5029\n",
            "Epoch 74/1000 - Training Loss: 12.8631 - Training Acc: 67.75% - Validation Acc: 71.50% - Validation Loss: 0.5016\n",
            "Epoch 75/1000 - Training Loss: 12.8299 - Training Acc: 67.88% - Validation Acc: 71.50% - Validation Loss: 0.5003\n",
            "Epoch 76/1000 - Training Loss: 12.7954 - Training Acc: 68.00% - Validation Acc: 71.50% - Validation Loss: 0.4990\n",
            "Epoch 77/1000 - Training Loss: 12.7596 - Training Acc: 68.12% - Validation Acc: 71.50% - Validation Loss: 0.4977\n",
            "Epoch 78/1000 - Training Loss: 12.7267 - Training Acc: 68.12% - Validation Acc: 71.50% - Validation Loss: 0.4964\n",
            "Epoch 79/1000 - Training Loss: 12.6935 - Training Acc: 68.50% - Validation Acc: 72.00% - Validation Loss: 0.4952\n",
            "Epoch 80/1000 - Training Loss: 12.6602 - Training Acc: 68.62% - Validation Acc: 72.00% - Validation Loss: 0.4939\n",
            "Epoch 81/1000 - Training Loss: 12.6287 - Training Acc: 69.00% - Validation Acc: 72.00% - Validation Loss: 0.4927\n",
            "Epoch 82/1000 - Training Loss: 12.5969 - Training Acc: 69.50% - Validation Acc: 72.00% - Validation Loss: 0.4914\n",
            "Epoch 83/1000 - Training Loss: 12.5631 - Training Acc: 69.88% - Validation Acc: 72.00% - Validation Loss: 0.4902\n",
            "Epoch 84/1000 - Training Loss: 12.5317 - Training Acc: 70.25% - Validation Acc: 72.50% - Validation Loss: 0.4890\n",
            "Epoch 85/1000 - Training Loss: 12.4984 - Training Acc: 70.62% - Validation Acc: 72.50% - Validation Loss: 0.4878\n",
            "Epoch 86/1000 - Training Loss: 12.4666 - Training Acc: 71.00% - Validation Acc: 72.00% - Validation Loss: 0.4866\n",
            "Epoch 87/1000 - Training Loss: 12.4379 - Training Acc: 71.25% - Validation Acc: 72.00% - Validation Loss: 0.4854\n",
            "Epoch 88/1000 - Training Loss: 12.4058 - Training Acc: 71.62% - Validation Acc: 72.50% - Validation Loss: 0.4842\n",
            "Epoch 89/1000 - Training Loss: 12.3752 - Training Acc: 72.12% - Validation Acc: 73.00% - Validation Loss: 0.4830\n",
            "Epoch 90/1000 - Training Loss: 12.3426 - Training Acc: 72.25% - Validation Acc: 73.50% - Validation Loss: 0.4818\n",
            "Epoch 91/1000 - Training Loss: 12.3132 - Training Acc: 72.25% - Validation Acc: 73.50% - Validation Loss: 0.4807\n",
            "Epoch 92/1000 - Training Loss: 12.2836 - Training Acc: 72.50% - Validation Acc: 73.50% - Validation Loss: 0.4795\n",
            "Epoch 93/1000 - Training Loss: 12.2525 - Training Acc: 72.62% - Validation Acc: 74.00% - Validation Loss: 0.4783\n",
            "Epoch 94/1000 - Training Loss: 12.2226 - Training Acc: 72.75% - Validation Acc: 74.50% - Validation Loss: 0.4772\n",
            "Epoch 95/1000 - Training Loss: 12.1929 - Training Acc: 72.88% - Validation Acc: 75.00% - Validation Loss: 0.4761\n",
            "Epoch 96/1000 - Training Loss: 12.1642 - Training Acc: 73.25% - Validation Acc: 75.00% - Validation Loss: 0.4749\n",
            "Epoch 97/1000 - Training Loss: 12.1349 - Training Acc: 73.50% - Validation Acc: 75.00% - Validation Loss: 0.4738\n",
            "Epoch 98/1000 - Training Loss: 12.1053 - Training Acc: 73.88% - Validation Acc: 75.50% - Validation Loss: 0.4727\n",
            "Epoch 99/1000 - Training Loss: 12.0754 - Training Acc: 74.12% - Validation Acc: 76.50% - Validation Loss: 0.4716\n",
            "Epoch 100/1000 - Training Loss: 12.0458 - Training Acc: 74.38% - Validation Acc: 76.50% - Validation Loss: 0.4705\n",
            "Epoch 101/1000 - Training Loss: 12.0176 - Training Acc: 74.88% - Validation Acc: 78.00% - Validation Loss: 0.4694\n",
            "Epoch 102/1000 - Training Loss: 11.9882 - Training Acc: 75.00% - Validation Acc: 78.00% - Validation Loss: 0.4683\n",
            "Epoch 103/1000 - Training Loss: 11.9616 - Training Acc: 75.50% - Validation Acc: 78.50% - Validation Loss: 0.4672\n",
            "Epoch 104/1000 - Training Loss: 11.9328 - Training Acc: 75.75% - Validation Acc: 79.50% - Validation Loss: 0.4661\n",
            "Epoch 105/1000 - Training Loss: 11.9051 - Training Acc: 76.38% - Validation Acc: 81.00% - Validation Loss: 0.4651\n",
            "Epoch 106/1000 - Training Loss: 11.8779 - Training Acc: 76.50% - Validation Acc: 81.50% - Validation Loss: 0.4640\n",
            "Epoch 107/1000 - Training Loss: 11.8467 - Training Acc: 76.75% - Validation Acc: 81.50% - Validation Loss: 0.4630\n",
            "Epoch 108/1000 - Training Loss: 11.8223 - Training Acc: 77.00% - Validation Acc: 82.00% - Validation Loss: 0.4619\n",
            "Epoch 109/1000 - Training Loss: 11.7952 - Training Acc: 77.00% - Validation Acc: 82.00% - Validation Loss: 0.4609\n",
            "Epoch 110/1000 - Training Loss: 11.7674 - Training Acc: 77.38% - Validation Acc: 82.00% - Validation Loss: 0.4599\n",
            "Epoch 111/1000 - Training Loss: 11.7396 - Training Acc: 77.62% - Validation Acc: 82.00% - Validation Loss: 0.4588\n",
            "Epoch 112/1000 - Training Loss: 11.7118 - Training Acc: 78.38% - Validation Acc: 82.00% - Validation Loss: 0.4578\n",
            "Epoch 113/1000 - Training Loss: 11.6854 - Training Acc: 78.75% - Validation Acc: 82.00% - Validation Loss: 0.4567\n",
            "Epoch 114/1000 - Training Loss: 11.6603 - Training Acc: 79.38% - Validation Acc: 82.00% - Validation Loss: 0.4557\n",
            "Epoch 115/1000 - Training Loss: 11.6337 - Training Acc: 79.62% - Validation Acc: 83.00% - Validation Loss: 0.4546\n",
            "Epoch 116/1000 - Training Loss: 11.6057 - Training Acc: 80.38% - Validation Acc: 83.00% - Validation Loss: 0.4536\n",
            "Epoch 117/1000 - Training Loss: 11.5798 - Training Acc: 80.88% - Validation Acc: 83.00% - Validation Loss: 0.4526\n",
            "Epoch 118/1000 - Training Loss: 11.5544 - Training Acc: 81.00% - Validation Acc: 83.00% - Validation Loss: 0.4515\n",
            "Epoch 119/1000 - Training Loss: 11.5270 - Training Acc: 81.25% - Validation Acc: 83.50% - Validation Loss: 0.4505\n",
            "Epoch 120/1000 - Training Loss: 11.5018 - Training Acc: 81.75% - Validation Acc: 83.50% - Validation Loss: 0.4495\n",
            "Epoch 121/1000 - Training Loss: 11.4765 - Training Acc: 81.88% - Validation Acc: 83.50% - Validation Loss: 0.4485\n",
            "Epoch 122/1000 - Training Loss: 11.4521 - Training Acc: 82.50% - Validation Acc: 84.00% - Validation Loss: 0.4475\n",
            "Epoch 123/1000 - Training Loss: 11.4265 - Training Acc: 83.00% - Validation Acc: 85.00% - Validation Loss: 0.4465\n",
            "Epoch 124/1000 - Training Loss: 11.4011 - Training Acc: 83.12% - Validation Acc: 85.00% - Validation Loss: 0.4455\n",
            "Epoch 125/1000 - Training Loss: 11.3773 - Training Acc: 83.88% - Validation Acc: 85.50% - Validation Loss: 0.4445\n",
            "Epoch 126/1000 - Training Loss: 11.3509 - Training Acc: 84.25% - Validation Acc: 85.50% - Validation Loss: 0.4435\n",
            "Epoch 127/1000 - Training Loss: 11.3261 - Training Acc: 84.88% - Validation Acc: 85.50% - Validation Loss: 0.4425\n",
            "Epoch 128/1000 - Training Loss: 11.3022 - Training Acc: 85.12% - Validation Acc: 86.50% - Validation Loss: 0.4415\n",
            "Epoch 129/1000 - Training Loss: 11.2769 - Training Acc: 85.38% - Validation Acc: 86.50% - Validation Loss: 0.4406\n",
            "Epoch 130/1000 - Training Loss: 11.2540 - Training Acc: 85.62% - Validation Acc: 86.50% - Validation Loss: 0.4396\n",
            "Epoch 131/1000 - Training Loss: 11.2293 - Training Acc: 86.00% - Validation Acc: 86.50% - Validation Loss: 0.4386\n",
            "Epoch 132/1000 - Training Loss: 11.2036 - Training Acc: 86.12% - Validation Acc: 86.50% - Validation Loss: 0.4377\n",
            "Epoch 133/1000 - Training Loss: 11.1805 - Training Acc: 86.50% - Validation Acc: 87.00% - Validation Loss: 0.4367\n",
            "Epoch 134/1000 - Training Loss: 11.1557 - Training Acc: 86.75% - Validation Acc: 87.00% - Validation Loss: 0.4357\n",
            "Epoch 135/1000 - Training Loss: 11.1308 - Training Acc: 86.88% - Validation Acc: 87.00% - Validation Loss: 0.4347\n",
            "Epoch 136/1000 - Training Loss: 11.1058 - Training Acc: 87.12% - Validation Acc: 87.50% - Validation Loss: 0.4338\n",
            "Epoch 137/1000 - Training Loss: 11.0837 - Training Acc: 87.38% - Validation Acc: 88.00% - Validation Loss: 0.4328\n",
            "Epoch 138/1000 - Training Loss: 11.0600 - Training Acc: 87.50% - Validation Acc: 88.50% - Validation Loss: 0.4318\n",
            "Epoch 139/1000 - Training Loss: 11.0362 - Training Acc: 87.62% - Validation Acc: 88.50% - Validation Loss: 0.4309\n",
            "Epoch 140/1000 - Training Loss: 11.0110 - Training Acc: 87.62% - Validation Acc: 88.50% - Validation Loss: 0.4299\n",
            "Epoch 141/1000 - Training Loss: 10.9884 - Training Acc: 87.88% - Validation Acc: 88.50% - Validation Loss: 0.4290\n",
            "Epoch 142/1000 - Training Loss: 10.9656 - Training Acc: 88.12% - Validation Acc: 88.50% - Validation Loss: 0.4280\n",
            "Epoch 143/1000 - Training Loss: 10.9399 - Training Acc: 88.00% - Validation Acc: 89.50% - Validation Loss: 0.4270\n",
            "Epoch 144/1000 - Training Loss: 10.9190 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4261\n",
            "Epoch 145/1000 - Training Loss: 10.8955 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4251\n",
            "Epoch 146/1000 - Training Loss: 10.8718 - Training Acc: 88.25% - Validation Acc: 89.50% - Validation Loss: 0.4242\n",
            "Epoch 147/1000 - Training Loss: 10.8505 - Training Acc: 88.38% - Validation Acc: 89.50% - Validation Loss: 0.4232\n",
            "Epoch 148/1000 - Training Loss: 10.8263 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4223\n",
            "Epoch 149/1000 - Training Loss: 10.8020 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4213\n",
            "Epoch 150/1000 - Training Loss: 10.7802 - Training Acc: 88.38% - Validation Acc: 90.00% - Validation Loss: 0.4204\n",
            "Epoch 151/1000 - Training Loss: 10.7586 - Training Acc: 88.50% - Validation Acc: 90.00% - Validation Loss: 0.4195\n",
            "Epoch 152/1000 - Training Loss: 10.7369 - Training Acc: 88.50% - Validation Acc: 90.00% - Validation Loss: 0.4185\n",
            "Epoch 153/1000 - Training Loss: 10.7137 - Training Acc: 88.50% - Validation Acc: 89.50% - Validation Loss: 0.4176\n",
            "Epoch 154/1000 - Training Loss: 10.6911 - Training Acc: 88.75% - Validation Acc: 89.50% - Validation Loss: 0.4167\n",
            "Epoch 155/1000 - Training Loss: 10.6692 - Training Acc: 89.00% - Validation Acc: 90.00% - Validation Loss: 0.4158\n",
            "Epoch 156/1000 - Training Loss: 10.6469 - Training Acc: 89.25% - Validation Acc: 90.00% - Validation Loss: 0.4149\n",
            "Epoch 157/1000 - Training Loss: 10.6255 - Training Acc: 89.25% - Validation Acc: 90.00% - Validation Loss: 0.4140\n",
            "Epoch 158/1000 - Training Loss: 10.6050 - Training Acc: 89.38% - Validation Acc: 90.00% - Validation Loss: 0.4131\n",
            "Epoch 159/1000 - Training Loss: 10.5830 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4122\n",
            "Epoch 160/1000 - Training Loss: 10.5611 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4113\n",
            "Epoch 161/1000 - Training Loss: 10.5413 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4104\n",
            "Epoch 162/1000 - Training Loss: 10.5216 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4095\n",
            "Epoch 163/1000 - Training Loss: 10.5002 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4086\n",
            "Epoch 164/1000 - Training Loss: 10.4794 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4077\n",
            "Epoch 165/1000 - Training Loss: 10.4560 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4069\n",
            "Epoch 166/1000 - Training Loss: 10.4348 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4060\n",
            "Epoch 167/1000 - Training Loss: 10.4153 - Training Acc: 89.50% - Validation Acc: 90.00% - Validation Loss: 0.4051\n",
            "Epoch 168/1000 - Training Loss: 10.3955 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4043\n",
            "Epoch 169/1000 - Training Loss: 10.3741 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4034\n",
            "Epoch 170/1000 - Training Loss: 10.3526 - Training Acc: 89.62% - Validation Acc: 90.00% - Validation Loss: 0.4026\n",
            "Epoch 171/1000 - Training Loss: 10.3329 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4017\n",
            "Epoch 172/1000 - Training Loss: 10.3129 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4008\n",
            "Epoch 173/1000 - Training Loss: 10.2932 - Training Acc: 89.88% - Validation Acc: 90.50% - Validation Loss: 0.4000\n",
            "Epoch 174/1000 - Training Loss: 10.2718 - Training Acc: 90.00% - Validation Acc: 90.50% - Validation Loss: 0.3991\n",
            "Epoch 175/1000 - Training Loss: 10.2526 - Training Acc: 90.25% - Validation Acc: 90.50% - Validation Loss: 0.3983\n",
            "Epoch 176/1000 - Training Loss: 10.2313 - Training Acc: 90.25% - Validation Acc: 91.00% - Validation Loss: 0.3974\n",
            "Epoch 177/1000 - Training Loss: 10.2122 - Training Acc: 90.25% - Validation Acc: 91.00% - Validation Loss: 0.3965\n",
            "Epoch 178/1000 - Training Loss: 10.1928 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3957\n",
            "Epoch 179/1000 - Training Loss: 10.1737 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3949\n",
            "Epoch 180/1000 - Training Loss: 10.1519 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3940\n",
            "Epoch 181/1000 - Training Loss: 10.1319 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3932\n",
            "Epoch 182/1000 - Training Loss: 10.1127 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3924\n",
            "Epoch 183/1000 - Training Loss: 10.0951 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3915\n",
            "Epoch 184/1000 - Training Loss: 10.0760 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3907\n",
            "Epoch 185/1000 - Training Loss: 10.0535 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3899\n",
            "Epoch 186/1000 - Training Loss: 10.0359 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3891\n",
            "Epoch 187/1000 - Training Loss: 10.0160 - Training Acc: 90.38% - Validation Acc: 91.00% - Validation Loss: 0.3883\n",
            "Epoch 188/1000 - Training Loss: 9.9962 - Training Acc: 90.50% - Validation Acc: 91.00% - Validation Loss: 0.3875\n",
            "Epoch 189/1000 - Training Loss: 9.9776 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3867\n",
            "Epoch 190/1000 - Training Loss: 9.9581 - Training Acc: 90.62% - Validation Acc: 91.00% - Validation Loss: 0.3859\n",
            "Epoch 191/1000 - Training Loss: 9.9375 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3851\n",
            "Epoch 192/1000 - Training Loss: 9.9192 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3843\n",
            "Epoch 193/1000 - Training Loss: 9.8998 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3835\n",
            "Epoch 194/1000 - Training Loss: 9.8816 - Training Acc: 90.50% - Validation Acc: 91.50% - Validation Loss: 0.3828\n",
            "Epoch 195/1000 - Training Loss: 9.8619 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3820\n",
            "Epoch 196/1000 - Training Loss: 9.8446 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3812\n",
            "Epoch 197/1000 - Training Loss: 9.8228 - Training Acc: 90.75% - Validation Acc: 91.50% - Validation Loss: 0.3804\n",
            "Epoch 198/1000 - Training Loss: 9.8058 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3796\n",
            "Epoch 199/1000 - Training Loss: 9.7861 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3789\n",
            "Epoch 200/1000 - Training Loss: 9.7658 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3781\n",
            "Epoch 201/1000 - Training Loss: 9.7482 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3773\n",
            "Epoch 202/1000 - Training Loss: 9.7297 - Training Acc: 90.62% - Validation Acc: 91.50% - Validation Loss: 0.3765\n",
            "Epoch 203/1000 - Training Loss: 9.7090 - Training Acc: 90.62% - Validation Acc: 92.00% - Validation Loss: 0.3758\n",
            "Epoch 204/1000 - Training Loss: 9.6923 - Training Acc: 90.62% - Validation Acc: 92.00% - Validation Loss: 0.3750\n",
            "Epoch 205/1000 - Training Loss: 9.6732 - Training Acc: 90.50% - Validation Acc: 92.00% - Validation Loss: 0.3743\n",
            "Epoch 206/1000 - Training Loss: 9.6528 - Training Acc: 90.38% - Validation Acc: 92.00% - Validation Loss: 0.3735\n",
            "Epoch 207/1000 - Training Loss: 9.6343 - Training Acc: 90.38% - Validation Acc: 92.50% - Validation Loss: 0.3728\n",
            "Epoch 208/1000 - Training Loss: 9.6147 - Training Acc: 90.38% - Validation Acc: 92.50% - Validation Loss: 0.3720\n",
            "Epoch 209/1000 - Training Loss: 9.5970 - Training Acc: 90.50% - Validation Acc: 92.50% - Validation Loss: 0.3713\n",
            "Epoch 210/1000 - Training Loss: 9.5770 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3706\n",
            "Epoch 211/1000 - Training Loss: 9.5591 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3698\n",
            "Epoch 212/1000 - Training Loss: 9.5403 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3691\n",
            "Epoch 213/1000 - Training Loss: 9.5197 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3684\n",
            "Epoch 214/1000 - Training Loss: 9.5040 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3677\n",
            "Epoch 215/1000 - Training Loss: 9.4836 - Training Acc: 90.62% - Validation Acc: 92.50% - Validation Loss: 0.3670\n",
            "Epoch 216/1000 - Training Loss: 9.4661 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3662\n",
            "Epoch 217/1000 - Training Loss: 9.4464 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3655\n",
            "Epoch 218/1000 - Training Loss: 9.4265 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3649\n",
            "Epoch 219/1000 - Training Loss: 9.4067 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3642\n",
            "Epoch 220/1000 - Training Loss: 9.3917 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3635\n",
            "Epoch 221/1000 - Training Loss: 9.3718 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3628\n",
            "Epoch 222/1000 - Training Loss: 9.3531 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3621\n",
            "Epoch 223/1000 - Training Loss: 9.3381 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3614\n",
            "Epoch 224/1000 - Training Loss: 9.3156 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3608\n",
            "Epoch 225/1000 - Training Loss: 9.2991 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3601\n",
            "Epoch 226/1000 - Training Loss: 9.2814 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3594\n",
            "Epoch 227/1000 - Training Loss: 9.2622 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3588\n",
            "Epoch 228/1000 - Training Loss: 9.2481 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3581\n",
            "Epoch 229/1000 - Training Loss: 9.2271 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3575\n",
            "Epoch 230/1000 - Training Loss: 9.2094 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3569\n",
            "Epoch 231/1000 - Training Loss: 9.1926 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3562\n",
            "Epoch 232/1000 - Training Loss: 9.1750 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3555\n",
            "Epoch 233/1000 - Training Loss: 9.1585 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3549\n",
            "Epoch 234/1000 - Training Loss: 9.1426 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3543\n",
            "Epoch 235/1000 - Training Loss: 9.1258 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3536\n",
            "Epoch 236/1000 - Training Loss: 9.1095 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3530\n",
            "Epoch 237/1000 - Training Loss: 9.0916 - Training Acc: 90.75% - Validation Acc: 92.50% - Validation Loss: 0.3524\n",
            "Epoch 238/1000 - Training Loss: 9.0748 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3517\n",
            "Epoch 239/1000 - Training Loss: 9.0548 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3511\n",
            "Epoch 240/1000 - Training Loss: 9.0395 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3505\n",
            "Epoch 241/1000 - Training Loss: 9.0248 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3498\n",
            "Epoch 242/1000 - Training Loss: 9.0074 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3492\n",
            "Epoch 243/1000 - Training Loss: 8.9904 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3486\n",
            "Epoch 244/1000 - Training Loss: 8.9749 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3480\n",
            "Epoch 245/1000 - Training Loss: 8.9561 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3474\n",
            "Epoch 246/1000 - Training Loss: 8.9435 - Training Acc: 90.88% - Validation Acc: 92.50% - Validation Loss: 0.3468\n",
            "Epoch 247/1000 - Training Loss: 8.9246 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3462\n",
            "Epoch 248/1000 - Training Loss: 8.9093 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3456\n",
            "Epoch 249/1000 - Training Loss: 8.8962 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3450\n",
            "Epoch 250/1000 - Training Loss: 8.8790 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3444\n",
            "Epoch 251/1000 - Training Loss: 8.8607 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3438\n",
            "Epoch 252/1000 - Training Loss: 8.8485 - Training Acc: 90.62% - Validation Acc: 93.00% - Validation Loss: 0.3433\n",
            "Epoch 253/1000 - Training Loss: 8.8325 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3427\n",
            "Epoch 254/1000 - Training Loss: 8.8170 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3421\n",
            "Epoch 255/1000 - Training Loss: 8.8041 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3416\n",
            "Epoch 256/1000 - Training Loss: 8.7863 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3410\n",
            "Epoch 257/1000 - Training Loss: 8.7707 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3404\n",
            "Epoch 258/1000 - Training Loss: 8.7574 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3399\n",
            "Epoch 259/1000 - Training Loss: 8.7447 - Training Acc: 90.62% - Validation Acc: 93.50% - Validation Loss: 0.3393\n",
            "Epoch 260/1000 - Training Loss: 8.7265 - Training Acc: 90.75% - Validation Acc: 93.50% - Validation Loss: 0.3388\n",
            "Epoch 261/1000 - Training Loss: 8.7095 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3383\n",
            "Epoch 262/1000 - Training Loss: 8.6959 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3377\n",
            "Epoch 263/1000 - Training Loss: 8.6836 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3372\n",
            "Epoch 264/1000 - Training Loss: 8.6679 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3367\n",
            "Epoch 265/1000 - Training Loss: 8.6541 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3362\n",
            "Epoch 266/1000 - Training Loss: 8.6414 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3357\n",
            "Epoch 267/1000 - Training Loss: 8.6242 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3352\n",
            "Epoch 268/1000 - Training Loss: 8.6085 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3347\n",
            "Epoch 269/1000 - Training Loss: 8.5966 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3342\n",
            "Epoch 270/1000 - Training Loss: 8.5846 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3338\n",
            "Epoch 271/1000 - Training Loss: 8.5713 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3333\n",
            "Epoch 272/1000 - Training Loss: 8.5571 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3328\n",
            "Epoch 273/1000 - Training Loss: 8.5413 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3324\n",
            "Epoch 274/1000 - Training Loss: 8.5294 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3320\n",
            "Epoch 275/1000 - Training Loss: 8.5142 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3316\n",
            "Epoch 276/1000 - Training Loss: 8.5034 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3312\n",
            "Epoch 277/1000 - Training Loss: 8.4850 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3309\n",
            "Epoch 278/1000 - Training Loss: 8.4774 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3305\n",
            "Epoch 279/1000 - Training Loss: 8.4627 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3302\n",
            "Epoch 280/1000 - Training Loss: 8.4490 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3298\n",
            "Epoch 281/1000 - Training Loss: 8.4349 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3295\n",
            "Epoch 282/1000 - Training Loss: 8.4251 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3291\n",
            "Epoch 283/1000 - Training Loss: 8.4148 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3288\n",
            "Epoch 284/1000 - Training Loss: 8.4028 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3285\n",
            "Epoch 285/1000 - Training Loss: 8.3928 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3281\n",
            "Epoch 286/1000 - Training Loss: 8.3763 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3278\n",
            "Epoch 287/1000 - Training Loss: 8.3673 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3275\n",
            "Epoch 288/1000 - Training Loss: 8.3610 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3272\n",
            "Epoch 289/1000 - Training Loss: 8.3487 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3269\n",
            "Epoch 290/1000 - Training Loss: 8.3412 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3266\n",
            "Epoch 291/1000 - Training Loss: 8.3248 - Training Acc: 90.88% - Validation Acc: 93.00% - Validation Loss: 0.3264\n",
            "Epoch 292/1000 - Training Loss: 8.3173 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3261\n",
            "Epoch 293/1000 - Training Loss: 8.3082 - Training Acc: 90.75% - Validation Acc: 93.00% - Validation Loss: 0.3259\n",
            "Epoch 294/1000 - Training Loss: 8.2938 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3256\n",
            "Epoch 295/1000 - Training Loss: 8.2873 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3254\n",
            "Epoch 296/1000 - Training Loss: 8.2814 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3252\n",
            "Epoch 297/1000 - Training Loss: 8.2703 - Training Acc: 90.88% - Validation Acc: 93.50% - Validation Loss: 0.3250\n",
            "Epoch 298/1000 - Training Loss: 8.2601 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3249\n",
            "Epoch 299/1000 - Training Loss: 8.2526 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3247\n",
            "Epoch 300/1000 - Training Loss: 8.2441 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3246\n",
            "Epoch 301/1000 - Training Loss: 8.2335 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3245\n",
            "Epoch 302/1000 - Training Loss: 8.2298 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3244\n",
            "Epoch 303/1000 - Training Loss: 8.2192 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3244\n",
            "Epoch 304/1000 - Training Loss: 8.2121 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3243\n",
            "Epoch 305/1000 - Training Loss: 8.2050 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3243\n",
            "Epoch 306/1000 - Training Loss: 8.1994 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3242\n",
            "Epoch 307/1000 - Training Loss: 8.1923 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3242\n",
            "Epoch 308/1000 - Training Loss: 8.1864 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3242\n",
            "Epoch 309/1000 - Training Loss: 8.1750 - Training Acc: 91.00% - Validation Acc: 93.50% - Validation Loss: 0.3243\n",
            "Epoch 310/1000 - Training Loss: 8.1702 - Training Acc: 91.00% - Validation Acc: 93.00% - Validation Loss: 0.3243\n",
            "Epoch 311/1000 - Training Loss: 8.1658 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3243\n",
            "Epoch 312/1000 - Training Loss: 8.1599 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3244\n",
            "Epoch 313/1000 - Training Loss: 8.1555 - Training Acc: 91.12% - Validation Acc: 93.00% - Validation Loss: 0.3244\n",
            "Epoch 314/1000 - Training Loss: 8.1538 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3244\n",
            "Epoch 315/1000 - Training Loss: 8.1430 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3245\n",
            "Epoch 316/1000 - Training Loss: 8.1373 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3246\n",
            "Epoch 317/1000 - Training Loss: 8.1383 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3246\n",
            "Epoch 318/1000 - Training Loss: 8.1309 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3247\n",
            "Epoch 319/1000 - Training Loss: 8.1250 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3248\n",
            "Epoch 320/1000 - Training Loss: 8.1208 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3248\n",
            "Epoch 321/1000 - Training Loss: 8.1142 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3249\n",
            "Epoch 322/1000 - Training Loss: 8.1148 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3250\n",
            "Epoch 323/1000 - Training Loss: 8.1097 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3251\n",
            "Epoch 324/1000 - Training Loss: 8.1048 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3251\n",
            "Epoch 325/1000 - Training Loss: 8.1039 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3252\n",
            "Epoch 326/1000 - Training Loss: 8.0987 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3253\n",
            "Epoch 327/1000 - Training Loss: 8.0950 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3254\n",
            "Epoch 328/1000 - Training Loss: 8.0974 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3255\n",
            "Epoch 329/1000 - Training Loss: 8.0950 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3256\n",
            "Epoch 330/1000 - Training Loss: 8.0907 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3257\n",
            "Epoch 331/1000 - Training Loss: 8.0905 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3257\n",
            "Epoch 332/1000 - Training Loss: 8.0858 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3258\n",
            "Epoch 333/1000 - Training Loss: 8.0793 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3259\n",
            "Epoch 334/1000 - Training Loss: 8.0841 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3260\n",
            "Epoch 335/1000 - Training Loss: 8.0841 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3261\n",
            "Epoch 336/1000 - Training Loss: 8.0805 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3262\n",
            "Epoch 337/1000 - Training Loss: 8.0766 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3262\n",
            "Epoch 338/1000 - Training Loss: 8.0747 - Training Acc: 91.00% - Validation Acc: 92.50% - Validation Loss: 0.3263\n",
            "Epoch 339/1000 - Training Loss: 8.0736 - Training Acc: 91.00% - Validation Acc: 92.50% - Validation Loss: 0.3264\n",
            "Epoch 340/1000 - Training Loss: 8.0694 - Training Acc: 91.00% - Validation Acc: 92.50% - Validation Loss: 0.3265\n",
            "Epoch 341/1000 - Training Loss: 8.0656 - Training Acc: 91.00% - Validation Acc: 92.50% - Validation Loss: 0.3265\n",
            "Epoch 342/1000 - Training Loss: 8.0640 - Training Acc: 91.00% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 343/1000 - Training Loss: 8.0598 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 344/1000 - Training Loss: 8.0628 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 345/1000 - Training Loss: 8.0572 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 346/1000 - Training Loss: 8.0556 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 347/1000 - Training Loss: 8.0595 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 348/1000 - Training Loss: 8.0441 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 349/1000 - Training Loss: 8.0463 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 350/1000 - Training Loss: 8.0404 - Training Acc: 91.12% - Validation Acc: 92.50% - Validation Loss: 0.3267\n",
            "Epoch 351/1000 - Training Loss: 8.0402 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 352/1000 - Training Loss: 8.0357 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 353/1000 - Training Loss: 8.0314 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3266\n",
            "Epoch 354/1000 - Training Loss: 8.0296 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3265\n",
            "Epoch 355/1000 - Training Loss: 8.0217 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3265\n",
            "Epoch 356/1000 - Training Loss: 8.0176 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3264\n",
            "Epoch 357/1000 - Training Loss: 8.0155 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3263\n",
            "Epoch 358/1000 - Training Loss: 8.0057 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3263\n",
            "Epoch 359/1000 - Training Loss: 8.0001 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3262\n",
            "Epoch 360/1000 - Training Loss: 7.9989 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3261\n",
            "Epoch 361/1000 - Training Loss: 7.9889 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3260\n",
            "Epoch 362/1000 - Training Loss: 7.9871 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3259\n",
            "Epoch 363/1000 - Training Loss: 7.9792 - Training Acc: 91.25% - Validation Acc: 92.50% - Validation Loss: 0.3257\n",
            "Epoch 364/1000 - Training Loss: 7.9694 - Training Acc: 91.38% - Validation Acc: 92.50% - Validation Loss: 0.3256\n",
            "Epoch 365/1000 - Training Loss: 7.9622 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3255\n",
            "Epoch 366/1000 - Training Loss: 7.9559 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3253\n",
            "Epoch 367/1000 - Training Loss: 7.9549 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3252\n",
            "Epoch 368/1000 - Training Loss: 7.9428 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3251\n",
            "Epoch 369/1000 - Training Loss: 7.9335 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3249\n",
            "Epoch 370/1000 - Training Loss: 7.9287 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3248\n",
            "Epoch 371/1000 - Training Loss: 7.9235 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3246\n",
            "Epoch 372/1000 - Training Loss: 7.9204 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3244\n",
            "Epoch 373/1000 - Training Loss: 7.9003 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3242\n",
            "Epoch 374/1000 - Training Loss: 7.8988 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3240\n",
            "Epoch 375/1000 - Training Loss: 7.8934 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3238\n",
            "Epoch 376/1000 - Training Loss: 7.8800 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3236\n",
            "Epoch 377/1000 - Training Loss: 7.8654 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3234\n",
            "Epoch 378/1000 - Training Loss: 7.8598 - Training Acc: 91.50% - Validation Acc: 93.00% - Validation Loss: 0.3232\n",
            "Epoch 379/1000 - Training Loss: 7.8504 - Training Acc: 91.62% - Validation Acc: 93.00% - Validation Loss: 0.3230\n",
            "Epoch 380/1000 - Training Loss: 7.8383 - Training Acc: 91.62% - Validation Acc: 93.00% - Validation Loss: 0.3228\n",
            "Epoch 381/1000 - Training Loss: 7.8226 - Training Acc: 91.62% - Validation Acc: 93.00% - Validation Loss: 0.3225\n",
            "Epoch 382/1000 - Training Loss: 7.8182 - Training Acc: 91.62% - Validation Acc: 93.00% - Validation Loss: 0.3223\n",
            "Epoch 383/1000 - Training Loss: 7.8040 - Training Acc: 91.75% - Validation Acc: 93.00% - Validation Loss: 0.3221\n",
            "Epoch 384/1000 - Training Loss: 7.8075 - Training Acc: 91.75% - Validation Acc: 93.00% - Validation Loss: 0.3218\n",
            "Epoch 385/1000 - Training Loss: 7.7938 - Training Acc: 91.75% - Validation Acc: 93.00% - Validation Loss: 0.3215\n",
            "Epoch 386/1000 - Training Loss: 7.7741 - Training Acc: 91.75% - Validation Acc: 93.00% - Validation Loss: 0.3212\n",
            "Epoch 387/1000 - Training Loss: 7.7573 - Training Acc: 91.75% - Validation Acc: 93.00% - Validation Loss: 0.3210\n",
            "Epoch 388/1000 - Training Loss: 7.7556 - Training Acc: 91.88% - Validation Acc: 93.00% - Validation Loss: 0.3207\n",
            "Epoch 389/1000 - Training Loss: 7.7410 - Training Acc: 91.88% - Validation Acc: 93.00% - Validation Loss: 0.3204\n",
            "Epoch 390/1000 - Training Loss: 7.7320 - Training Acc: 91.88% - Validation Acc: 93.00% - Validation Loss: 0.3202\n",
            "Epoch 391/1000 - Training Loss: 7.7209 - Training Acc: 91.88% - Validation Acc: 93.00% - Validation Loss: 0.3199\n",
            "Epoch 392/1000 - Training Loss: 7.7084 - Training Acc: 92.00% - Validation Acc: 93.00% - Validation Loss: 0.3196\n",
            "Epoch 393/1000 - Training Loss: 7.6944 - Training Acc: 92.00% - Validation Acc: 93.00% - Validation Loss: 0.3193\n",
            "Epoch 394/1000 - Training Loss: 7.6815 - Training Acc: 92.00% - Validation Acc: 93.00% - Validation Loss: 0.3190\n",
            "Epoch 395/1000 - Training Loss: 7.6647 - Training Acc: 92.12% - Validation Acc: 93.00% - Validation Loss: 0.3187\n",
            "Epoch 396/1000 - Training Loss: 7.6634 - Training Acc: 92.25% - Validation Acc: 93.00% - Validation Loss: 0.3183\n",
            "Epoch 397/1000 - Training Loss: 7.6480 - Training Acc: 92.25% - Validation Acc: 93.00% - Validation Loss: 0.3180\n",
            "Epoch 398/1000 - Training Loss: 7.6351 - Training Acc: 92.25% - Validation Acc: 93.00% - Validation Loss: 0.3177\n",
            "Epoch 399/1000 - Training Loss: 7.6131 - Training Acc: 92.25% - Validation Acc: 93.00% - Validation Loss: 0.3174\n",
            "Epoch 400/1000 - Training Loss: 7.6072 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3170\n",
            "Epoch 401/1000 - Training Loss: 7.5905 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3167\n",
            "Epoch 402/1000 - Training Loss: 7.5848 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3163\n",
            "Epoch 403/1000 - Training Loss: 7.5698 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3159\n",
            "Epoch 404/1000 - Training Loss: 7.5507 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3156\n",
            "Epoch 405/1000 - Training Loss: 7.5359 - Training Acc: 92.38% - Validation Acc: 93.00% - Validation Loss: 0.3152\n",
            "Epoch 406/1000 - Training Loss: 7.5291 - Training Acc: 92.38% - Validation Acc: 93.50% - Validation Loss: 0.3148\n",
            "Epoch 407/1000 - Training Loss: 7.5025 - Training Acc: 92.62% - Validation Acc: 93.50% - Validation Loss: 0.3145\n",
            "Epoch 408/1000 - Training Loss: 7.5001 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3141\n",
            "Epoch 409/1000 - Training Loss: 7.4800 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3137\n",
            "Epoch 410/1000 - Training Loss: 7.4651 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3133\n",
            "Epoch 411/1000 - Training Loss: 7.4569 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3128\n",
            "Epoch 412/1000 - Training Loss: 7.4449 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3124\n",
            "Epoch 413/1000 - Training Loss: 7.4203 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3120\n",
            "Epoch 414/1000 - Training Loss: 7.4001 - Training Acc: 92.75% - Validation Acc: 93.50% - Validation Loss: 0.3116\n",
            "Epoch 415/1000 - Training Loss: 7.3839 - Training Acc: 92.88% - Validation Acc: 93.50% - Validation Loss: 0.3112\n",
            "Epoch 416/1000 - Training Loss: 7.3751 - Training Acc: 92.88% - Validation Acc: 93.50% - Validation Loss: 0.3108\n",
            "Epoch 417/1000 - Training Loss: 7.3625 - Training Acc: 92.88% - Validation Acc: 93.50% - Validation Loss: 0.3104\n",
            "Epoch 418/1000 - Training Loss: 7.3444 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3099\n",
            "Epoch 419/1000 - Training Loss: 7.3331 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3095\n",
            "Epoch 420/1000 - Training Loss: 7.3192 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3091\n",
            "Epoch 421/1000 - Training Loss: 7.3023 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3086\n",
            "Epoch 422/1000 - Training Loss: 7.2819 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3082\n",
            "Epoch 423/1000 - Training Loss: 7.2770 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3077\n",
            "Epoch 424/1000 - Training Loss: 7.2603 - Training Acc: 93.00% - Validation Acc: 93.50% - Validation Loss: 0.3073\n",
            "Epoch 425/1000 - Training Loss: 7.2487 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3068\n",
            "Epoch 426/1000 - Training Loss: 7.2196 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3063\n",
            "Epoch 427/1000 - Training Loss: 7.2158 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3058\n",
            "Epoch 428/1000 - Training Loss: 7.1898 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3054\n",
            "Epoch 429/1000 - Training Loss: 7.1769 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3049\n",
            "Epoch 430/1000 - Training Loss: 7.1701 - Training Acc: 93.12% - Validation Acc: 93.50% - Validation Loss: 0.3044\n",
            "Epoch 431/1000 - Training Loss: 7.1557 - Training Acc: 93.25% - Validation Acc: 93.50% - Validation Loss: 0.3039\n",
            "Epoch 432/1000 - Training Loss: 7.1313 - Training Acc: 93.25% - Validation Acc: 93.50% - Validation Loss: 0.3034\n",
            "Epoch 433/1000 - Training Loss: 7.1234 - Training Acc: 93.50% - Validation Acc: 93.50% - Validation Loss: 0.3029\n",
            "Epoch 434/1000 - Training Loss: 7.0991 - Training Acc: 93.50% - Validation Acc: 93.50% - Validation Loss: 0.3023\n",
            "Epoch 435/1000 - Training Loss: 7.0671 - Training Acc: 93.50% - Validation Acc: 93.50% - Validation Loss: 0.3019\n",
            "Epoch 436/1000 - Training Loss: 7.0590 - Training Acc: 93.62% - Validation Acc: 93.50% - Validation Loss: 0.3014\n",
            "Epoch 437/1000 - Training Loss: 7.0444 - Training Acc: 93.62% - Validation Acc: 93.50% - Validation Loss: 0.3009\n",
            "Epoch 438/1000 - Training Loss: 7.0288 - Training Acc: 93.62% - Validation Acc: 93.50% - Validation Loss: 0.3003\n",
            "Epoch 439/1000 - Training Loss: 7.0125 - Training Acc: 93.75% - Validation Acc: 93.50% - Validation Loss: 0.2998\n",
            "Epoch 440/1000 - Training Loss: 6.9881 - Training Acc: 93.88% - Validation Acc: 93.50% - Validation Loss: 0.2993\n",
            "Epoch 441/1000 - Training Loss: 6.9853 - Training Acc: 94.00% - Validation Acc: 93.50% - Validation Loss: 0.2987\n",
            "Epoch 442/1000 - Training Loss: 6.9709 - Training Acc: 94.00% - Validation Acc: 93.50% - Validation Loss: 0.2982\n",
            "Epoch 443/1000 - Training Loss: 6.9411 - Training Acc: 94.00% - Validation Acc: 93.50% - Validation Loss: 0.2977\n",
            "Epoch 444/1000 - Training Loss: 6.9324 - Training Acc: 94.00% - Validation Acc: 93.50% - Validation Loss: 0.2971\n",
            "Epoch 445/1000 - Training Loss: 6.9132 - Training Acc: 94.00% - Validation Acc: 93.50% - Validation Loss: 0.2966\n",
            "Epoch 446/1000 - Training Loss: 6.8951 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2960\n",
            "Epoch 447/1000 - Training Loss: 6.8767 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2955\n",
            "Epoch 448/1000 - Training Loss: 6.8641 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2949\n",
            "Epoch 449/1000 - Training Loss: 6.8345 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2944\n",
            "Epoch 450/1000 - Training Loss: 6.8234 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2939\n",
            "Epoch 451/1000 - Training Loss: 6.8066 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2934\n",
            "Epoch 452/1000 - Training Loss: 6.7886 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2928\n",
            "Epoch 453/1000 - Training Loss: 6.7653 - Training Acc: 94.12% - Validation Acc: 93.50% - Validation Loss: 0.2923\n",
            "Epoch 454/1000 - Training Loss: 6.7530 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2917\n",
            "Epoch 455/1000 - Training Loss: 6.7342 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2912\n",
            "Epoch 456/1000 - Training Loss: 6.7168 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2907\n",
            "Epoch 457/1000 - Training Loss: 6.7102 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2902\n",
            "Epoch 458/1000 - Training Loss: 6.6800 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2897\n",
            "Epoch 459/1000 - Training Loss: 6.6847 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2891\n",
            "Epoch 460/1000 - Training Loss: 6.6548 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2885\n",
            "Epoch 461/1000 - Training Loss: 6.6342 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2880\n",
            "Epoch 462/1000 - Training Loss: 6.6268 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2874\n",
            "Epoch 463/1000 - Training Loss: 6.5967 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2869\n",
            "Epoch 464/1000 - Training Loss: 6.5848 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2864\n",
            "Epoch 465/1000 - Training Loss: 6.5716 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2858\n",
            "Epoch 466/1000 - Training Loss: 6.5470 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2853\n",
            "Epoch 467/1000 - Training Loss: 6.5421 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2847\n",
            "Epoch 468/1000 - Training Loss: 6.5169 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2841\n",
            "Epoch 469/1000 - Training Loss: 6.5086 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2835\n",
            "Epoch 470/1000 - Training Loss: 6.4881 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2830\n",
            "Epoch 471/1000 - Training Loss: 6.4628 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2824\n",
            "Epoch 472/1000 - Training Loss: 6.4498 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2819\n",
            "Epoch 473/1000 - Training Loss: 6.4420 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2813\n",
            "Epoch 474/1000 - Training Loss: 6.4022 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2807\n",
            "Epoch 475/1000 - Training Loss: 6.4058 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2801\n",
            "Epoch 476/1000 - Training Loss: 6.3955 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2795\n",
            "Epoch 477/1000 - Training Loss: 6.3669 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2789\n",
            "Epoch 478/1000 - Training Loss: 6.3527 - Training Acc: 94.25% - Validation Acc: 93.50% - Validation Loss: 0.2782\n",
            "Epoch 479/1000 - Training Loss: 6.3338 - Training Acc: 94.38% - Validation Acc: 93.50% - Validation Loss: 0.2776\n",
            "Epoch 480/1000 - Training Loss: 6.3129 - Training Acc: 94.38% - Validation Acc: 93.50% - Validation Loss: 0.2770\n",
            "Epoch 481/1000 - Training Loss: 6.2959 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2764\n",
            "Epoch 482/1000 - Training Loss: 6.2863 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2758\n",
            "Epoch 483/1000 - Training Loss: 6.2661 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2751\n",
            "Epoch 484/1000 - Training Loss: 6.2546 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2745\n",
            "Epoch 485/1000 - Training Loss: 6.2310 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2738\n",
            "Epoch 486/1000 - Training Loss: 6.2066 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2732\n",
            "Epoch 487/1000 - Training Loss: 6.1914 - Training Acc: 94.38% - Validation Acc: 94.00% - Validation Loss: 0.2725\n",
            "Epoch 488/1000 - Training Loss: 6.1778 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2719\n",
            "Epoch 489/1000 - Training Loss: 6.1574 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2712\n",
            "Epoch 490/1000 - Training Loss: 6.1237 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2707\n",
            "Epoch 491/1000 - Training Loss: 6.1201 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2700\n",
            "Epoch 492/1000 - Training Loss: 6.0804 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2695\n",
            "Epoch 493/1000 - Training Loss: 6.0833 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2688\n",
            "Epoch 494/1000 - Training Loss: 6.0950 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2680\n",
            "Epoch 495/1000 - Training Loss: 6.0537 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2673\n",
            "Epoch 496/1000 - Training Loss: 6.0348 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2666\n",
            "Epoch 497/1000 - Training Loss: 6.0100 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2659\n",
            "Epoch 498/1000 - Training Loss: 5.9941 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2652\n",
            "Epoch 499/1000 - Training Loss: 5.9858 - Training Acc: 94.50% - Validation Acc: 94.00% - Validation Loss: 0.2645\n",
            "Epoch 500/1000 - Training Loss: 5.9618 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2637\n",
            "Epoch 501/1000 - Training Loss: 5.9287 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2631\n",
            "Epoch 502/1000 - Training Loss: 5.9217 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2623\n",
            "Epoch 503/1000 - Training Loss: 5.9069 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2616\n",
            "Epoch 504/1000 - Training Loss: 5.8856 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2609\n",
            "Epoch 505/1000 - Training Loss: 5.8880 - Training Acc: 94.75% - Validation Acc: 94.00% - Validation Loss: 0.2600\n",
            "Epoch 506/1000 - Training Loss: 5.8523 - Training Acc: 94.88% - Validation Acc: 94.00% - Validation Loss: 0.2593\n",
            "Epoch 507/1000 - Training Loss: 5.8356 - Training Acc: 95.00% - Validation Acc: 94.00% - Validation Loss: 0.2585\n",
            "Epoch 508/1000 - Training Loss: 5.7986 - Training Acc: 95.00% - Validation Acc: 94.00% - Validation Loss: 0.2578\n",
            "Epoch 509/1000 - Training Loss: 5.8058 - Training Acc: 95.00% - Validation Acc: 94.00% - Validation Loss: 0.2570\n",
            "Epoch 510/1000 - Training Loss: 5.7698 - Training Acc: 95.12% - Validation Acc: 94.00% - Validation Loss: 0.2563\n",
            "Epoch 511/1000 - Training Loss: 5.7730 - Training Acc: 95.12% - Validation Acc: 94.00% - Validation Loss: 0.2555\n",
            "Epoch 512/1000 - Training Loss: 5.7384 - Training Acc: 95.12% - Validation Acc: 94.00% - Validation Loss: 0.2547\n",
            "Epoch 513/1000 - Training Loss: 5.7256 - Training Acc: 95.38% - Validation Acc: 94.00% - Validation Loss: 0.2540\n",
            "Epoch 514/1000 - Training Loss: 5.7028 - Training Acc: 95.38% - Validation Acc: 94.00% - Validation Loss: 0.2532\n",
            "Epoch 515/1000 - Training Loss: 5.6826 - Training Acc: 95.50% - Validation Acc: 94.00% - Validation Loss: 0.2525\n",
            "Epoch 516/1000 - Training Loss: 5.6577 - Training Acc: 95.62% - Validation Acc: 94.00% - Validation Loss: 0.2518\n",
            "Epoch 517/1000 - Training Loss: 5.6489 - Training Acc: 95.75% - Validation Acc: 94.00% - Validation Loss: 0.2510\n",
            "Epoch 518/1000 - Training Loss: 5.6348 - Training Acc: 95.75% - Validation Acc: 94.00% - Validation Loss: 0.2502\n",
            "Epoch 519/1000 - Training Loss: 5.5927 - Training Acc: 95.75% - Validation Acc: 94.00% - Validation Loss: 0.2496\n",
            "Epoch 520/1000 - Training Loss: 5.5914 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2488\n",
            "Epoch 521/1000 - Training Loss: 5.5858 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2480\n",
            "Epoch 522/1000 - Training Loss: 5.5454 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2473\n",
            "Epoch 523/1000 - Training Loss: 5.5229 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2466\n",
            "Epoch 524/1000 - Training Loss: 5.5213 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2458\n",
            "Epoch 525/1000 - Training Loss: 5.5076 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2450\n",
            "Epoch 526/1000 - Training Loss: 5.4833 - Training Acc: 95.88% - Validation Acc: 94.00% - Validation Loss: 0.2442\n",
            "Epoch 527/1000 - Training Loss: 5.4760 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2433\n",
            "Epoch 528/1000 - Training Loss: 5.4593 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2424\n",
            "Epoch 529/1000 - Training Loss: 5.4371 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2415\n",
            "Epoch 530/1000 - Training Loss: 5.3889 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2408\n",
            "Epoch 531/1000 - Training Loss: 5.3881 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2401\n",
            "Epoch 532/1000 - Training Loss: 5.3649 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2394\n",
            "Epoch 533/1000 - Training Loss: 5.3572 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2386\n",
            "Epoch 534/1000 - Training Loss: 5.3443 - Training Acc: 96.00% - Validation Acc: 94.00% - Validation Loss: 0.2377\n",
            "Epoch 535/1000 - Training Loss: 5.2996 - Training Acc: 96.12% - Validation Acc: 94.00% - Validation Loss: 0.2371\n",
            "Epoch 536/1000 - Training Loss: 5.3046 - Training Acc: 96.25% - Validation Acc: 94.50% - Validation Loss: 0.2363\n",
            "Epoch 537/1000 - Training Loss: 5.2848 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2355\n",
            "Epoch 538/1000 - Training Loss: 5.2882 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2345\n",
            "Epoch 539/1000 - Training Loss: 5.2575 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2336\n",
            "Epoch 540/1000 - Training Loss: 5.2132 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2329\n",
            "Epoch 541/1000 - Training Loss: 5.1926 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2323\n",
            "Epoch 542/1000 - Training Loss: 5.1876 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2315\n",
            "Epoch 543/1000 - Training Loss: 5.1774 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2307\n",
            "Epoch 544/1000 - Training Loss: 5.1405 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2301\n",
            "Epoch 545/1000 - Training Loss: 5.1476 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2291\n",
            "Epoch 546/1000 - Training Loss: 5.1215 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2283\n",
            "Epoch 547/1000 - Training Loss: 5.0929 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2276\n",
            "Epoch 548/1000 - Training Loss: 5.0992 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2266\n",
            "Epoch 549/1000 - Training Loss: 5.0926 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2255\n",
            "Epoch 550/1000 - Training Loss: 5.0566 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2246\n",
            "Epoch 551/1000 - Training Loss: 5.0059 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2241\n",
            "Epoch 552/1000 - Training Loss: 5.0206 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2231\n",
            "Epoch 553/1000 - Training Loss: 4.9933 - Training Acc: 96.38% - Validation Acc: 94.50% - Validation Loss: 0.2224\n",
            "Epoch 554/1000 - Training Loss: 4.9912 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2214\n",
            "Epoch 555/1000 - Training Loss: 4.9460 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2207\n",
            "Epoch 556/1000 - Training Loss: 4.9764 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2196\n",
            "Epoch 557/1000 - Training Loss: 4.9444 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2186\n",
            "Epoch 558/1000 - Training Loss: 4.8959 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2180\n",
            "Epoch 559/1000 - Training Loss: 4.9161 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2169\n",
            "Epoch 560/1000 - Training Loss: 4.8750 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2162\n",
            "Epoch 561/1000 - Training Loss: 4.8648 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2154\n",
            "Epoch 562/1000 - Training Loss: 4.8245 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2149\n",
            "Epoch 563/1000 - Training Loss: 4.8308 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2141\n",
            "Epoch 564/1000 - Training Loss: 4.8487 - Training Acc: 96.50% - Validation Acc: 94.50% - Validation Loss: 0.2128\n",
            "Epoch 565/1000 - Training Loss: 4.8065 - Training Acc: 96.62% - Validation Acc: 94.50% - Validation Loss: 0.2119\n",
            "Epoch 566/1000 - Training Loss: 4.7711 - Training Acc: 96.62% - Validation Acc: 94.50% - Validation Loss: 0.2113\n",
            "Epoch 567/1000 - Training Loss: 4.7361 - Training Acc: 96.62% - Validation Acc: 95.50% - Validation Loss: 0.2109\n",
            "Epoch 568/1000 - Training Loss: 4.7686 - Training Acc: 96.62% - Validation Acc: 95.50% - Validation Loss: 0.2099\n",
            "Epoch 569/1000 - Training Loss: 4.7295 - Training Acc: 96.62% - Validation Acc: 95.50% - Validation Loss: 0.2091\n",
            "Epoch 570/1000 - Training Loss: 4.7351 - Training Acc: 96.62% - Validation Acc: 95.50% - Validation Loss: 0.2082\n",
            "Epoch 571/1000 - Training Loss: 4.7146 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2072\n",
            "Epoch 572/1000 - Training Loss: 4.6971 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2062\n",
            "Epoch 573/1000 - Training Loss: 4.6852 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2052\n",
            "Epoch 574/1000 - Training Loss: 4.6666 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2042\n",
            "Epoch 575/1000 - Training Loss: 4.6391 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2035\n",
            "Epoch 576/1000 - Training Loss: 4.6353 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2025\n",
            "Epoch 577/1000 - Training Loss: 4.6052 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2017\n",
            "Epoch 578/1000 - Training Loss: 4.5711 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2012\n",
            "Epoch 579/1000 - Training Loss: 4.5834 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.2003\n",
            "Epoch 580/1000 - Training Loss: 4.5764 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1992\n",
            "Epoch 581/1000 - Training Loss: 4.5439 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1984\n",
            "Epoch 582/1000 - Training Loss: 4.5348 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1975\n",
            "Epoch 583/1000 - Training Loss: 4.5128 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1966\n",
            "Epoch 584/1000 - Training Loss: 4.4955 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1958\n",
            "Epoch 585/1000 - Training Loss: 4.4619 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1952\n",
            "Epoch 586/1000 - Training Loss: 4.4631 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1942\n",
            "Epoch 587/1000 - Training Loss: 4.4537 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1932\n",
            "Epoch 588/1000 - Training Loss: 4.4480 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1918\n",
            "Epoch 589/1000 - Training Loss: 4.4127 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1909\n",
            "Epoch 590/1000 - Training Loss: 4.3927 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1900\n",
            "Epoch 591/1000 - Training Loss: 4.3600 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1894\n",
            "Epoch 592/1000 - Training Loss: 4.3394 - Training Acc: 96.75% - Validation Acc: 95.50% - Validation Loss: 0.1889\n",
            "Epoch 593/1000 - Training Loss: 4.3487 - Training Acc: 96.75% - Validation Acc: 96.00% - Validation Loss: 0.1880\n",
            "Epoch 594/1000 - Training Loss: 4.3197 - Training Acc: 96.75% - Validation Acc: 96.00% - Validation Loss: 0.1874\n",
            "Epoch 595/1000 - Training Loss: 4.3360 - Training Acc: 96.75% - Validation Acc: 96.50% - Validation Loss: 0.1861\n",
            "Epoch 596/1000 - Training Loss: 4.3152 - Training Acc: 96.75% - Validation Acc: 96.50% - Validation Loss: 0.1848\n",
            "Epoch 597/1000 - Training Loss: 4.2533 - Training Acc: 96.75% - Validation Acc: 96.50% - Validation Loss: 0.1844\n",
            "Epoch 598/1000 - Training Loss: 4.2648 - Training Acc: 96.88% - Validation Acc: 96.50% - Validation Loss: 0.1834\n",
            "Epoch 599/1000 - Training Loss: 4.2562 - Training Acc: 96.88% - Validation Acc: 96.50% - Validation Loss: 0.1822\n",
            "Epoch 600/1000 - Training Loss: 4.2206 - Training Acc: 96.88% - Validation Acc: 96.50% - Validation Loss: 0.1815\n",
            "Epoch 601/1000 - Training Loss: 4.1791 - Training Acc: 96.88% - Validation Acc: 96.50% - Validation Loss: 0.1812\n",
            "Epoch 602/1000 - Training Loss: 4.1997 - Training Acc: 96.88% - Validation Acc: 97.00% - Validation Loss: 0.1803\n",
            "Epoch 603/1000 - Training Loss: 4.1583 - Training Acc: 96.88% - Validation Acc: 97.00% - Validation Loss: 0.1797\n",
            "Epoch 604/1000 - Training Loss: 4.1536 - Training Acc: 96.88% - Validation Acc: 97.00% - Validation Loss: 0.1786\n",
            "Epoch 605/1000 - Training Loss: 4.1413 - Training Acc: 96.88% - Validation Acc: 97.00% - Validation Loss: 0.1777\n",
            "Epoch 606/1000 - Training Loss: 4.1165 - Training Acc: 96.88% - Validation Acc: 97.00% - Validation Loss: 0.1772\n",
            "Epoch 607/1000 - Training Loss: 4.1304 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1760\n",
            "Epoch 608/1000 - Training Loss: 4.1140 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1746\n",
            "Epoch 609/1000 - Training Loss: 4.0477 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1744\n",
            "Epoch 610/1000 - Training Loss: 4.0616 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1735\n",
            "Epoch 611/1000 - Training Loss: 4.0415 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1727\n",
            "Epoch 612/1000 - Training Loss: 4.0686 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1710\n",
            "Epoch 613/1000 - Training Loss: 3.9770 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1708\n",
            "Epoch 614/1000 - Training Loss: 3.9950 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1698\n",
            "Epoch 615/1000 - Training Loss: 3.9682 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1690\n",
            "Epoch 616/1000 - Training Loss: 3.9697 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1680\n",
            "Epoch 617/1000 - Training Loss: 3.9495 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1670\n",
            "Epoch 618/1000 - Training Loss: 3.8910 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1669\n",
            "Epoch 619/1000 - Training Loss: 3.8974 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1665\n",
            "Epoch 620/1000 - Training Loss: 3.8945 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1659\n",
            "Epoch 621/1000 - Training Loss: 3.8622 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1655\n",
            "Epoch 622/1000 - Training Loss: 3.8785 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1643\n",
            "Epoch 623/1000 - Training Loss: 3.8548 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1631\n",
            "Epoch 624/1000 - Training Loss: 3.8075 - Training Acc: 97.00% - Validation Acc: 97.00% - Validation Loss: 0.1627\n",
            "Epoch 625/1000 - Training Loss: 3.8465 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1610\n",
            "Epoch 626/1000 - Training Loss: 3.7775 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1605\n",
            "Epoch 627/1000 - Training Loss: 3.7568 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1602\n",
            "Epoch 628/1000 - Training Loss: 3.8002 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1587\n",
            "Epoch 629/1000 - Training Loss: 3.7573 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1576\n",
            "Epoch 630/1000 - Training Loss: 3.6653 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1584\n",
            "Epoch 631/1000 - Training Loss: 3.7241 - Training Acc: 97.12% - Validation Acc: 97.00% - Validation Loss: 0.1574\n",
            "Epoch 632/1000 - Training Loss: 3.7319 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1552\n",
            "Epoch 633/1000 - Training Loss: 3.7057 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1538\n",
            "Epoch 634/1000 - Training Loss: 3.6373 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1536\n",
            "Epoch 635/1000 - Training Loss: 3.6436 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1532\n",
            "Epoch 636/1000 - Training Loss: 3.6450 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1519\n",
            "Epoch 637/1000 - Training Loss: 3.6542 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1503\n",
            "Epoch 638/1000 - Training Loss: 3.5878 - Training Acc: 97.12% - Validation Acc: 97.50% - Validation Loss: 0.1495\n",
            "Epoch 639/1000 - Training Loss: 3.5989 - Training Acc: 97.25% - Validation Acc: 97.50% - Validation Loss: 0.1484\n",
            "Epoch 640/1000 - Training Loss: 3.5100 - Training Acc: 97.25% - Validation Acc: 97.50% - Validation Loss: 0.1492\n",
            "Epoch 641/1000 - Training Loss: 3.5493 - Training Acc: 97.25% - Validation Acc: 97.50% - Validation Loss: 0.1483\n",
            "Epoch 642/1000 - Training Loss: 3.5431 - Training Acc: 97.25% - Validation Acc: 97.50% - Validation Loss: 0.1468\n",
            "Epoch 643/1000 - Training Loss: 3.4901 - Training Acc: 97.38% - Validation Acc: 97.50% - Validation Loss: 0.1463\n",
            "Epoch 644/1000 - Training Loss: 3.5128 - Training Acc: 97.38% - Validation Acc: 97.50% - Validation Loss: 0.1451\n",
            "Epoch 645/1000 - Training Loss: 3.4711 - Training Acc: 97.38% - Validation Acc: 97.50% - Validation Loss: 0.1443\n",
            "Epoch 646/1000 - Training Loss: 3.3894 - Training Acc: 97.25% - Validation Acc: 98.00% - Validation Loss: 0.1458\n",
            "Epoch 647/1000 - Training Loss: 3.4424 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1451\n",
            "Epoch 648/1000 - Training Loss: 3.4974 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1422\n",
            "Epoch 649/1000 - Training Loss: 3.4161 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1416\n",
            "Epoch 650/1000 - Training Loss: 3.4193 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1398\n",
            "Epoch 651/1000 - Training Loss: 3.3214 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1406\n",
            "Epoch 652/1000 - Training Loss: 3.3171 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1400\n",
            "Epoch 653/1000 - Training Loss: 3.3224 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1401\n",
            "Epoch 654/1000 - Training Loss: 3.3104 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1402\n",
            "Epoch 655/1000 - Training Loss: 3.3012 - Training Acc: 97.38% - Validation Acc: 98.00% - Validation Loss: 0.1406\n",
            "Epoch 656/1000 - Training Loss: 3.3587 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1371\n",
            "Epoch 657/1000 - Training Loss: 3.2794 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1364\n",
            "Epoch 658/1000 - Training Loss: 3.2221 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1367\n",
            "Epoch 659/1000 - Training Loss: 3.2343 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1361\n",
            "Epoch 660/1000 - Training Loss: 3.2349 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1348\n",
            "Epoch 661/1000 - Training Loss: 3.2115 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1337\n",
            "Epoch 662/1000 - Training Loss: 3.1998 - Training Acc: 97.50% - Validation Acc: 98.00% - Validation Loss: 0.1333\n",
            "Epoch 663/1000 - Training Loss: 3.1863 - Training Acc: 97.62% - Validation Acc: 98.00% - Validation Loss: 0.1322\n",
            "Epoch 664/1000 - Training Loss: 3.1696 - Training Acc: 97.88% - Validation Acc: 98.00% - Validation Loss: 0.1303\n",
            "Epoch 665/1000 - Training Loss: 3.0951 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1313\n",
            "Epoch 666/1000 - Training Loss: 3.0920 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1322\n",
            "Epoch 667/1000 - Training Loss: 3.0942 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1327\n",
            "Epoch 668/1000 - Training Loss: 3.0645 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1315\n",
            "Epoch 669/1000 - Training Loss: 3.1281 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1302\n",
            "Epoch 670/1000 - Training Loss: 3.0581 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1305\n",
            "Epoch 671/1000 - Training Loss: 3.0740 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1289\n",
            "Epoch 672/1000 - Training Loss: 2.9713 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1297\n",
            "Epoch 673/1000 - Training Loss: 3.1036 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1260\n",
            "Epoch 674/1000 - Training Loss: 2.9448 - Training Acc: 97.75% - Validation Acc: 98.00% - Validation Loss: 0.1273\n",
            "Epoch 675/1000 - Training Loss: 3.0422 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1207\n",
            "Epoch 676/1000 - Training Loss: 2.8770 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1234\n",
            "Epoch 677/1000 - Training Loss: 2.9159 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1249\n",
            "Epoch 678/1000 - Training Loss: 2.9101 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1252\n",
            "Epoch 679/1000 - Training Loss: 2.9738 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1209\n",
            "Epoch 680/1000 - Training Loss: 2.8742 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1210\n",
            "Epoch 681/1000 - Training Loss: 2.8844 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1215\n",
            "Epoch 682/1000 - Training Loss: 2.7749 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1241\n",
            "Epoch 683/1000 - Training Loss: 2.8419 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1212\n",
            "Epoch 684/1000 - Training Loss: 2.8440 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1216\n",
            "Epoch 685/1000 - Training Loss: 2.8105 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1215\n",
            "Epoch 686/1000 - Training Loss: 2.8511 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1190\n",
            "Epoch 687/1000 - Training Loss: 2.7701 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1183\n",
            "Epoch 688/1000 - Training Loss: 2.7484 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1174\n",
            "Epoch 689/1000 - Training Loss: 2.7528 - Training Acc: 98.00% - Validation Acc: 98.00% - Validation Loss: 0.1159\n",
            "Epoch 690/1000 - Training Loss: 2.6381 - Training Acc: 98.00% - Validation Acc: 97.50% - Validation Loss: 0.1216\n",
            "Epoch 691/1000 - Training Loss: 2.8137 - Training Acc: 98.12% - Validation Acc: 98.50% - Validation Loss: 0.1145\n",
            "Epoch 692/1000 - Training Loss: 2.6437 - Training Acc: 98.12% - Validation Acc: 97.50% - Validation Loss: 0.1164\n",
            "Epoch 693/1000 - Training Loss: 2.7566 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1124\n",
            "Epoch 694/1000 - Training Loss: 2.5826 - Training Acc: 98.12% - Validation Acc: 97.50% - Validation Loss: 0.1168\n",
            "Epoch 695/1000 - Training Loss: 2.6113 - Training Acc: 98.00% - Validation Acc: 97.50% - Validation Loss: 0.1202\n",
            "Epoch 696/1000 - Training Loss: 2.7030 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1154\n",
            "Epoch 697/1000 - Training Loss: 2.5829 - Training Acc: 98.12% - Validation Acc: 97.50% - Validation Loss: 0.1179\n",
            "Epoch 698/1000 - Training Loss: 2.6474 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-b4a18b382b35>:47: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 699/1000 - Training Loss: 2.5493 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1126\n",
            "Epoch 700/1000 - Training Loss: 2.5608 - Training Acc: 98.12% - Validation Acc: 98.50% - Validation Loss: 0.1067\n",
            "Epoch 701/1000 - Training Loss: 2.5554 - Training Acc: 98.25% - Validation Acc: 98.50% - Validation Loss: 0.1052\n",
            "Epoch 702/1000 - Training Loss: 2.4526 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1089\n",
            "Epoch 703/1000 - Training Loss: 2.5305 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1097\n",
            "Epoch 704/1000 - Training Loss: 2.4129 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1151\n",
            "Epoch 705/1000 - Training Loss: 2.6868 - Training Acc: 98.38% - Validation Acc: 98.50% - Validation Loss: 0.0974\n",
            "Epoch 706/1000 - Training Loss: 2.3143 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1067\n",
            "Epoch 707/1000 - Training Loss: 2.4019 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1091\n",
            "Epoch 708/1000 - Training Loss: 2.4282 - Training Acc: 98.12% - Validation Acc: 98.50% - Validation Loss: 0.1074\n",
            "Epoch 709/1000 - Training Loss: 2.3880 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1104\n",
            "Epoch 710/1000 - Training Loss: 2.4564 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1074\n",
            "Epoch 711/1000 - Training Loss: 2.4521 - Training Acc: 98.25% - Validation Acc: 98.50% - Validation Loss: 0.1003\n",
            "Epoch 712/1000 - Training Loss: 2.3251 - Training Acc: 98.12% - Validation Acc: 98.50% - Validation Loss: 0.1050\n",
            "Epoch 713/1000 - Training Loss: 2.2563 - Training Acc: 98.12% - Validation Acc: 98.00% - Validation Loss: 0.1093\n",
            "Epoch 714/1000 - Training Loss: 2.4313 - Training Acc: 98.50% - Validation Acc: 98.50% - Validation Loss: 0.0964\n",
            "Epoch 715/1000 - Training Loss: 2.3496 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0991\n",
            "Epoch 716/1000 - Training Loss: 2.3015 - Training Acc: 98.50% - Validation Acc: 98.50% - Validation Loss: 0.0915\n",
            "Epoch 717/1000 - Training Loss: 2.1712 - Training Acc: 98.38% - Validation Acc: 98.50% - Validation Loss: 0.0971\n",
            "Epoch 718/1000 - Training Loss: 2.3637 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0954\n",
            "Epoch 719/1000 - Training Loss: 2.2899 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0936\n",
            "Epoch 720/1000 - Training Loss: 2.1439 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0964\n",
            "Epoch 721/1000 - Training Loss: 2.1966 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0947\n",
            "Epoch 722/1000 - Training Loss: 2.2915 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0977\n",
            "Epoch 723/1000 - Training Loss: 2.0580 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0953\n",
            "Epoch 724/1000 - Training Loss: 2.0876 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0947\n",
            "Epoch 725/1000 - Training Loss: 2.1906 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0906\n",
            "Epoch 726/1000 - Training Loss: 2.0408 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0976\n",
            "Epoch 727/1000 - Training Loss: 1.9977 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0989\n",
            "Epoch 728/1000 - Training Loss: 2.2422 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0825\n",
            "Epoch 729/1000 - Training Loss: 1.9178 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0939\n",
            "Epoch 730/1000 - Training Loss: 1.9947 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.1010\n",
            "Epoch 731/1000 - Training Loss: 2.0162 - Training Acc: 98.38% - Validation Acc: 99.00% - Validation Loss: 0.1016\n",
            "Epoch 732/1000 - Training Loss: 2.2277 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0784\n",
            "Epoch 733/1000 - Training Loss: 1.6850 - Training Acc: 98.38% - Validation Acc: 99.00% - Validation Loss: 0.1022\n",
            "Epoch 734/1000 - Training Loss: 2.2185 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0856\n",
            "Epoch 735/1000 - Training Loss: 1.7860 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0983\n",
            "Epoch 736/1000 - Training Loss: 2.1019 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0869\n",
            "Epoch 737/1000 - Training Loss: 1.9080 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0759\n",
            "Epoch 738/1000 - Training Loss: 1.7995 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0910\n",
            "Epoch 739/1000 - Training Loss: 1.7134 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0903\n",
            "Epoch 740/1000 - Training Loss: 2.1150 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0881\n",
            "Epoch 741/1000 - Training Loss: 1.9641 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0897\n",
            "Epoch 742/1000 - Training Loss: 1.7357 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0858\n",
            "Epoch 743/1000 - Training Loss: 1.9514 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0853\n",
            "Epoch 744/1000 - Training Loss: 1.7791 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0790\n",
            "Epoch 745/1000 - Training Loss: 1.7721 - Training Acc: 98.50% - Validation Acc: 98.50% - Validation Loss: 0.0923\n",
            "Epoch 746/1000 - Training Loss: 1.9633 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0772\n",
            "Epoch 747/1000 - Training Loss: 1.7647 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0717\n",
            "Epoch 748/1000 - Training Loss: 1.8217 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0672\n",
            "Epoch 749/1000 - Training Loss: 1.6496 - Training Acc: 98.38% - Validation Acc: 98.50% - Validation Loss: 0.0929\n",
            "Epoch 750/1000 - Training Loss: 1.7693 - Training Acc: 98.38% - Validation Acc: 98.50% - Validation Loss: 0.0922\n",
            "Epoch 751/1000 - Training Loss: 1.8564 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0708\n",
            "Epoch 752/1000 - Training Loss: 1.4920 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0739\n",
            "Epoch 753/1000 - Training Loss: 1.7208 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0784\n",
            "Epoch 754/1000 - Training Loss: 1.5104 - Training Acc: 98.12% - Validation Acc: 98.50% - Validation Loss: 0.0999\n",
            "Epoch 755/1000 - Training Loss: 2.1624 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0786\n",
            "Epoch 756/1000 - Training Loss: 1.7195 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0757\n",
            "Epoch 757/1000 - Training Loss: 1.6546 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0674\n",
            "Epoch 758/1000 - Training Loss: 1.5738 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0875\n",
            "Epoch 759/1000 - Training Loss: 1.4873 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0650\n",
            "Epoch 760/1000 - Training Loss: 1.4826 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0840\n",
            "Epoch 761/1000 - Training Loss: 1.3939 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0707\n",
            "Epoch 762/1000 - Training Loss: 1.4329 - Training Acc: 98.50% - Validation Acc: 99.00% - Validation Loss: 0.0761\n",
            "Epoch 763/1000 - Training Loss: 1.5323 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0900\n",
            "Epoch 764/1000 - Training Loss: 1.4272 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0769\n",
            "Epoch 765/1000 - Training Loss: 1.6495 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0576\n",
            "Epoch 766/1000 - Training Loss: 1.3383 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0639\n",
            "Epoch 767/1000 - Training Loss: 1.4407 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0773\n",
            "Epoch 768/1000 - Training Loss: 1.4433 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0614\n",
            "Epoch 769/1000 - Training Loss: 1.3471 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0671\n",
            "Epoch 770/1000 - Training Loss: 1.3761 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0764\n",
            "Epoch 771/1000 - Training Loss: 1.4984 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0725\n",
            "Epoch 772/1000 - Training Loss: 1.2454 - Training Acc: 98.62% - Validation Acc: 99.00% - Validation Loss: 0.0952\n",
            "Epoch 773/1000 - Training Loss: 1.8386 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0747\n",
            "Epoch 774/1000 - Training Loss: 1.3615 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0951\n",
            "Epoch 775/1000 - Training Loss: 1.4087 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0770\n",
            "Epoch 776/1000 - Training Loss: 1.4563 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0762\n",
            "Epoch 777/1000 - Training Loss: 1.4190 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0551\n",
            "Epoch 778/1000 - Training Loss: 1.2987 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0578\n",
            "Epoch 779/1000 - Training Loss: 1.0755 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0780\n",
            "Epoch 780/1000 - Training Loss: 1.3028 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0806\n",
            "Epoch 781/1000 - Training Loss: 1.2476 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0733\n",
            "Epoch 782/1000 - Training Loss: 1.4133 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0717\n",
            "Epoch 783/1000 - Training Loss: 1.2243 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0809\n",
            "Epoch 784/1000 - Training Loss: 1.2067 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0760\n",
            "Epoch 785/1000 - Training Loss: 1.2426 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0888\n",
            "Epoch 786/1000 - Training Loss: 1.3814 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0792\n",
            "Epoch 787/1000 - Training Loss: 1.5967 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0651\n",
            "Epoch 788/1000 - Training Loss: 1.2447 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0780\n",
            "Epoch 789/1000 - Training Loss: 1.1774 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0891\n",
            "Epoch 790/1000 - Training Loss: 1.7152 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0687\n",
            "Epoch 791/1000 - Training Loss: 1.1993 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0812\n",
            "Epoch 792/1000 - Training Loss: 1.2747 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0878\n",
            "Epoch 793/1000 - Training Loss: 1.2899 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0659\n",
            "Epoch 794/1000 - Training Loss: 1.0665 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.1006\n",
            "Epoch 795/1000 - Training Loss: 1.4558 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0671\n",
            "Epoch 796/1000 - Training Loss: 1.7515 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0600\n",
            "Epoch 797/1000 - Training Loss: 1.0443 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0651\n",
            "Epoch 798/1000 - Training Loss: 1.2126 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0883\n",
            "Epoch 799/1000 - Training Loss: 1.5176 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0605\n",
            "Epoch 800/1000 - Training Loss: 1.2170 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0903\n",
            "Epoch 801/1000 - Training Loss: 1.1430 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0970\n",
            "Epoch 802/1000 - Training Loss: 1.4813 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0782\n",
            "Epoch 803/1000 - Training Loss: 1.6829 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0691\n",
            "Epoch 804/1000 - Training Loss: 1.1926 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.0951\n",
            "Epoch 805/1000 - Training Loss: 1.5075 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1194\n",
            "Epoch 806/1000 - Training Loss: 2.4332 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0993\n",
            "Epoch 807/1000 - Training Loss: 1.7193 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0486\n",
            "Epoch 808/1000 - Training Loss: 0.9201 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0689\n",
            "Epoch 809/1000 - Training Loss: 1.3906 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.1003\n",
            "Epoch 810/1000 - Training Loss: 1.7766 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.0679\n",
            "Epoch 811/1000 - Training Loss: 1.0501 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1015\n",
            "Epoch 812/1000 - Training Loss: 1.8375 - Training Acc: 99.50% - Validation Acc: 99.00% - Validation Loss: 0.0171\n",
            "Epoch 813/1000 - Training Loss: 1.0668 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0588\n",
            "Epoch 814/1000 - Training Loss: 1.1161 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1016\n",
            "Epoch 815/1000 - Training Loss: 1.4610 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.0727\n",
            "Epoch 816/1000 - Training Loss: 1.0525 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0838\n",
            "Epoch 817/1000 - Training Loss: 1.5652 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0406\n",
            "Epoch 818/1000 - Training Loss: 0.8297 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.1310\n",
            "Epoch 819/1000 - Training Loss: 1.8384 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.0896\n",
            "Epoch 820/1000 - Training Loss: 1.4921 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0776\n",
            "Epoch 821/1000 - Training Loss: 2.0091 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0920\n",
            "Epoch 822/1000 - Training Loss: 2.0030 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1292\n",
            "Epoch 823/1000 - Training Loss: 2.2685 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.0483\n",
            "Epoch 824/1000 - Training Loss: 0.9994 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.0648\n",
            "Epoch 825/1000 - Training Loss: 1.8070 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1163\n",
            "Epoch 826/1000 - Training Loss: 1.6312 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1320\n",
            "Epoch 827/1000 - Training Loss: 1.3399 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.1852\n",
            "Epoch 828/1000 - Training Loss: 3.0186 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0816\n",
            "Epoch 829/1000 - Training Loss: 1.4839 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.0767\n",
            "Epoch 830/1000 - Training Loss: 2.2926 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.0162\n",
            "Epoch 831/1000 - Training Loss: 0.7508 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1636\n",
            "Epoch 832/1000 - Training Loss: 3.1950 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.0747\n",
            "Epoch 833/1000 - Training Loss: 1.4574 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.0891\n",
            "Epoch 834/1000 - Training Loss: 1.6907 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.0232\n",
            "Epoch 835/1000 - Training Loss: 0.9194 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.0651\n",
            "Epoch 836/1000 - Training Loss: 1.3642 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1256\n",
            "Epoch 837/1000 - Training Loss: 1.6842 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.0977\n",
            "Epoch 838/1000 - Training Loss: 1.3420 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0893\n",
            "Epoch 839/1000 - Training Loss: 1.4972 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0433\n",
            "Epoch 840/1000 - Training Loss: 3.2094 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0235\n",
            "Epoch 841/1000 - Training Loss: 1.4184 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0764\n",
            "Epoch 842/1000 - Training Loss: 1.1656 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0806\n",
            "Epoch 843/1000 - Training Loss: 0.9120 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1911\n",
            "Epoch 844/1000 - Training Loss: 2.8122 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.0990\n",
            "Epoch 845/1000 - Training Loss: 1.2446 - Training Acc: 98.62% - Validation Acc: 98.50% - Validation Loss: 0.2247\n",
            "Epoch 846/1000 - Training Loss: 3.8091 - Training Acc: 99.25% - Validation Acc: 99.50% - Validation Loss: 0.1321\n",
            "Epoch 847/1000 - Training Loss: 1.7289 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0701\n",
            "Epoch 848/1000 - Training Loss: 1.0844 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.1439\n",
            "Epoch 849/1000 - Training Loss: 1.8796 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.0840\n",
            "Epoch 850/1000 - Training Loss: 1.5545 - Training Acc: 99.25% - Validation Acc: 99.50% - Validation Loss: 0.0868\n",
            "Epoch 851/1000 - Training Loss: 1.7791 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.0974\n",
            "Epoch 852/1000 - Training Loss: 1.9862 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0277\n",
            "Epoch 853/1000 - Training Loss: 1.2954 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1466\n",
            "Epoch 854/1000 - Training Loss: 2.3993 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.1582\n",
            "Epoch 855/1000 - Training Loss: 2.7855 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.0078\n",
            "Epoch 856/1000 - Training Loss: 1.8376 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.0175\n",
            "Epoch 857/1000 - Training Loss: 1.4468 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.0744\n",
            "Epoch 858/1000 - Training Loss: 4.6483 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.1475\n",
            "Epoch 859/1000 - Training Loss: 2.7847 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1127\n",
            "Epoch 860/1000 - Training Loss: 2.7339 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.0774\n",
            "Epoch 861/1000 - Training Loss: 1.6176 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1245\n",
            "Epoch 862/1000 - Training Loss: 1.8409 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1462\n",
            "Epoch 863/1000 - Training Loss: 2.3641 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1732\n",
            "Epoch 864/1000 - Training Loss: 4.0414 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.0514\n",
            "Epoch 865/1000 - Training Loss: 1.6909 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.0076\n",
            "Epoch 866/1000 - Training Loss: 2.7451 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.1595\n",
            "Epoch 867/1000 - Training Loss: 2.1115 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.2247\n",
            "Epoch 868/1000 - Training Loss: 2.2681 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1273\n",
            "Epoch 869/1000 - Training Loss: 1.5835 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.3029\n",
            "Epoch 870/1000 - Training Loss: 5.6471 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1303\n",
            "Epoch 871/1000 - Training Loss: 3.3990 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.1731\n",
            "Epoch 872/1000 - Training Loss: 1.5374 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.2250\n",
            "Epoch 873/1000 - Training Loss: 4.8827 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1704\n",
            "Epoch 874/1000 - Training Loss: 2.6722 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.1296\n",
            "Epoch 875/1000 - Training Loss: 2.0328 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.2164\n",
            "Epoch 876/1000 - Training Loss: 3.6463 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.1933\n",
            "Epoch 877/1000 - Training Loss: 4.6947 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.2712\n",
            "Epoch 878/1000 - Training Loss: 6.3121 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.2801\n",
            "Epoch 879/1000 - Training Loss: 4.6803 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 880/1000 - Training Loss: 3.6850 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.1910\n",
            "Epoch 881/1000 - Training Loss: 5.6789 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.2012\n",
            "Epoch 882/1000 - Training Loss: 5.0416 - Training Acc: 99.50% - Validation Acc: 99.00% - Validation Loss: 0.1253\n",
            "Epoch 883/1000 - Training Loss: 2.4668 - Training Acc: 99.00% - Validation Acc: 99.00% - Validation Loss: 0.2527\n",
            "Epoch 884/1000 - Training Loss: 5.4205 - Training Acc: 99.38% - Validation Acc: 99.00% - Validation Loss: 0.2012\n",
            "Epoch 885/1000 - Training Loss: 4.0969 - Training Acc: 98.38% - Validation Acc: 98.00% - Validation Loss: 0.4492\n",
            "Epoch 886/1000 - Training Loss: 9.6430 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1725\n",
            "Epoch 887/1000 - Training Loss: 3.1621 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.1733\n",
            "Epoch 888/1000 - Training Loss: 3.8039 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.2032\n",
            "Epoch 889/1000 - Training Loss: 3.8070 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.3454\n",
            "Epoch 890/1000 - Training Loss: 5.0240 - Training Acc: 98.88% - Validation Acc: 99.00% - Validation Loss: 0.2761\n",
            "Epoch 891/1000 - Training Loss: 6.2502 - Training Acc: 98.75% - Validation Acc: 98.50% - Validation Loss: 0.3521\n",
            "Epoch 892/1000 - Training Loss: 5.8787 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.1890\n",
            "Epoch 893/1000 - Training Loss: 5.6947 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.1821\n",
            "Epoch 894/1000 - Training Loss: 3.6880 - Training Acc: 99.25% - Validation Acc: 99.00% - Validation Loss: 0.2049\n",
            "Epoch 895/1000 - Training Loss: 4.8904 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1730\n",
            "Epoch 896/1000 - Training Loss: 4.9217 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 897/1000 - Training Loss: 4.5561 - Training Acc: 99.25% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 898/1000 - Training Loss: 3.6396 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.3412\n",
            "Epoch 899/1000 - Training Loss: 6.0068 - Training Acc: 99.12% - Validation Acc: 99.00% - Validation Loss: 0.1944\n",
            "Epoch 900/1000 - Training Loss: 8.0161 - Training Acc: 99.25% - Validation Acc: 99.50% - Validation Loss: 0.1732\n",
            "Epoch 901/1000 - Training Loss: 7.8338 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 902/1000 - Training Loss: 4.2022 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.3454\n",
            "Epoch 903/1000 - Training Loss: 7.3730 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 904/1000 - Training Loss: 2.9151 - Training Acc: 98.75% - Validation Acc: 99.00% - Validation Loss: 0.2743\n",
            "Epoch 905/1000 - Training Loss: 6.7963 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.0819\n",
            "Epoch 906/1000 - Training Loss: 6.1551 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 907/1000 - Training Loss: 6.3223 - Training Acc: 99.00% - Validation Acc: 99.50% - Validation Loss: 0.1753\n",
            "Epoch 908/1000 - Training Loss: 6.7558 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 909/1000 - Training Loss: 4.4238 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 910/1000 - Training Loss: 5.1107 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 911/1000 - Training Loss: 5.0985 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 912/1000 - Training Loss: 4.7856 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 913/1000 - Training Loss: 6.1090 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 914/1000 - Training Loss: 7.0885 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 915/1000 - Training Loss: 2.7779 - Training Acc: 99.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 916/1000 - Training Loss: 9.1684 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 917/1000 - Training Loss: 2.7087 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 918/1000 - Training Loss: 3.2436 - Training Acc: 99.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 919/1000 - Training Loss: 4.7643 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 920/1000 - Training Loss: 3.7522 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 921/1000 - Training Loss: 3.8688 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 922/1000 - Training Loss: 5.6526 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 923/1000 - Training Loss: 5.1254 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 924/1000 - Training Loss: 2.3871 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 925/1000 - Training Loss: 4.6479 - Training Acc: 99.12% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 926/1000 - Training Loss: 7.9727 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 927/1000 - Training Loss: 3.8188 - Training Acc: 99.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 928/1000 - Training Loss: 4.3268 - Training Acc: 99.00% - Validation Acc: 99.50% - Validation Loss: 0.1728\n",
            "Epoch 929/1000 - Training Loss: 6.5191 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 930/1000 - Training Loss: 4.4598 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 931/1000 - Training Loss: 3.3195 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 932/1000 - Training Loss: 3.3953 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 933/1000 - Training Loss: 3.4273 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 934/1000 - Training Loss: 3.3943 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 935/1000 - Training Loss: 4.0188 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 936/1000 - Training Loss: 3.4642 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 937/1000 - Training Loss: 4.5193 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 938/1000 - Training Loss: 3.2380 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 939/1000 - Training Loss: 3.4360 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 940/1000 - Training Loss: 2.3783 - Training Acc: 99.62% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 941/1000 - Training Loss: 4.3402 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 942/1000 - Training Loss: 2.3073 - Training Acc: 99.38% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 943/1000 - Training Loss: 2.5836 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 944/1000 - Training Loss: 2.1587 - Training Acc: 99.50% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 945/1000 - Training Loss: 3.9453 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 946/1000 - Training Loss: 3.2392 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 947/1000 - Training Loss: 2.1587 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 948/1000 - Training Loss: 1.6081 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 949/1000 - Training Loss: 1.6092 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 950/1000 - Training Loss: 1.0793 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 951/1000 - Training Loss: 1.0793 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 952/1000 - Training Loss: 2.1587 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 953/1000 - Training Loss: 0.0002 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 954/1000 - Training Loss: 1.0793 - Training Acc: 99.88% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 955/1000 - Training Loss: 1.0901 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 956/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 957/1000 - Training Loss: 1.0793 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 958/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 959/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 960/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 961/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 962/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 963/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 964/1000 - Training Loss: 0.0000 - Training Acc: 100.00% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 965/1000 - Training Loss: 0.0000 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 966/1000 - Training Loss: 1.0794 - Training Acc: 100.00% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 967/1000 - Training Loss: 1.0794 - Training Acc: 100.00% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 968/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 969/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 970/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 971/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 972/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 973/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 974/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 975/1000 - Training Loss: 2.1587 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 976/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 977/1000 - Training Loss: 2.3108 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 978/1000 - Training Loss: 1.2175 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 979/1000 - Training Loss: 1.0794 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.0203\n",
            "Epoch 980/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 981/1000 - Training Loss: 3.2381 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 982/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 983/1000 - Training Loss: 2.1660 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 984/1000 - Training Loss: 1.0794 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 985/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 986/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 987/1000 - Training Loss: 2.1587 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 988/1000 - Training Loss: 2.8277 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 989/1000 - Training Loss: 1.0794 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 990/1000 - Training Loss: 2.1587 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 991/1000 - Training Loss: 1.0794 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 992/1000 - Training Loss: 1.8935 - Training Acc: 99.88% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 993/1000 - Training Loss: 3.0626 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 994/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 995/1000 - Training Loss: 1.0794 - Training Acc: 99.62% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 996/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 997/1000 - Training Loss: 2.1587 - Training Acc: 99.62% - Validation Acc: 100.00% - Validation Loss: 0.0000\n",
            "Epoch 998/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 999/1000 - Training Loss: 2.1587 - Training Acc: 99.75% - Validation Acc: 99.50% - Validation Loss: 0.1727\n",
            "Epoch 1000/1000 - Training Loss: 3.2381 - Training Acc: 99.75% - Validation Acc: 100.00% - Validation Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o408_hRrC4pU"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}